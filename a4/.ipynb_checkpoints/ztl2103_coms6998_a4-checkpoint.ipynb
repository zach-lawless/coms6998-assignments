{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMS 6998 - Practical Deep Learning System Performance\n",
    "\n",
    "## Assignment 4\n",
    "* **Name**: Zach Lawless\n",
    "* **UNI**: ztl2103\n",
    "\n",
    "### Problem 1: *Transfer Learning: Shallow Learning vs Fine Tuning, PyTorch* (30 points)\n",
    "\n",
    "#### Q1\n",
    "\n",
    "Using [this PyTorch tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) as reference.\n",
    "\n",
    "* a: (4 points)\n",
    "\n",
    "Using the `vgg-flowers` Visual Domain Decathalon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '~/data/vgg-flowers'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
    "                                              shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funtion to plot image\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[:4])\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 102 target classes in the training dataset\n"
     ]
    }
   ],
   "source": [
    "# Show how many classes there are in the training dataset\n",
    "print(f\"There are {len(class_names)} target classes in the training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of training images per class\n",
      "Class 0028: 10 images\n",
      "Class 0075: 10 images\n",
      "Class 0029: 10 images\n",
      "Class 0009: 10 images\n",
      "Class 0092: 10 images\n",
      "Class 0085: 10 images\n",
      "Class 0061: 10 images\n",
      "Class 0056: 10 images\n",
      "Class 0074: 10 images\n",
      "Class 0050: 10 images\n",
      "Class 0013: 10 images\n",
      "Class 0069: 10 images\n",
      "Class 0040: 10 images\n",
      "Class 0072: 10 images\n",
      "Class 0003: 10 images\n",
      "Class 0017: 10 images\n",
      "Class 0101: 10 images\n",
      "Class 0094: 10 images\n",
      "Class 0026: 10 images\n",
      "Class 0046: 10 images\n",
      "Class 0008: 10 images\n",
      "Class 0071: 10 images\n",
      "Class 0049: 10 images\n",
      "Class 0080: 10 images\n",
      "Class 0067: 10 images\n",
      "Class 0044: 10 images\n",
      "Class 0100: 10 images\n",
      "Class 0102: 10 images\n",
      "Class 0007: 10 images\n",
      "Class 0089: 10 images\n",
      "Class 0015: 10 images\n",
      "Class 0087: 10 images\n",
      "Class 0004: 10 images\n",
      "Class 0077: 10 images\n",
      "Class 0022: 10 images\n",
      "Class 0030: 10 images\n",
      "Class 0090: 10 images\n",
      "Class 0060: 10 images\n",
      "Class 0005: 10 images\n",
      "Class 0025: 10 images\n",
      "Class 0036: 10 images\n",
      "Class 0064: 10 images\n",
      "Class 0053: 10 images\n",
      "Class 0099: 10 images\n",
      "Class 0034: 10 images\n",
      "Class 0065: 10 images\n",
      "Class 0024: 10 images\n",
      "Class 0066: 10 images\n",
      "Class 0059: 10 images\n",
      "Class 0032: 10 images\n",
      "Class 0097: 10 images\n",
      "Class 0096: 10 images\n",
      "Class 0084: 10 images\n",
      "Class 0042: 10 images\n",
      "Class 0016: 10 images\n",
      "Class 0086: 10 images\n",
      "Class 0012: 10 images\n",
      "Class 0051: 10 images\n",
      "Class 0095: 10 images\n",
      "Class 0045: 10 images\n",
      "Class 0037: 10 images\n",
      "Class 0058: 10 images\n",
      "Class 0057: 10 images\n",
      "Class 0055: 10 images\n",
      "Class 0020: 10 images\n",
      "Class 0014: 10 images\n",
      "Class 0019: 10 images\n",
      "Class 0073: 10 images\n",
      "Class 0070: 10 images\n",
      "Class 0078: 10 images\n",
      "Class 0018: 10 images\n",
      "Class 0011: 10 images\n",
      "Class 0039: 10 images\n",
      "Class 0083: 10 images\n",
      "Class 0082: 10 images\n",
      "Class 0047: 10 images\n",
      "Class 0023: 10 images\n",
      "Class 0063: 10 images\n",
      "Class 0021: 10 images\n",
      "Class 0001: 10 images\n",
      "Class 0006: 10 images\n",
      "Class 0062: 10 images\n",
      "Class 0031: 10 images\n",
      "Class 0033: 10 images\n",
      "Class 0041: 10 images\n",
      "Class 0027: 10 images\n",
      "Class 0093: 10 images\n",
      "Class 0052: 10 images\n",
      "Class 0098: 10 images\n",
      "Class 0081: 10 images\n",
      "Class 0038: 10 images\n",
      "Class 0002: 10 images\n",
      "Class 0010: 10 images\n",
      "Class 0076: 10 images\n",
      "Class 0088: 10 images\n",
      "Class 0054: 10 images\n",
      "Class 0035: 10 images\n",
      "Class 0048: 10 images\n",
      "Class 0043: 10 images\n",
      "Class 0091: 10 images\n",
      "Class 0079: 10 images\n",
      "Class 0068: 10 images\n"
     ]
    }
   ],
   "source": [
    "# Show the distribution of training images per class\n",
    "base_dir = '/home/jupyter/data/vgg-flowers/train/'\n",
    "\n",
    "print(\"Distribution of training images per class\")\n",
    "for cn in os.listdir(base_dir):\n",
    "    cn_images = os.listdir(os.path.join(base_dir, cn))\n",
    "    print(f'Class {cn}: {len(cn_images)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained Resnet50 and change final layer\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b: (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 60 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=60, gamma=0.1)\n",
    "\n",
    "# Send model to device\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6525 Acc: 0.0225\n",
      "val Loss: 4.5929 Acc: 0.0196\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.5687 Acc: 0.0265\n",
      "val Loss: 4.4863 Acc: 0.0578\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.4612 Acc: 0.0882\n",
      "val Loss: 4.3703 Acc: 0.1412\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.3490 Acc: 0.1373\n",
      "val Loss: 4.2245 Acc: 0.2235\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.2027 Acc: 0.2225\n",
      "val Loss: 4.0441 Acc: 0.2716\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.0379 Acc: 0.2804\n",
      "val Loss: 3.8313 Acc: 0.2902\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 3.8418 Acc: 0.3157\n",
      "val Loss: 3.6239 Acc: 0.3412\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 3.6518 Acc: 0.3657\n",
      "val Loss: 3.4105 Acc: 0.3824\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 3.4888 Acc: 0.4137\n",
      "val Loss: 3.2118 Acc: 0.3961\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 3.2932 Acc: 0.4333\n",
      "val Loss: 3.0344 Acc: 0.4353\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 3.0963 Acc: 0.4500\n",
      "val Loss: 2.8744 Acc: 0.4637\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 2.9870 Acc: 0.4765\n",
      "val Loss: 2.7030 Acc: 0.4843\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 2.8186 Acc: 0.4892\n",
      "val Loss: 2.5716 Acc: 0.5049\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 2.6741 Acc: 0.5353\n",
      "val Loss: 2.4413 Acc: 0.5294\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 2.5149 Acc: 0.5696\n",
      "val Loss: 2.3390 Acc: 0.5382\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 2.3622 Acc: 0.5804\n",
      "val Loss: 2.2405 Acc: 0.5686\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 2.2649 Acc: 0.5922\n",
      "val Loss: 2.1320 Acc: 0.5853\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 2.1877 Acc: 0.6127\n",
      "val Loss: 2.0334 Acc: 0.5990\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 2.0731 Acc: 0.6314\n",
      "val Loss: 1.9588 Acc: 0.5961\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 1.9700 Acc: 0.6471\n",
      "val Loss: 1.8846 Acc: 0.6167\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 1.8955 Acc: 0.6745\n",
      "val Loss: 1.7982 Acc: 0.6402\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 1.8297 Acc: 0.6696\n",
      "val Loss: 1.7503 Acc: 0.6343\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 1.6957 Acc: 0.6961\n",
      "val Loss: 1.6790 Acc: 0.6696\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 1.6827 Acc: 0.6980\n",
      "val Loss: 1.6344 Acc: 0.6784\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 1.5710 Acc: 0.7216\n",
      "val Loss: 1.5978 Acc: 0.6765\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 1.5661 Acc: 0.7157\n",
      "val Loss: 1.5337 Acc: 0.6814\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 1.4684 Acc: 0.7392\n",
      "val Loss: 1.4947 Acc: 0.6931\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 1.3775 Acc: 0.7716\n",
      "val Loss: 1.4557 Acc: 0.7029\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 1.3484 Acc: 0.7676\n",
      "val Loss: 1.3992 Acc: 0.7078\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 1.2873 Acc: 0.7755\n",
      "val Loss: 1.3689 Acc: 0.7157\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 1.2200 Acc: 0.7892\n",
      "val Loss: 1.3266 Acc: 0.7186\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 1.1552 Acc: 0.8098\n",
      "val Loss: 1.3034 Acc: 0.7343\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 1.1471 Acc: 0.8127\n",
      "val Loss: 1.2778 Acc: 0.7167\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 1.0886 Acc: 0.8078\n",
      "val Loss: 1.2153 Acc: 0.7333\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 1.0536 Acc: 0.8343\n",
      "val Loss: 1.2033 Acc: 0.7284\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.9640 Acc: 0.8412\n",
      "val Loss: 1.1998 Acc: 0.7324\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.9604 Acc: 0.8284\n",
      "val Loss: 1.1479 Acc: 0.7333\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.8995 Acc: 0.8549\n",
      "val Loss: 1.1398 Acc: 0.7431\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.8513 Acc: 0.8716\n",
      "val Loss: 1.1092 Acc: 0.7598\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.8372 Acc: 0.8667\n",
      "val Loss: 1.1065 Acc: 0.7480\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.8257 Acc: 0.8529\n",
      "val Loss: 1.0717 Acc: 0.7539\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.8377 Acc: 0.8578\n",
      "val Loss: 1.0633 Acc: 0.7598\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.7829 Acc: 0.8735\n",
      "val Loss: 1.0501 Acc: 0.7627\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.7282 Acc: 0.8873\n",
      "val Loss: 1.0434 Acc: 0.7569\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.7156 Acc: 0.8833\n",
      "val Loss: 0.9955 Acc: 0.7686\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.6386 Acc: 0.9078\n",
      "val Loss: 0.9926 Acc: 0.7667\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.6950 Acc: 0.8755\n",
      "val Loss: 0.9833 Acc: 0.7686\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.6490 Acc: 0.9049\n",
      "val Loss: 0.9869 Acc: 0.7608\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.6291 Acc: 0.9000\n",
      "val Loss: 0.9786 Acc: 0.7667\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.6319 Acc: 0.8882\n",
      "val Loss: 0.9523 Acc: 0.7814\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.5856 Acc: 0.9127\n",
      "val Loss: 0.9507 Acc: 0.7745\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.5804 Acc: 0.9127\n",
      "val Loss: 0.9392 Acc: 0.7657\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.5209 Acc: 0.9216\n",
      "val Loss: 0.9366 Acc: 0.7716\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.5545 Acc: 0.9049\n",
      "val Loss: 0.9032 Acc: 0.7784\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.5601 Acc: 0.9088\n",
      "val Loss: 0.9176 Acc: 0.7804\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.4757 Acc: 0.9255\n",
      "val Loss: 0.8888 Acc: 0.7922\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.5129 Acc: 0.9196\n",
      "val Loss: 0.8959 Acc: 0.7833\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.4034 Acc: 0.9363\n",
      "val Loss: 0.8909 Acc: 0.7814\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.4339 Acc: 0.9284\n",
      "val Loss: 0.8798 Acc: 0.7784\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.4116 Acc: 0.9412\n",
      "val Loss: 0.8729 Acc: 0.7902\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.4593 Acc: 0.9382\n",
      "val Loss: 0.8544 Acc: 0.7931\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.4205 Acc: 0.9461\n",
      "val Loss: 0.8548 Acc: 0.7941\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.4051 Acc: 0.9382\n",
      "val Loss: 0.8613 Acc: 0.7931\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.4255 Acc: 0.9373\n",
      "val Loss: 0.8595 Acc: 0.7912\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.4773 Acc: 0.9186\n",
      "val Loss: 0.8565 Acc: 0.7863\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.4133 Acc: 0.9363\n",
      "val Loss: 0.8591 Acc: 0.7941\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.4452 Acc: 0.9265\n",
      "val Loss: 0.8596 Acc: 0.7931\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.4269 Acc: 0.9265\n",
      "val Loss: 0.8584 Acc: 0.7941\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.4521 Acc: 0.9186\n",
      "val Loss: 0.8563 Acc: 0.7922\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.4023 Acc: 0.9392\n",
      "val Loss: 0.8564 Acc: 0.7902\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.4228 Acc: 0.9402\n",
      "val Loss: 0.8595 Acc: 0.7882\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.4395 Acc: 0.9284\n",
      "val Loss: 0.8572 Acc: 0.7912\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.4447 Acc: 0.9373\n",
      "val Loss: 0.8573 Acc: 0.7873\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.3785 Acc: 0.9441\n",
      "val Loss: 0.8507 Acc: 0.7931\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.4256 Acc: 0.9382\n",
      "val Loss: 0.8571 Acc: 0.7892\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.3887 Acc: 0.9382\n",
      "val Loss: 0.8555 Acc: 0.7882\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.4061 Acc: 0.9451\n",
      "val Loss: 0.8547 Acc: 0.7902\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.3863 Acc: 0.9422\n",
      "val Loss: 0.8563 Acc: 0.7892\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.4352 Acc: 0.9284\n",
      "val Loss: 0.8537 Acc: 0.7912\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.4198 Acc: 0.9275\n",
      "val Loss: 0.8543 Acc: 0.7902\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.4190 Acc: 0.9392\n",
      "val Loss: 0.8531 Acc: 0.7892\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.3878 Acc: 0.9431\n",
      "val Loss: 0.8544 Acc: 0.7922\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.4196 Acc: 0.9382\n",
      "val Loss: 0.8522 Acc: 0.7902\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.3927 Acc: 0.9422\n",
      "val Loss: 0.8523 Acc: 0.7882\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.4125 Acc: 0.9343\n",
      "val Loss: 0.8529 Acc: 0.7902\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.4130 Acc: 0.9314\n",
      "val Loss: 0.8532 Acc: 0.7892\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.4139 Acc: 0.9333\n",
      "val Loss: 0.8587 Acc: 0.7902\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.3835 Acc: 0.9461\n",
      "val Loss: 0.8578 Acc: 0.7961\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.3967 Acc: 0.9412\n",
      "val Loss: 0.8483 Acc: 0.7912\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.3783 Acc: 0.9412\n",
      "val Loss: 0.8462 Acc: 0.7931\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.3730 Acc: 0.9451\n",
      "val Loss: 0.8487 Acc: 0.7941\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.4188 Acc: 0.9265\n",
      "val Loss: 0.8492 Acc: 0.7922\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.4076 Acc: 0.9353\n",
      "val Loss: 0.8544 Acc: 0.7873\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.4050 Acc: 0.9353\n",
      "val Loss: 0.8470 Acc: 0.7922\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.4261 Acc: 0.9275\n",
      "val Loss: 0.8484 Acc: 0.7912\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.4144 Acc: 0.9353\n",
      "val Loss: 0.8511 Acc: 0.7882\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.3830 Acc: 0.9471\n",
      "val Loss: 0.8470 Acc: 0.7882\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.3897 Acc: 0.9343\n",
      "val Loss: 0.8451 Acc: 0.7902\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.3899 Acc: 0.9314\n",
      "val Loss: 0.8452 Acc: 0.7961\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.3802 Acc: 0.9471\n",
      "val Loss: 0.8495 Acc: 0.7951\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3821 Acc: 0.9412\n",
      "val Loss: 0.8418 Acc: 0.7941\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.3760 Acc: 0.9471\n",
      "val Loss: 0.8371 Acc: 0.7990\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.3931 Acc: 0.9402\n",
      "val Loss: 0.8451 Acc: 0.7931\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.4081 Acc: 0.9363\n",
      "val Loss: 0.8471 Acc: 0.7980\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.4061 Acc: 0.9343\n",
      "val Loss: 0.8468 Acc: 0.7922\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.3722 Acc: 0.9392\n",
      "val Loss: 0.8472 Acc: 0.7951\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.3839 Acc: 0.9314\n",
      "val Loss: 0.8454 Acc: 0.7922\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.3413 Acc: 0.9510\n",
      "val Loss: 0.8452 Acc: 0.7902\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.4032 Acc: 0.9275\n",
      "val Loss: 0.8430 Acc: 0.7971\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.3556 Acc: 0.9500\n",
      "val Loss: 0.8440 Acc: 0.7912\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.3781 Acc: 0.9422\n",
      "val Loss: 0.8491 Acc: 0.7912\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.3975 Acc: 0.9324\n",
      "val Loss: 0.8428 Acc: 0.7941\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.3658 Acc: 0.9412\n",
      "val Loss: 0.8387 Acc: 0.7971\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.3477 Acc: 0.9510\n",
      "val Loss: 0.8367 Acc: 0.7912\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.3902 Acc: 0.9412\n",
      "val Loss: 0.8393 Acc: 0.7922\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.3894 Acc: 0.9314\n",
      "val Loss: 0.8395 Acc: 0.7912\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.4131 Acc: 0.9382\n",
      "val Loss: 0.8390 Acc: 0.7961\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.3654 Acc: 0.9451\n",
      "val Loss: 0.8366 Acc: 0.7951\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.3776 Acc: 0.9382\n",
      "val Loss: 0.8374 Acc: 0.7990\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.3712 Acc: 0.9392\n",
      "val Loss: 0.8449 Acc: 0.7941\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.3853 Acc: 0.9392\n",
      "val Loss: 0.8451 Acc: 0.7961\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.3655 Acc: 0.9382\n",
      "val Loss: 0.8400 Acc: 0.7971\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.3933 Acc: 0.9441\n",
      "val Loss: 0.8440 Acc: 0.7922\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.3730 Acc: 0.9431\n",
      "val Loss: 0.8399 Acc: 0.7990\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.3344 Acc: 0.9461\n",
      "val Loss: 0.8377 Acc: 0.7971\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.4217 Acc: 0.9235\n",
      "val Loss: 0.8398 Acc: 0.7922\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.3726 Acc: 0.9373\n",
      "val Loss: 0.8421 Acc: 0.7961\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.3788 Acc: 0.9382\n",
      "val Loss: 0.8434 Acc: 0.8000\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.3359 Acc: 0.9451\n",
      "val Loss: 0.8367 Acc: 0.8000\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.3554 Acc: 0.9441\n",
      "val Loss: 0.8430 Acc: 0.7941\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.3421 Acc: 0.9480\n",
      "val Loss: 0.8430 Acc: 0.7931\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.3747 Acc: 0.9422\n",
      "val Loss: 0.8425 Acc: 0.7961\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.3460 Acc: 0.9480\n",
      "val Loss: 0.8407 Acc: 0.7941\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.3476 Acc: 0.9402\n",
      "val Loss: 0.8378 Acc: 0.7951\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.3410 Acc: 0.9490\n",
      "val Loss: 0.8370 Acc: 0.7990\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.3360 Acc: 0.9520\n",
      "val Loss: 0.8431 Acc: 0.7971\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.3532 Acc: 0.9500\n",
      "val Loss: 0.8425 Acc: 0.7922\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.3655 Acc: 0.9441\n",
      "val Loss: 0.8388 Acc: 0.7922\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.3634 Acc: 0.9451\n",
      "val Loss: 0.8400 Acc: 0.7951\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.3530 Acc: 0.9510\n",
      "val Loss: 0.8362 Acc: 0.7931\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.3265 Acc: 0.9618\n",
      "val Loss: 0.8365 Acc: 0.7980\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.3588 Acc: 0.9451\n",
      "val Loss: 0.8373 Acc: 0.7961\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.3743 Acc: 0.9275\n",
      "val Loss: 0.8353 Acc: 0.7951\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.3650 Acc: 0.9412\n",
      "val Loss: 0.8411 Acc: 0.7971\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.3754 Acc: 0.9382\n",
      "val Loss: 0.8389 Acc: 0.7971\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.3967 Acc: 0.9363\n",
      "val Loss: 0.8373 Acc: 0.7941\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.3502 Acc: 0.9451\n",
      "val Loss: 0.8393 Acc: 0.8000\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.3402 Acc: 0.9520\n",
      "val Loss: 0.8376 Acc: 0.7971\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.3091 Acc: 0.9647\n",
      "val Loss: 0.8376 Acc: 0.7990\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.3174 Acc: 0.9549\n",
      "val Loss: 0.8400 Acc: 0.7990\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.4095 Acc: 0.9343\n",
      "val Loss: 0.8453 Acc: 0.7912\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.3929 Acc: 0.9412\n",
      "val Loss: 0.8398 Acc: 0.7922\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.3611 Acc: 0.9471\n",
      "val Loss: 0.8330 Acc: 0.7971\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.3684 Acc: 0.9373\n",
      "val Loss: 0.8316 Acc: 0.7941\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.3547 Acc: 0.9392\n",
      "val Loss: 0.8353 Acc: 0.8000\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.3536 Acc: 0.9471\n",
      "val Loss: 0.8350 Acc: 0.7951\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.3669 Acc: 0.9471\n",
      "val Loss: 0.8393 Acc: 0.7951\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.3987 Acc: 0.9304\n",
      "val Loss: 0.8391 Acc: 0.7951\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.3833 Acc: 0.9343\n",
      "val Loss: 0.8345 Acc: 0.7931\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.3608 Acc: 0.9500\n",
      "val Loss: 0.8393 Acc: 0.7912\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.3820 Acc: 0.9363\n",
      "val Loss: 0.8410 Acc: 0.7961\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.3983 Acc: 0.9294\n",
      "val Loss: 0.8383 Acc: 0.7892\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.3605 Acc: 0.9402\n",
      "val Loss: 0.8410 Acc: 0.7902\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.3227 Acc: 0.9578\n",
      "val Loss: 0.8352 Acc: 0.7922\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.3506 Acc: 0.9529\n",
      "val Loss: 0.8373 Acc: 0.7961\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.3462 Acc: 0.9480\n",
      "val Loss: 0.8374 Acc: 0.7951\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.3663 Acc: 0.9402\n",
      "val Loss: 0.8327 Acc: 0.8029\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.3568 Acc: 0.9500\n",
      "val Loss: 0.8397 Acc: 0.7912\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.3592 Acc: 0.9461\n",
      "val Loss: 0.8389 Acc: 0.7931\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.3619 Acc: 0.9422\n",
      "val Loss: 0.8361 Acc: 0.7951\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.3644 Acc: 0.9422\n",
      "val Loss: 0.8387 Acc: 0.7951\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.3583 Acc: 0.9294\n",
      "val Loss: 0.8372 Acc: 0.7990\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.3168 Acc: 0.9627\n",
      "val Loss: 0.8314 Acc: 0.7980\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.3773 Acc: 0.9461\n",
      "val Loss: 0.8342 Acc: 0.7961\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.3731 Acc: 0.9382\n",
      "val Loss: 0.8347 Acc: 0.7961\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.3558 Acc: 0.9441\n",
      "val Loss: 0.8324 Acc: 0.7971\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.3510 Acc: 0.9490\n",
      "val Loss: 0.8350 Acc: 0.7941\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.3476 Acc: 0.9510\n",
      "val Loss: 0.8382 Acc: 0.7941\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.3565 Acc: 0.9520\n",
      "val Loss: 0.8345 Acc: 0.8000\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.3517 Acc: 0.9510\n",
      "val Loss: 0.8334 Acc: 0.7990\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.3577 Acc: 0.9490\n",
      "val Loss: 0.8365 Acc: 0.7961\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.3496 Acc: 0.9441\n",
      "val Loss: 0.8354 Acc: 0.7980\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.3709 Acc: 0.9392\n",
      "val Loss: 0.8350 Acc: 0.8000\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.3740 Acc: 0.9441\n",
      "val Loss: 0.8339 Acc: 0.7941\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.3320 Acc: 0.9510\n",
      "val Loss: 0.8356 Acc: 0.7922\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.3351 Acc: 0.9529\n",
      "val Loss: 0.8321 Acc: 0.7980\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.3526 Acc: 0.9431\n",
      "val Loss: 0.8325 Acc: 0.7941\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.4063 Acc: 0.9343\n",
      "val Loss: 0.8335 Acc: 0.7922\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.3833 Acc: 0.9373\n",
      "val Loss: 0.8385 Acc: 0.7951\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.3564 Acc: 0.9471\n",
      "val Loss: 0.8422 Acc: 0.7961\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.3463 Acc: 0.9461\n",
      "val Loss: 0.8366 Acc: 0.7971\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.3160 Acc: 0.9578\n",
      "val Loss: 0.8336 Acc: 0.7931\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.3915 Acc: 0.9402\n",
      "val Loss: 0.8357 Acc: 0.7951\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.3690 Acc: 0.9441\n",
      "val Loss: 0.8376 Acc: 0.8000\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.3561 Acc: 0.9441\n",
      "val Loss: 0.8369 Acc: 0.7951\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.3283 Acc: 0.9647\n",
      "val Loss: 0.8405 Acc: 0.7980\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.3810 Acc: 0.9284\n",
      "val Loss: 0.8399 Acc: 0.7961\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.4119 Acc: 0.9275\n",
      "val Loss: 0.8354 Acc: 0.7931\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.3580 Acc: 0.9441\n",
      "val Loss: 0.8381 Acc: 0.7961\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.3720 Acc: 0.9422\n",
      "val Loss: 0.8426 Acc: 0.7931\n",
      "\n",
      "Training complete in 26m 20s\n",
      "Best val Acc: 0.802941\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c: (6 points)\n",
    "\n",
    "Repeating the exercise above except with uniform learning rate equal to 0.1 and 0.01 and no learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Performing experiment with fixed lr=0.1 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 5.2498 Acc: 0.0196\n",
      "val Loss: 749843.8510 Acc: 0.0098\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 5.0352 Acc: 0.0206\n",
      "val Loss: 795.6690 Acc: 0.0108\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.4090 Acc: 0.0363\n",
      "val Loss: 10.2650 Acc: 0.0373\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.0696 Acc: 0.0422\n",
      "val Loss: 5.0215 Acc: 0.0500\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 3.8868 Acc: 0.0559\n",
      "val Loss: 3.8237 Acc: 0.0824\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 3.6877 Acc: 0.0882\n",
      "val Loss: 4.1769 Acc: 0.1176\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 3.5217 Acc: 0.1245\n",
      "val Loss: 3.7040 Acc: 0.1314\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 3.3368 Acc: 0.1480\n",
      "val Loss: 3.5623 Acc: 0.1686\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 3.2987 Acc: 0.1647\n",
      "val Loss: 3.5789 Acc: 0.1647\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 3.2518 Acc: 0.1667\n",
      "val Loss: 3.5565 Acc: 0.1735\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 3.0891 Acc: 0.1941\n",
      "val Loss: 3.9557 Acc: 0.1451\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 3.0188 Acc: 0.2392\n",
      "val Loss: 3.5213 Acc: 0.1892\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 2.8823 Acc: 0.2324\n",
      "val Loss: 3.8216 Acc: 0.1765\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 2.7834 Acc: 0.2480\n",
      "val Loss: 3.6240 Acc: 0.2039\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 2.6396 Acc: 0.3029\n",
      "val Loss: 3.1627 Acc: 0.2500\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 2.6257 Acc: 0.3069\n",
      "val Loss: 3.8536 Acc: 0.2020\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 2.5366 Acc: 0.3186\n",
      "val Loss: 3.5336 Acc: 0.2461\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 2.4416 Acc: 0.3324\n",
      "val Loss: 3.7555 Acc: 0.2059\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 2.3806 Acc: 0.3667\n",
      "val Loss: 3.2821 Acc: 0.2520\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 2.2433 Acc: 0.3735\n",
      "val Loss: 3.3031 Acc: 0.2863\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 2.2374 Acc: 0.3784\n",
      "val Loss: 3.2288 Acc: 0.2725\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 2.0776 Acc: 0.4314\n",
      "val Loss: 3.0558 Acc: 0.3157\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 2.0227 Acc: 0.4324\n",
      "val Loss: 4.3364 Acc: 0.2569\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 2.0176 Acc: 0.4343\n",
      "val Loss: 3.0314 Acc: 0.3284\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 1.8823 Acc: 0.4598\n",
      "val Loss: 3.1605 Acc: 0.3176\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 1.9202 Acc: 0.4500\n",
      "val Loss: 3.2581 Acc: 0.3049\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 1.8703 Acc: 0.4863\n",
      "val Loss: 5.9186 Acc: 0.2118\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 1.8041 Acc: 0.4824\n",
      "val Loss: 3.4629 Acc: 0.3078\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 1.7214 Acc: 0.5235\n",
      "val Loss: 3.4604 Acc: 0.3127\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 1.6890 Acc: 0.5196\n",
      "val Loss: 3.4055 Acc: 0.3235\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 1.6670 Acc: 0.5147\n",
      "val Loss: 3.7097 Acc: 0.3049\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 1.6647 Acc: 0.5088\n",
      "val Loss: 3.5649 Acc: 0.3000\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 1.6227 Acc: 0.5402\n",
      "val Loss: 4.2674 Acc: 0.2627\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 1.5345 Acc: 0.5422\n",
      "val Loss: 3.2455 Acc: 0.3412\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 1.5738 Acc: 0.5412\n",
      "val Loss: 3.9550 Acc: 0.2725\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 1.4978 Acc: 0.5627\n",
      "val Loss: 3.4374 Acc: 0.3176\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 1.3860 Acc: 0.5833\n",
      "val Loss: 3.1897 Acc: 0.3814\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 1.3081 Acc: 0.6284\n",
      "val Loss: 3.2801 Acc: 0.3510\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 1.3171 Acc: 0.6088\n",
      "val Loss: 3.6173 Acc: 0.3392\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 1.3667 Acc: 0.6098\n",
      "val Loss: 3.9286 Acc: 0.2696\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 1.2711 Acc: 0.6225\n",
      "val Loss: 3.7613 Acc: 0.3225\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 1.1994 Acc: 0.6245\n",
      "val Loss: 3.3390 Acc: 0.3578\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 1.1891 Acc: 0.6431\n",
      "val Loss: 3.0645 Acc: 0.3755\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 1.0711 Acc: 0.6961\n",
      "val Loss: 3.5324 Acc: 0.3402\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 1.1806 Acc: 0.6657\n",
      "val Loss: 3.4956 Acc: 0.3520\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.9744 Acc: 0.7167\n",
      "val Loss: 3.2732 Acc: 0.4020\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 1.0943 Acc: 0.6824\n",
      "val Loss: 3.3572 Acc: 0.4020\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.9824 Acc: 0.7147\n",
      "val Loss: 3.2358 Acc: 0.3843\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.9305 Acc: 0.7284\n",
      "val Loss: 3.2426 Acc: 0.3843\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.9876 Acc: 0.7167\n",
      "val Loss: 7.3918 Acc: 0.2206\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.9070 Acc: 0.7343\n",
      "val Loss: 3.2567 Acc: 0.3941\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.9001 Acc: 0.7471\n",
      "val Loss: 3.6451 Acc: 0.3500\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.9538 Acc: 0.7167\n",
      "val Loss: 3.7217 Acc: 0.3510\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.9866 Acc: 0.7069\n",
      "val Loss: 3.9140 Acc: 0.3725\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.9311 Acc: 0.7304\n",
      "val Loss: 3.9310 Acc: 0.3451\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.8976 Acc: 0.7265\n",
      "val Loss: 3.3994 Acc: 0.3941\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.9367 Acc: 0.7431\n",
      "val Loss: 3.5609 Acc: 0.3980\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.8457 Acc: 0.7539\n",
      "val Loss: 3.2680 Acc: 0.3882\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.7462 Acc: 0.7686\n",
      "val Loss: 4.0766 Acc: 0.3814\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.6597 Acc: 0.7912\n",
      "val Loss: 3.4203 Acc: 0.4069\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.7366 Acc: 0.7814\n",
      "val Loss: 3.4418 Acc: 0.4314\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.6132 Acc: 0.8147\n",
      "val Loss: 3.6396 Acc: 0.4137\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.6955 Acc: 0.7961\n",
      "val Loss: 3.6491 Acc: 0.3990\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.6091 Acc: 0.8118\n",
      "val Loss: 3.4732 Acc: 0.4078\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.5812 Acc: 0.8255\n",
      "val Loss: 3.5415 Acc: 0.4137\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.6948 Acc: 0.7971\n",
      "val Loss: 4.1944 Acc: 0.3892\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.6311 Acc: 0.8314\n",
      "val Loss: 3.3628 Acc: 0.4333\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.5960 Acc: 0.8088\n",
      "val Loss: 3.6900 Acc: 0.4324\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.6471 Acc: 0.8157\n",
      "val Loss: 4.1339 Acc: 0.3735\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.5607 Acc: 0.8284\n",
      "val Loss: 3.7407 Acc: 0.4275\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.5317 Acc: 0.8461\n",
      "val Loss: 4.2721 Acc: 0.3775\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.5809 Acc: 0.8343\n",
      "val Loss: 3.7498 Acc: 0.4314\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.6328 Acc: 0.8069\n",
      "val Loss: 3.6281 Acc: 0.4049\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.5156 Acc: 0.8490\n",
      "val Loss: 3.1814 Acc: 0.4716\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.5180 Acc: 0.8559\n",
      "val Loss: 3.4923 Acc: 0.4363\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.4815 Acc: 0.8520\n",
      "val Loss: 3.8130 Acc: 0.4304\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.5183 Acc: 0.8392\n",
      "val Loss: 3.7529 Acc: 0.4343\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.4936 Acc: 0.8529\n",
      "val Loss: 3.1415 Acc: 0.4667\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.4275 Acc: 0.8775\n",
      "val Loss: 3.5126 Acc: 0.4745\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.5677 Acc: 0.8275\n",
      "val Loss: 4.1178 Acc: 0.3980\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.5304 Acc: 0.8490\n",
      "val Loss: 3.6072 Acc: 0.4569\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.4828 Acc: 0.8471\n",
      "val Loss: 3.6504 Acc: 0.4314\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.4239 Acc: 0.8814\n",
      "val Loss: 3.3109 Acc: 0.4647\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.4520 Acc: 0.8676\n",
      "val Loss: 3.5952 Acc: 0.4353\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.4575 Acc: 0.8667\n",
      "val Loss: 3.8310 Acc: 0.4069\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.4239 Acc: 0.8716\n",
      "val Loss: 3.6362 Acc: 0.4402\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.4274 Acc: 0.8745\n",
      "val Loss: 3.9087 Acc: 0.4098\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.4322 Acc: 0.8578\n",
      "val Loss: 4.0313 Acc: 0.4363\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.4434 Acc: 0.8784\n",
      "val Loss: 3.4245 Acc: 0.4647\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.3615 Acc: 0.8971\n",
      "val Loss: 3.3488 Acc: 0.4618\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.4242 Acc: 0.8686\n",
      "val Loss: 3.8551 Acc: 0.4471\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.4617 Acc: 0.8667\n",
      "val Loss: 3.7298 Acc: 0.4441\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.3903 Acc: 0.8902\n",
      "val Loss: 3.5832 Acc: 0.4480\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.3795 Acc: 0.8941\n",
      "val Loss: 3.9082 Acc: 0.4265\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.3918 Acc: 0.8833\n",
      "val Loss: 3.7573 Acc: 0.4206\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.4024 Acc: 0.8755\n",
      "val Loss: 3.7049 Acc: 0.4529\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.3746 Acc: 0.8824\n",
      "val Loss: 3.4581 Acc: 0.4598\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.4179 Acc: 0.8853\n",
      "val Loss: 3.5201 Acc: 0.4569\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.3839 Acc: 0.8863\n",
      "val Loss: 3.5684 Acc: 0.4363\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.3827 Acc: 0.8990\n",
      "val Loss: 3.7491 Acc: 0.4618\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3857 Acc: 0.8902\n",
      "val Loss: 3.7695 Acc: 0.4304\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.3273 Acc: 0.9059\n",
      "val Loss: 3.4107 Acc: 0.4549\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.3823 Acc: 0.8745\n",
      "val Loss: 3.7824 Acc: 0.4284\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.3889 Acc: 0.8755\n",
      "val Loss: 3.7604 Acc: 0.4353\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.3542 Acc: 0.8941\n",
      "val Loss: 4.1533 Acc: 0.4029\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.3811 Acc: 0.8971\n",
      "val Loss: 3.7874 Acc: 0.4314\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.4145 Acc: 0.8853\n",
      "val Loss: 3.4110 Acc: 0.4500\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.3383 Acc: 0.8951\n",
      "val Loss: 3.6860 Acc: 0.4637\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.2602 Acc: 0.9176\n",
      "val Loss: 4.0530 Acc: 0.4500\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.3316 Acc: 0.9098\n",
      "val Loss: 3.6794 Acc: 0.4637\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.3544 Acc: 0.8912\n",
      "val Loss: 4.0061 Acc: 0.4382\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.4155 Acc: 0.8814\n",
      "val Loss: 4.0666 Acc: 0.4441\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.3127 Acc: 0.9078\n",
      "val Loss: 3.9296 Acc: 0.4490\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.3207 Acc: 0.9029\n",
      "val Loss: 3.5122 Acc: 0.5010\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.9088\n",
      "val Loss: 3.5558 Acc: 0.4686\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.2980 Acc: 0.9137\n",
      "val Loss: 3.8435 Acc: 0.4598\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.2943 Acc: 0.9294\n",
      "val Loss: 3.5092 Acc: 0.4853\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.2666 Acc: 0.9294\n",
      "val Loss: 3.7671 Acc: 0.4686\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.2975 Acc: 0.9078\n",
      "val Loss: 3.5571 Acc: 0.4794\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.3030 Acc: 0.9118\n",
      "val Loss: 3.6217 Acc: 0.4745\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.2654 Acc: 0.9294\n",
      "val Loss: 3.7679 Acc: 0.4559\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.2541 Acc: 0.9216\n",
      "val Loss: 3.6834 Acc: 0.4882\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.3081 Acc: 0.9069\n",
      "val Loss: 3.7195 Acc: 0.4627\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.3327 Acc: 0.8941\n",
      "val Loss: 3.7218 Acc: 0.4637\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.2549 Acc: 0.9245\n",
      "val Loss: 3.6883 Acc: 0.4618\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.2322 Acc: 0.9304\n",
      "val Loss: 3.6057 Acc: 0.4559\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.2570 Acc: 0.9265\n",
      "val Loss: 3.4722 Acc: 0.4922\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.2894 Acc: 0.9225\n",
      "val Loss: 3.5445 Acc: 0.4882\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.2819 Acc: 0.9186\n",
      "val Loss: 3.7723 Acc: 0.4696\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.2217 Acc: 0.9373\n",
      "val Loss: 3.9951 Acc: 0.4784\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.2032 Acc: 0.9500\n",
      "val Loss: 3.3078 Acc: 0.5000\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.2573 Acc: 0.9304\n",
      "val Loss: 3.7207 Acc: 0.4873\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.2524 Acc: 0.9255\n",
      "val Loss: 3.7365 Acc: 0.4784\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.2628 Acc: 0.9157\n",
      "val Loss: 3.3411 Acc: 0.5069\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.2339 Acc: 0.9265\n",
      "val Loss: 3.5268 Acc: 0.4941\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.2771 Acc: 0.9265\n",
      "val Loss: 3.8559 Acc: 0.4667\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.2307 Acc: 0.9225\n",
      "val Loss: 3.6794 Acc: 0.4696\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.2065 Acc: 0.9431\n",
      "val Loss: 3.5744 Acc: 0.4804\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.2897 Acc: 0.9186\n",
      "val Loss: 3.4794 Acc: 0.4931\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.2878 Acc: 0.9108\n",
      "val Loss: 3.6485 Acc: 0.4657\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.2893 Acc: 0.9108\n",
      "val Loss: 3.4630 Acc: 0.4824\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.2526 Acc: 0.9304\n",
      "val Loss: 3.7093 Acc: 0.4735\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.2526 Acc: 0.9333\n",
      "val Loss: 3.6844 Acc: 0.4775\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.2365 Acc: 0.9275\n",
      "val Loss: 3.5997 Acc: 0.4696\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.2528 Acc: 0.9284\n",
      "val Loss: 3.4315 Acc: 0.4863\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.2260 Acc: 0.9294\n",
      "val Loss: 3.4859 Acc: 0.4980\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.2481 Acc: 0.9333\n",
      "val Loss: 3.6822 Acc: 0.4755\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.2271 Acc: 0.9265\n",
      "val Loss: 3.7619 Acc: 0.4824\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.2657 Acc: 0.9206\n",
      "val Loss: 3.4394 Acc: 0.4824\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.2411 Acc: 0.9314\n",
      "val Loss: 3.6095 Acc: 0.4745\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.2251 Acc: 0.9373\n",
      "val Loss: 3.4996 Acc: 0.4912\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.2274 Acc: 0.9373\n",
      "val Loss: 3.6188 Acc: 0.4912\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.2494 Acc: 0.9265\n",
      "val Loss: 3.6437 Acc: 0.4882\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.2674 Acc: 0.9255\n",
      "val Loss: 3.8681 Acc: 0.4765\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.2132 Acc: 0.9343\n",
      "val Loss: 3.4962 Acc: 0.5000\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.1827 Acc: 0.9500\n",
      "val Loss: 3.3931 Acc: 0.4902\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.1863 Acc: 0.9451\n",
      "val Loss: 3.7097 Acc: 0.4647\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.2251 Acc: 0.9422\n",
      "val Loss: 3.5575 Acc: 0.4843\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.1920 Acc: 0.9480\n",
      "val Loss: 3.5869 Acc: 0.4951\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.2669 Acc: 0.9216\n",
      "val Loss: 3.7659 Acc: 0.4873\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.2064 Acc: 0.9373\n",
      "val Loss: 3.5788 Acc: 0.4922\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.2289 Acc: 0.9275\n",
      "val Loss: 3.7677 Acc: 0.4686\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.2223 Acc: 0.9294\n",
      "val Loss: 4.0917 Acc: 0.4559\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.2547 Acc: 0.9225\n",
      "val Loss: 3.4315 Acc: 0.5127\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.1909 Acc: 0.9441\n",
      "val Loss: 3.6746 Acc: 0.4775\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.2565 Acc: 0.9245\n",
      "val Loss: 3.2990 Acc: 0.5147\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.1984 Acc: 0.9500\n",
      "val Loss: 3.9395 Acc: 0.4735\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.2030 Acc: 0.9422\n",
      "val Loss: 3.1920 Acc: 0.5196\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.2041 Acc: 0.9402\n",
      "val Loss: 3.5791 Acc: 0.4814\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.1433 Acc: 0.9637\n",
      "val Loss: 3.3980 Acc: 0.4980\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.1438 Acc: 0.9627\n",
      "val Loss: 3.2865 Acc: 0.5167\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.1830 Acc: 0.9520\n",
      "val Loss: 3.3839 Acc: 0.5157\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.1848 Acc: 0.9471\n",
      "val Loss: 3.5344 Acc: 0.5049\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.1994 Acc: 0.9451\n",
      "val Loss: 3.7443 Acc: 0.4873\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.2195 Acc: 0.9392\n",
      "val Loss: 3.8128 Acc: 0.4804\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.2170 Acc: 0.9431\n",
      "val Loss: 3.6214 Acc: 0.4961\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.2584 Acc: 0.9265\n",
      "val Loss: 3.6759 Acc: 0.5010\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.2036 Acc: 0.9431\n",
      "val Loss: 3.4050 Acc: 0.4980\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.1703 Acc: 0.9490\n",
      "val Loss: 3.4007 Acc: 0.4912\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.1909 Acc: 0.9431\n",
      "val Loss: 3.7264 Acc: 0.4931\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.2022 Acc: 0.9480\n",
      "val Loss: 3.5900 Acc: 0.4951\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.2054 Acc: 0.9373\n",
      "val Loss: 3.4971 Acc: 0.5108\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.1649 Acc: 0.9480\n",
      "val Loss: 3.4710 Acc: 0.5196\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.1917 Acc: 0.9510\n",
      "val Loss: 3.5507 Acc: 0.5049\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.1887 Acc: 0.9461\n",
      "val Loss: 3.2824 Acc: 0.5147\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.1764 Acc: 0.9539\n",
      "val Loss: 3.5367 Acc: 0.5000\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.1844 Acc: 0.9461\n",
      "val Loss: 3.3338 Acc: 0.4990\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.2095 Acc: 0.9471\n",
      "val Loss: 3.5892 Acc: 0.4951\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.1999 Acc: 0.9412\n",
      "val Loss: 3.5408 Acc: 0.4912\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.1650 Acc: 0.9510\n",
      "val Loss: 3.4062 Acc: 0.5088\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.1944 Acc: 0.9392\n",
      "val Loss: 3.4182 Acc: 0.5010\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.1182 Acc: 0.9637\n",
      "val Loss: 3.7257 Acc: 0.4971\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.2010 Acc: 0.9343\n",
      "val Loss: 3.7699 Acc: 0.4833\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.1785 Acc: 0.9510\n",
      "val Loss: 3.6733 Acc: 0.4980\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.1558 Acc: 0.9569\n",
      "val Loss: 3.9372 Acc: 0.4765\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.1507 Acc: 0.9559\n",
      "val Loss: 3.5078 Acc: 0.5098\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.1335 Acc: 0.9608\n",
      "val Loss: 3.6874 Acc: 0.4990\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.9500\n",
      "val Loss: 3.5239 Acc: 0.5167\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.1490 Acc: 0.9520\n",
      "val Loss: 3.6698 Acc: 0.5127\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.1720 Acc: 0.9510\n",
      "val Loss: 3.6226 Acc: 0.5147\n",
      "\n",
      "Training complete in 26m 32s\n",
      "Best val Acc: 0.519608\n",
      "----- Performing experiment with fixed lr=0.01 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6535 Acc: 0.0196\n",
      "val Loss: 4.2255 Acc: 0.1118\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 3.8918 Acc: 0.1706\n",
      "val Loss: 2.8557 Acc: 0.3490\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 2.7883 Acc: 0.3853\n",
      "val Loss: 2.1084 Acc: 0.4941\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 1.9877 Acc: 0.5451\n",
      "val Loss: 1.6396 Acc: 0.6049\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 1.5040 Acc: 0.6353\n",
      "val Loss: 1.5362 Acc: 0.6049\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 1.1264 Acc: 0.7343\n",
      "val Loss: 1.2352 Acc: 0.6990\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.8980 Acc: 0.8000\n",
      "val Loss: 1.0860 Acc: 0.7098\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.6885 Acc: 0.8392\n",
      "val Loss: 1.0197 Acc: 0.7382\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.6251 Acc: 0.8549\n",
      "val Loss: 0.9793 Acc: 0.7480\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.6186 Acc: 0.8598\n",
      "val Loss: 0.9167 Acc: 0.7686\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.4995 Acc: 0.8902\n",
      "val Loss: 0.8827 Acc: 0.7784\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.4431 Acc: 0.9020\n",
      "val Loss: 0.9743 Acc: 0.7549\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.4877 Acc: 0.8843\n",
      "val Loss: 0.9543 Acc: 0.7412\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.4208 Acc: 0.8980\n",
      "val Loss: 0.9568 Acc: 0.7490\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.3943 Acc: 0.9088\n",
      "val Loss: 0.9183 Acc: 0.7608\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.3478 Acc: 0.9088\n",
      "val Loss: 0.8956 Acc: 0.7667\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.3351 Acc: 0.9137\n",
      "val Loss: 0.9187 Acc: 0.7676\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.3085 Acc: 0.9255\n",
      "val Loss: 0.9120 Acc: 0.7676\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.2862 Acc: 0.9333\n",
      "val Loss: 0.9498 Acc: 0.7500\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.2495 Acc: 0.9402\n",
      "val Loss: 0.9972 Acc: 0.7598\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.3256 Acc: 0.9275\n",
      "val Loss: 0.9184 Acc: 0.7618\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.2706 Acc: 0.9314\n",
      "val Loss: 0.9351 Acc: 0.7676\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.3113 Acc: 0.9324\n",
      "val Loss: 0.8850 Acc: 0.7745\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.2549 Acc: 0.9441\n",
      "val Loss: 0.8549 Acc: 0.7863\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.2683 Acc: 0.9314\n",
      "val Loss: 0.9606 Acc: 0.7578\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.2679 Acc: 0.9314\n",
      "val Loss: 0.9476 Acc: 0.7647\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.2703 Acc: 0.9353\n",
      "val Loss: 0.8371 Acc: 0.7833\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.2684 Acc: 0.9382\n",
      "val Loss: 0.8780 Acc: 0.7833\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.2319 Acc: 0.9461\n",
      "val Loss: 0.8606 Acc: 0.7873\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.2271 Acc: 0.9471\n",
      "val Loss: 0.8427 Acc: 0.7833\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.2447 Acc: 0.9422\n",
      "val Loss: 0.8703 Acc: 0.7716\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.2019 Acc: 0.9480\n",
      "val Loss: 0.8136 Acc: 0.7961\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.1975 Acc: 0.9490\n",
      "val Loss: 0.8630 Acc: 0.7735\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.1833 Acc: 0.9559\n",
      "val Loss: 0.8066 Acc: 0.7961\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.1863 Acc: 0.9490\n",
      "val Loss: 0.8058 Acc: 0.7980\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.1897 Acc: 0.9520\n",
      "val Loss: 0.7909 Acc: 0.7971\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.2230 Acc: 0.9382\n",
      "val Loss: 0.9319 Acc: 0.7618\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.2080 Acc: 0.9441\n",
      "val Loss: 0.9643 Acc: 0.7647\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.1903 Acc: 0.9549\n",
      "val Loss: 0.8941 Acc: 0.7794\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.1582 Acc: 0.9647\n",
      "val Loss: 0.7836 Acc: 0.8000\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.2118 Acc: 0.9451\n",
      "val Loss: 0.7886 Acc: 0.8020\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.2027 Acc: 0.9490\n",
      "val Loss: 0.8985 Acc: 0.7696\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.2023 Acc: 0.9529\n",
      "val Loss: 0.9072 Acc: 0.7745\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.1558 Acc: 0.9578\n",
      "val Loss: 0.8512 Acc: 0.7892\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.1878 Acc: 0.9500\n",
      "val Loss: 0.9067 Acc: 0.7873\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.2235 Acc: 0.9422\n",
      "val Loss: 0.8608 Acc: 0.7971\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.1827 Acc: 0.9520\n",
      "val Loss: 0.8987 Acc: 0.7745\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.1603 Acc: 0.9569\n",
      "val Loss: 0.8393 Acc: 0.7990\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.1824 Acc: 0.9520\n",
      "val Loss: 0.8874 Acc: 0.7755\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.2245 Acc: 0.9402\n",
      "val Loss: 0.9322 Acc: 0.7775\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.2070 Acc: 0.9461\n",
      "val Loss: 0.8474 Acc: 0.7980\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.1970 Acc: 0.9490\n",
      "val Loss: 0.8773 Acc: 0.7824\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.1359 Acc: 0.9676\n",
      "val Loss: 0.8410 Acc: 0.7951\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.1690 Acc: 0.9539\n",
      "val Loss: 0.9226 Acc: 0.7814\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.2067 Acc: 0.9451\n",
      "val Loss: 0.8882 Acc: 0.7951\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.1611 Acc: 0.9569\n",
      "val Loss: 0.8185 Acc: 0.8010\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.1533 Acc: 0.9618\n",
      "val Loss: 0.8919 Acc: 0.7873\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.1513 Acc: 0.9549\n",
      "val Loss: 0.9020 Acc: 0.7882\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.1412 Acc: 0.9627\n",
      "val Loss: 0.9397 Acc: 0.7873\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.1849 Acc: 0.9500\n",
      "val Loss: 0.8706 Acc: 0.8049\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.1615 Acc: 0.9539\n",
      "val Loss: 0.8590 Acc: 0.7863\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.1412 Acc: 0.9608\n",
      "val Loss: 0.9284 Acc: 0.7725\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.1583 Acc: 0.9578\n",
      "val Loss: 0.8657 Acc: 0.7882\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.1539 Acc: 0.9618\n",
      "val Loss: 0.8341 Acc: 0.7853\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.1560 Acc: 0.9539\n",
      "val Loss: 0.8384 Acc: 0.7951\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.1303 Acc: 0.9627\n",
      "val Loss: 0.8228 Acc: 0.7951\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.1751 Acc: 0.9510\n",
      "val Loss: 0.8517 Acc: 0.8059\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.1158 Acc: 0.9706\n",
      "val Loss: 0.9028 Acc: 0.7912\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.1247 Acc: 0.9696\n",
      "val Loss: 0.8860 Acc: 0.7941\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.1291 Acc: 0.9667\n",
      "val Loss: 0.8688 Acc: 0.8098\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.1083 Acc: 0.9735\n",
      "val Loss: 0.8919 Acc: 0.8118\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.1105 Acc: 0.9696\n",
      "val Loss: 0.9020 Acc: 0.7931\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.1322 Acc: 0.9706\n",
      "val Loss: 0.9909 Acc: 0.7696\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.1347 Acc: 0.9618\n",
      "val Loss: 0.9382 Acc: 0.7843\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.1670 Acc: 0.9539\n",
      "val Loss: 0.9405 Acc: 0.7824\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.1615 Acc: 0.9510\n",
      "val Loss: 0.9438 Acc: 0.7765\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.1284 Acc: 0.9706\n",
      "val Loss: 0.9022 Acc: 0.7892\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.1924 Acc: 0.9480\n",
      "val Loss: 0.9512 Acc: 0.7843\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.1622 Acc: 0.9569\n",
      "val Loss: 0.9486 Acc: 0.7843\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.1284 Acc: 0.9686\n",
      "val Loss: 0.9317 Acc: 0.7902\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.1330 Acc: 0.9686\n",
      "val Loss: 0.9510 Acc: 0.7941\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.1171 Acc: 0.9676\n",
      "val Loss: 0.8280 Acc: 0.7961\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.1026 Acc: 0.9735\n",
      "val Loss: 0.8234 Acc: 0.8049\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.1035 Acc: 0.9735\n",
      "val Loss: 0.8953 Acc: 0.7745\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.0987 Acc: 0.9745\n",
      "val Loss: 0.8729 Acc: 0.7833\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.0980 Acc: 0.9716\n",
      "val Loss: 0.8394 Acc: 0.8049\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.1200 Acc: 0.9686\n",
      "val Loss: 0.8471 Acc: 0.8088\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.1150 Acc: 0.9706\n",
      "val Loss: 0.8261 Acc: 0.8020\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.0916 Acc: 0.9765\n",
      "val Loss: 0.8530 Acc: 0.8010\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.1133 Acc: 0.9706\n",
      "val Loss: 0.8710 Acc: 0.8039\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.0902 Acc: 0.9755\n",
      "val Loss: 0.8878 Acc: 0.7902\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.0904 Acc: 0.9784\n",
      "val Loss: 0.9202 Acc: 0.7863\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.0986 Acc: 0.9765\n",
      "val Loss: 0.8792 Acc: 0.7941\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.0891 Acc: 0.9735\n",
      "val Loss: 0.8462 Acc: 0.8020\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.0834 Acc: 0.9775\n",
      "val Loss: 0.8161 Acc: 0.7990\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.1207 Acc: 0.9667\n",
      "val Loss: 0.8602 Acc: 0.7941\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.1185 Acc: 0.9657\n",
      "val Loss: 0.9602 Acc: 0.7902\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.1437 Acc: 0.9627\n",
      "val Loss: 0.8959 Acc: 0.8029\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.1163 Acc: 0.9667\n",
      "val Loss: 0.8369 Acc: 0.8098\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.1405 Acc: 0.9588\n",
      "val Loss: 0.8714 Acc: 0.7971\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.1176 Acc: 0.9696\n",
      "val Loss: 0.8425 Acc: 0.7941\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.1223 Acc: 0.9647\n",
      "val Loss: 0.8530 Acc: 0.7882\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.1247 Acc: 0.9578\n",
      "val Loss: 0.8856 Acc: 0.7990\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.1159 Acc: 0.9637\n",
      "val Loss: 0.9700 Acc: 0.7804\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.9716\n",
      "val Loss: 0.9461 Acc: 0.7725\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.1146 Acc: 0.9686\n",
      "val Loss: 0.9951 Acc: 0.7706\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.1307 Acc: 0.9657\n",
      "val Loss: 0.8993 Acc: 0.7775\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9676\n",
      "val Loss: 0.9888 Acc: 0.7853\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.1062 Acc: 0.9686\n",
      "val Loss: 0.8805 Acc: 0.7971\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.0814 Acc: 0.9735\n",
      "val Loss: 0.8543 Acc: 0.7971\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.0934 Acc: 0.9735\n",
      "val Loss: 0.8965 Acc: 0.7971\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.9676\n",
      "val Loss: 0.9464 Acc: 0.7873\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.1087 Acc: 0.9725\n",
      "val Loss: 0.9104 Acc: 0.7755\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.0691 Acc: 0.9814\n",
      "val Loss: 0.9380 Acc: 0.7833\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.0837 Acc: 0.9775\n",
      "val Loss: 0.8992 Acc: 0.7951\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.0623 Acc: 0.9804\n",
      "val Loss: 0.9035 Acc: 0.7931\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.0843 Acc: 0.9833\n",
      "val Loss: 0.8910 Acc: 0.7931\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.1011 Acc: 0.9696\n",
      "val Loss: 0.9090 Acc: 0.8000\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.1111 Acc: 0.9676\n",
      "val Loss: 0.9687 Acc: 0.7804\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.1038 Acc: 0.9735\n",
      "val Loss: 0.9768 Acc: 0.7804\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.0918 Acc: 0.9745\n",
      "val Loss: 0.9067 Acc: 0.7853\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.0999 Acc: 0.9755\n",
      "val Loss: 0.9458 Acc: 0.7667\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.1120 Acc: 0.9686\n",
      "val Loss: 0.9427 Acc: 0.7814\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.0942 Acc: 0.9716\n",
      "val Loss: 0.9125 Acc: 0.7814\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.1169 Acc: 0.9735\n",
      "val Loss: 0.9099 Acc: 0.7804\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.0810 Acc: 0.9784\n",
      "val Loss: 0.9144 Acc: 0.7990\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.0933 Acc: 0.9745\n",
      "val Loss: 0.8732 Acc: 0.8010\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.0840 Acc: 0.9735\n",
      "val Loss: 0.9325 Acc: 0.7843\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.0803 Acc: 0.9843\n",
      "val Loss: 0.8639 Acc: 0.8049\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.0737 Acc: 0.9824\n",
      "val Loss: 0.8860 Acc: 0.8098\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.0912 Acc: 0.9824\n",
      "val Loss: 0.8698 Acc: 0.8059\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.0887 Acc: 0.9765\n",
      "val Loss: 0.9184 Acc: 0.7882\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.9725\n",
      "val Loss: 0.9169 Acc: 0.7843\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.1217 Acc: 0.9647\n",
      "val Loss: 0.9421 Acc: 0.7873\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.1191 Acc: 0.9667\n",
      "val Loss: 0.8454 Acc: 0.8059\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.0851 Acc: 0.9735\n",
      "val Loss: 0.9001 Acc: 0.8000\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.9716\n",
      "val Loss: 0.9537 Acc: 0.7765\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.9745\n",
      "val Loss: 0.9460 Acc: 0.7843\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.0873 Acc: 0.9794\n",
      "val Loss: 0.9132 Acc: 0.7980\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.0892 Acc: 0.9804\n",
      "val Loss: 0.9264 Acc: 0.8020\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.0942 Acc: 0.9755\n",
      "val Loss: 0.9117 Acc: 0.8010\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.0804 Acc: 0.9833\n",
      "val Loss: 0.9318 Acc: 0.8039\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.0962 Acc: 0.9735\n",
      "val Loss: 0.9076 Acc: 0.7990\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.0709 Acc: 0.9775\n",
      "val Loss: 0.8777 Acc: 0.8059\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.0949 Acc: 0.9775\n",
      "val Loss: 0.8631 Acc: 0.7941\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.9833\n",
      "val Loss: 0.8163 Acc: 0.8078\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.0855 Acc: 0.9784\n",
      "val Loss: 0.8598 Acc: 0.8069\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9725\n",
      "val Loss: 0.8324 Acc: 0.8078\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.1040 Acc: 0.9706\n",
      "val Loss: 0.8894 Acc: 0.7931\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.0806 Acc: 0.9784\n",
      "val Loss: 0.8633 Acc: 0.8010\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.0669 Acc: 0.9804\n",
      "val Loss: 0.8433 Acc: 0.8020\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.0660 Acc: 0.9843\n",
      "val Loss: 0.8534 Acc: 0.8049\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.0728 Acc: 0.9794\n",
      "val Loss: 0.8677 Acc: 0.8000\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.0726 Acc: 0.9814\n",
      "val Loss: 0.8342 Acc: 0.8078\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.1060 Acc: 0.9696\n",
      "val Loss: 0.8034 Acc: 0.8078\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.0684 Acc: 0.9794\n",
      "val Loss: 0.8457 Acc: 0.8088\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.0720 Acc: 0.9794\n",
      "val Loss: 0.8542 Acc: 0.8010\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.0586 Acc: 0.9853\n",
      "val Loss: 0.8311 Acc: 0.8049\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.0613 Acc: 0.9814\n",
      "val Loss: 0.8499 Acc: 0.8059\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.0788 Acc: 0.9775\n",
      "val Loss: 0.8651 Acc: 0.8069\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.9784\n",
      "val Loss: 0.9157 Acc: 0.7971\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.0779 Acc: 0.9814\n",
      "val Loss: 0.8370 Acc: 0.8088\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.0555 Acc: 0.9843\n",
      "val Loss: 0.7870 Acc: 0.8127\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.0645 Acc: 0.9824\n",
      "val Loss: 0.8713 Acc: 0.7971\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.1015 Acc: 0.9696\n",
      "val Loss: 0.9098 Acc: 0.7990\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.0911 Acc: 0.9735\n",
      "val Loss: 0.9173 Acc: 0.8000\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.0935 Acc: 0.9716\n",
      "val Loss: 0.8590 Acc: 0.8078\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.0846 Acc: 0.9794\n",
      "val Loss: 0.9291 Acc: 0.7912\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.0858 Acc: 0.9735\n",
      "val Loss: 0.9659 Acc: 0.7882\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.0930 Acc: 0.9725\n",
      "val Loss: 1.0006 Acc: 0.7725\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.1077 Acc: 0.9696\n",
      "val Loss: 0.9939 Acc: 0.7725\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.0974 Acc: 0.9725\n",
      "val Loss: 0.9288 Acc: 0.7971\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.0871 Acc: 0.9725\n",
      "val Loss: 0.8784 Acc: 0.7980\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.0674 Acc: 0.9843\n",
      "val Loss: 0.9035 Acc: 0.7951\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.1042 Acc: 0.9765\n",
      "val Loss: 0.9110 Acc: 0.7882\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.0608 Acc: 0.9853\n",
      "val Loss: 0.8832 Acc: 0.7951\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.0963 Acc: 0.9716\n",
      "val Loss: 0.9273 Acc: 0.7843\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.0933 Acc: 0.9735\n",
      "val Loss: 0.9509 Acc: 0.7853\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.0799 Acc: 0.9814\n",
      "val Loss: 0.9393 Acc: 0.7843\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.0792 Acc: 0.9765\n",
      "val Loss: 0.9259 Acc: 0.7843\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.0967 Acc: 0.9745\n",
      "val Loss: 0.8909 Acc: 0.7980\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.1013 Acc: 0.9716\n",
      "val Loss: 0.9797 Acc: 0.7735\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.0998 Acc: 0.9745\n",
      "val Loss: 0.8821 Acc: 0.7892\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.0682 Acc: 0.9784\n",
      "val Loss: 0.9600 Acc: 0.7873\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.0727 Acc: 0.9755\n",
      "val Loss: 0.9821 Acc: 0.7755\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.0656 Acc: 0.9794\n",
      "val Loss: 0.9541 Acc: 0.7755\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.0742 Acc: 0.9794\n",
      "val Loss: 0.9153 Acc: 0.7882\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.0888 Acc: 0.9765\n",
      "val Loss: 0.9702 Acc: 0.7804\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.0708 Acc: 0.9833\n",
      "val Loss: 0.9473 Acc: 0.7833\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.0873 Acc: 0.9755\n",
      "val Loss: 0.9878 Acc: 0.7882\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.0674 Acc: 0.9784\n",
      "val Loss: 0.9714 Acc: 0.7784\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.0975 Acc: 0.9775\n",
      "val Loss: 0.9702 Acc: 0.7922\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.0668 Acc: 0.9794\n",
      "val Loss: 0.9936 Acc: 0.7961\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.0732 Acc: 0.9804\n",
      "val Loss: 0.9708 Acc: 0.7882\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.0870 Acc: 0.9755\n",
      "val Loss: 0.9667 Acc: 0.7990\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.0665 Acc: 0.9784\n",
      "val Loss: 0.9840 Acc: 0.7931\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.0825 Acc: 0.9794\n",
      "val Loss: 0.9631 Acc: 0.7971\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.0612 Acc: 0.9853\n",
      "val Loss: 0.9282 Acc: 0.7931\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.0708 Acc: 0.9784\n",
      "val Loss: 0.9146 Acc: 0.7912\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.0899 Acc: 0.9775\n",
      "val Loss: 0.9275 Acc: 0.7931\n",
      "\n",
      "Training complete in 26m 32s\n",
      "Best val Acc: 0.812745\n"
     ]
    }
   ],
   "source": [
    "LR_SET = [0.1, 0.01]\n",
    "\n",
    "for lr in LR_SET:\n",
    "    print(f'----- Performing experiment with fixed lr={lr} -----\\n')\n",
    "    \n",
    "    # Load pretrained Resnet50 and change final layer\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "    # Set up criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 1000 epochs (never going to step, setting artifically high for fixing lr)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1000, gamma=0.1)\n",
    "\n",
    "    # Send model to device\n",
    "    model_ft = model_ft.to(device)\n",
    "    \n",
    "    # Train model\n",
    "    model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for the three learning rate approaches are below in the table.\n",
    "\n",
    "| Learning Rate Setting | Best Validation Accuracy |\n",
    "| ----------------------|--------------------------|\n",
    "| 0.01 with decay       | 0.803                    |\n",
    "| 0.1 fixed             | 0.520                    |\n",
    "| 0.01 fixed            | 0.813                    |\n",
    "\n",
    "The fixed learning rate of 0.01 for all 200 epochs gave the best validation accuracy for the three experiments.\n",
    "\n",
    "#### Q2\n",
    "\n",
    "* a: (8 points)\n",
    "\n",
    "Treating `ResNet50` as a feature extractor and only training the final layer with various learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Performing experiment with fixed lr=0.001 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6785 Acc: 0.0147\n",
      "val Loss: 4.6396 Acc: 0.0127\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.6253 Acc: 0.0127\n",
      "val Loss: 4.5816 Acc: 0.0255\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.5631 Acc: 0.0284\n",
      "val Loss: 4.5245 Acc: 0.0441\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.5190 Acc: 0.0412\n",
      "val Loss: 4.4712 Acc: 0.0745\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.4662 Acc: 0.0755\n",
      "val Loss: 4.4178 Acc: 0.1069\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.4225 Acc: 0.0912\n",
      "val Loss: 4.3649 Acc: 0.1441\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 4.3570 Acc: 0.1490\n",
      "val Loss: 4.3146 Acc: 0.1755\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 4.3206 Acc: 0.1961\n",
      "val Loss: 4.2642 Acc: 0.2059\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 4.2621 Acc: 0.2343\n",
      "val Loss: 4.2143 Acc: 0.2461\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 4.2095 Acc: 0.2392\n",
      "val Loss: 4.1662 Acc: 0.2735\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 4.1708 Acc: 0.2853\n",
      "val Loss: 4.1173 Acc: 0.3029\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 4.1370 Acc: 0.3000\n",
      "val Loss: 4.0754 Acc: 0.3216\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 4.0773 Acc: 0.3422\n",
      "val Loss: 4.0217 Acc: 0.3431\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 4.0473 Acc: 0.3647\n",
      "val Loss: 3.9758 Acc: 0.3549\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 3.9868 Acc: 0.3853\n",
      "val Loss: 3.9305 Acc: 0.3657\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 3.9625 Acc: 0.3804\n",
      "val Loss: 3.8916 Acc: 0.3892\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 3.9057 Acc: 0.3971\n",
      "val Loss: 3.8446 Acc: 0.4196\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 3.8594 Acc: 0.4529\n",
      "val Loss: 3.8022 Acc: 0.4147\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 3.8225 Acc: 0.4382\n",
      "val Loss: 3.7600 Acc: 0.4353\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 3.7966 Acc: 0.4529\n",
      "val Loss: 3.7217 Acc: 0.4559\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 3.7670 Acc: 0.4382\n",
      "val Loss: 3.6766 Acc: 0.4549\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 3.7112 Acc: 0.4716\n",
      "val Loss: 3.6358 Acc: 0.4696\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 3.6610 Acc: 0.4725\n",
      "val Loss: 3.5992 Acc: 0.4608\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 3.6250 Acc: 0.4892\n",
      "val Loss: 3.5590 Acc: 0.4686\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 3.5829 Acc: 0.4961\n",
      "val Loss: 3.5194 Acc: 0.4931\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 3.5724 Acc: 0.4873\n",
      "val Loss: 3.4879 Acc: 0.4951\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 3.5307 Acc: 0.5088\n",
      "val Loss: 3.4473 Acc: 0.5010\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 3.4682 Acc: 0.5186\n",
      "val Loss: 3.4181 Acc: 0.5049\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 3.4580 Acc: 0.5186\n",
      "val Loss: 3.3808 Acc: 0.4951\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 3.3998 Acc: 0.5412\n",
      "val Loss: 3.3397 Acc: 0.5127\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 3.3858 Acc: 0.5343\n",
      "val Loss: 3.3068 Acc: 0.5049\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 3.3589 Acc: 0.5265\n",
      "val Loss: 3.2781 Acc: 0.5108\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 3.3057 Acc: 0.5510\n",
      "val Loss: 3.2438 Acc: 0.5235\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 3.3018 Acc: 0.5422\n",
      "val Loss: 3.2249 Acc: 0.5225\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 3.2481 Acc: 0.5431\n",
      "val Loss: 3.1865 Acc: 0.5225\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 3.2440 Acc: 0.5373\n",
      "val Loss: 3.1503 Acc: 0.5402\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 3.1851 Acc: 0.5431\n",
      "val Loss: 3.1142 Acc: 0.5363\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 3.1600 Acc: 0.5676\n",
      "val Loss: 3.0943 Acc: 0.5382\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 3.0908 Acc: 0.6000\n",
      "val Loss: 3.0607 Acc: 0.5441\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 3.1136 Acc: 0.5578\n",
      "val Loss: 3.0366 Acc: 0.5539\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 3.0684 Acc: 0.5961\n",
      "val Loss: 3.0099 Acc: 0.5461\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 3.0535 Acc: 0.5627\n",
      "val Loss: 2.9838 Acc: 0.5588\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 2.9852 Acc: 0.6000\n",
      "val Loss: 2.9471 Acc: 0.5529\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 3.0238 Acc: 0.5706\n",
      "val Loss: 2.9315 Acc: 0.5657\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 2.9405 Acc: 0.6245\n",
      "val Loss: 2.9010 Acc: 0.5627\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 2.9553 Acc: 0.5863\n",
      "val Loss: 2.8781 Acc: 0.5637\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 2.9092 Acc: 0.5843\n",
      "val Loss: 2.8521 Acc: 0.5765\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 2.8975 Acc: 0.5853\n",
      "val Loss: 2.8434 Acc: 0.5716\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 2.8776 Acc: 0.5971\n",
      "val Loss: 2.8084 Acc: 0.5637\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 2.8474 Acc: 0.6029\n",
      "val Loss: 2.7833 Acc: 0.5657\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 2.8232 Acc: 0.5951\n",
      "val Loss: 2.7598 Acc: 0.5814\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 2.8043 Acc: 0.6127\n",
      "val Loss: 2.7509 Acc: 0.5745\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 2.7742 Acc: 0.6049\n",
      "val Loss: 2.7249 Acc: 0.5784\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 2.7621 Acc: 0.6137\n",
      "val Loss: 2.7020 Acc: 0.5853\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 2.7341 Acc: 0.6167\n",
      "val Loss: 2.6768 Acc: 0.5814\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 2.7224 Acc: 0.6069\n",
      "val Loss: 2.6626 Acc: 0.5824\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 2.6762 Acc: 0.6186\n",
      "val Loss: 2.6368 Acc: 0.5931\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 2.6908 Acc: 0.6225\n",
      "val Loss: 2.6164 Acc: 0.5873\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 2.6495 Acc: 0.6471\n",
      "val Loss: 2.5969 Acc: 0.5804\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 2.6298 Acc: 0.6235\n",
      "val Loss: 2.5721 Acc: 0.5922\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 2.5988 Acc: 0.6245\n",
      "val Loss: 2.5702 Acc: 0.5912\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 2.6115 Acc: 0.6451\n",
      "val Loss: 2.5739 Acc: 0.5902\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 2.6147 Acc: 0.6392\n",
      "val Loss: 2.5774 Acc: 0.5882\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 2.5960 Acc: 0.6167\n",
      "val Loss: 2.5658 Acc: 0.5873\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 2.6088 Acc: 0.6431\n",
      "val Loss: 2.5670 Acc: 0.5931\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 2.5992 Acc: 0.6324\n",
      "val Loss: 2.5660 Acc: 0.5912\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 2.6220 Acc: 0.6137\n",
      "val Loss: 2.5690 Acc: 0.5922\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 2.6198 Acc: 0.6196\n",
      "val Loss: 2.5676 Acc: 0.5902\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 2.6002 Acc: 0.6196\n",
      "val Loss: 2.5614 Acc: 0.5912\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 2.6124 Acc: 0.6225\n",
      "val Loss: 2.5680 Acc: 0.5902\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 2.5829 Acc: 0.6471\n",
      "val Loss: 2.5586 Acc: 0.5990\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 2.5804 Acc: 0.6373\n",
      "val Loss: 2.5536 Acc: 0.5990\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 2.5976 Acc: 0.6235\n",
      "val Loss: 2.5523 Acc: 0.5961\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 2.6177 Acc: 0.6167\n",
      "val Loss: 2.5554 Acc: 0.5902\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 2.6381 Acc: 0.6118\n",
      "val Loss: 2.5522 Acc: 0.5912\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 2.5715 Acc: 0.6392\n",
      "val Loss: 2.5422 Acc: 0.5980\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 2.5930 Acc: 0.6196\n",
      "val Loss: 2.5500 Acc: 0.5912\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 2.5747 Acc: 0.6373\n",
      "val Loss: 2.5468 Acc: 0.5980\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 2.5767 Acc: 0.6147\n",
      "val Loss: 2.5483 Acc: 0.5951\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 2.6146 Acc: 0.6167\n",
      "val Loss: 2.5409 Acc: 0.5980\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 2.5857 Acc: 0.6363\n",
      "val Loss: 2.5385 Acc: 0.5971\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 2.5666 Acc: 0.6245\n",
      "val Loss: 2.5357 Acc: 0.5980\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 2.5822 Acc: 0.6363\n",
      "val Loss: 2.5367 Acc: 0.6020\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 2.5768 Acc: 0.6186\n",
      "val Loss: 2.5358 Acc: 0.5961\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 2.5833 Acc: 0.6500\n",
      "val Loss: 2.5353 Acc: 0.5980\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 2.5749 Acc: 0.6343\n",
      "val Loss: 2.5316 Acc: 0.5931\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 2.5821 Acc: 0.6245\n",
      "val Loss: 2.5341 Acc: 0.5971\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 2.5655 Acc: 0.6275\n",
      "val Loss: 2.5225 Acc: 0.6029\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 2.5446 Acc: 0.6471\n",
      "val Loss: 2.5213 Acc: 0.6020\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 2.5689 Acc: 0.6441\n",
      "val Loss: 2.5243 Acc: 0.5971\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 2.5374 Acc: 0.6353\n",
      "val Loss: 2.5135 Acc: 0.5980\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 2.5171 Acc: 0.6608\n",
      "val Loss: 2.5222 Acc: 0.6000\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 2.5368 Acc: 0.6373\n",
      "val Loss: 2.5179 Acc: 0.5990\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 2.5337 Acc: 0.6461\n",
      "val Loss: 2.5105 Acc: 0.6020\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 2.5646 Acc: 0.6314\n",
      "val Loss: 2.5120 Acc: 0.6069\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 2.5858 Acc: 0.6314\n",
      "val Loss: 2.5138 Acc: 0.5980\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 2.5153 Acc: 0.6520\n",
      "val Loss: 2.5112 Acc: 0.6049\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 2.5257 Acc: 0.6353\n",
      "val Loss: 2.5052 Acc: 0.6059\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 2.5102 Acc: 0.6608\n",
      "val Loss: 2.5055 Acc: 0.6059\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 2.5395 Acc: 0.6314\n",
      "val Loss: 2.5055 Acc: 0.5980\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 2.5571 Acc: 0.6402\n",
      "val Loss: 2.5055 Acc: 0.6049\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 2.5433 Acc: 0.6314\n",
      "val Loss: 2.5032 Acc: 0.6049\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 2.5581 Acc: 0.6324\n",
      "val Loss: 2.5045 Acc: 0.6010\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 2.5425 Acc: 0.6392\n",
      "val Loss: 2.5036 Acc: 0.6059\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 2.5300 Acc: 0.6461\n",
      "val Loss: 2.4945 Acc: 0.6049\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 2.5357 Acc: 0.6216\n",
      "val Loss: 2.5061 Acc: 0.6010\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 2.5239 Acc: 0.6451\n",
      "val Loss: 2.4923 Acc: 0.6059\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 2.5006 Acc: 0.6608\n",
      "val Loss: 2.4993 Acc: 0.6029\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 2.5097 Acc: 0.6422\n",
      "val Loss: 2.4866 Acc: 0.6039\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 2.5278 Acc: 0.6500\n",
      "val Loss: 2.4917 Acc: 0.6069\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 2.5264 Acc: 0.6412\n",
      "val Loss: 2.4840 Acc: 0.6069\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 2.5223 Acc: 0.6549\n",
      "val Loss: 2.4780 Acc: 0.6029\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 2.5125 Acc: 0.6373\n",
      "val Loss: 2.4846 Acc: 0.5990\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 2.6108 Acc: 0.6059\n",
      "val Loss: 2.4908 Acc: 0.5990\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 2.5036 Acc: 0.6353\n",
      "val Loss: 2.4863 Acc: 0.6088\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 2.5662 Acc: 0.6078\n",
      "val Loss: 2.4863 Acc: 0.6039\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 2.4893 Acc: 0.6559\n",
      "val Loss: 2.4732 Acc: 0.6059\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 2.5219 Acc: 0.6471\n",
      "val Loss: 2.4752 Acc: 0.6108\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 2.5121 Acc: 0.6275\n",
      "val Loss: 2.4753 Acc: 0.6098\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 2.5205 Acc: 0.6441\n",
      "val Loss: 2.4758 Acc: 0.6059\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 2.5001 Acc: 0.6225\n",
      "val Loss: 2.4765 Acc: 0.6088\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 2.5129 Acc: 0.6245\n",
      "val Loss: 2.4690 Acc: 0.6088\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 2.5009 Acc: 0.6441\n",
      "val Loss: 2.4696 Acc: 0.6029\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 2.5148 Acc: 0.6392\n",
      "val Loss: 2.4802 Acc: 0.5990\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 2.4891 Acc: 0.6500\n",
      "val Loss: 2.4765 Acc: 0.6049\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 2.4829 Acc: 0.6559\n",
      "val Loss: 2.4794 Acc: 0.6059\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 2.5295 Acc: 0.6245\n",
      "val Loss: 2.4731 Acc: 0.6039\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 2.4569 Acc: 0.6735\n",
      "val Loss: 2.4629 Acc: 0.6137\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 2.4588 Acc: 0.6422\n",
      "val Loss: 2.4609 Acc: 0.6088\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 2.5348 Acc: 0.6412\n",
      "val Loss: 2.4735 Acc: 0.6059\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 2.4560 Acc: 0.6676\n",
      "val Loss: 2.4612 Acc: 0.6137\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 2.5180 Acc: 0.6284\n",
      "val Loss: 2.4723 Acc: 0.6049\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 2.5202 Acc: 0.6353\n",
      "val Loss: 2.4776 Acc: 0.6088\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 2.4624 Acc: 0.6598\n",
      "val Loss: 2.4680 Acc: 0.6118\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 2.4851 Acc: 0.6627\n",
      "val Loss: 2.4699 Acc: 0.6088\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 2.4820 Acc: 0.6480\n",
      "val Loss: 2.4748 Acc: 0.6049\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 2.5066 Acc: 0.6569\n",
      "val Loss: 2.4819 Acc: 0.6059\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 2.5121 Acc: 0.6353\n",
      "val Loss: 2.4750 Acc: 0.6078\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 2.4620 Acc: 0.6598\n",
      "val Loss: 2.4719 Acc: 0.6098\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 2.4814 Acc: 0.6647\n",
      "val Loss: 2.4726 Acc: 0.6069\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 2.4939 Acc: 0.6559\n",
      "val Loss: 2.4702 Acc: 0.6049\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 2.5012 Acc: 0.6363\n",
      "val Loss: 2.4643 Acc: 0.6137\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 2.4747 Acc: 0.6412\n",
      "val Loss: 2.4706 Acc: 0.6088\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 2.4927 Acc: 0.6520\n",
      "val Loss: 2.4613 Acc: 0.6127\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 2.4830 Acc: 0.6471\n",
      "val Loss: 2.4667 Acc: 0.6049\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 2.4738 Acc: 0.6745\n",
      "val Loss: 2.4578 Acc: 0.6088\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 2.4938 Acc: 0.6647\n",
      "val Loss: 2.4714 Acc: 0.6118\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 2.4787 Acc: 0.6637\n",
      "val Loss: 2.4652 Acc: 0.6069\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 2.5016 Acc: 0.6363\n",
      "val Loss: 2.4711 Acc: 0.6088\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 2.4993 Acc: 0.6461\n",
      "val Loss: 2.4599 Acc: 0.6118\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 2.4774 Acc: 0.6422\n",
      "val Loss: 2.4681 Acc: 0.6059\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 2.5320 Acc: 0.6235\n",
      "val Loss: 2.4763 Acc: 0.6039\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 2.4967 Acc: 0.6392\n",
      "val Loss: 2.4631 Acc: 0.6108\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 2.4981 Acc: 0.6363\n",
      "val Loss: 2.4624 Acc: 0.6088\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 2.4938 Acc: 0.6451\n",
      "val Loss: 2.4701 Acc: 0.6039\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 2.4969 Acc: 0.6431\n",
      "val Loss: 2.4681 Acc: 0.6029\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 2.4641 Acc: 0.6608\n",
      "val Loss: 2.4593 Acc: 0.6088\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 2.5052 Acc: 0.6686\n",
      "val Loss: 2.4723 Acc: 0.6010\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 2.4991 Acc: 0.6363\n",
      "val Loss: 2.4635 Acc: 0.6039\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 2.4834 Acc: 0.6559\n",
      "val Loss: 2.4642 Acc: 0.6137\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 2.4939 Acc: 0.6392\n",
      "val Loss: 2.4648 Acc: 0.6088\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 2.4670 Acc: 0.6627\n",
      "val Loss: 2.4667 Acc: 0.6108\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 2.4582 Acc: 0.6402\n",
      "val Loss: 2.4698 Acc: 0.6078\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 2.4771 Acc: 0.6529\n",
      "val Loss: 2.4698 Acc: 0.6049\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 2.4889 Acc: 0.6373\n",
      "val Loss: 2.4695 Acc: 0.6078\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 2.5065 Acc: 0.6500\n",
      "val Loss: 2.4692 Acc: 0.6108\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 2.4831 Acc: 0.6373\n",
      "val Loss: 2.4650 Acc: 0.6078\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 2.5031 Acc: 0.6333\n",
      "val Loss: 2.4734 Acc: 0.6078\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 2.4925 Acc: 0.6480\n",
      "val Loss: 2.4678 Acc: 0.6098\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 2.4819 Acc: 0.6363\n",
      "val Loss: 2.4583 Acc: 0.6098\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 2.4960 Acc: 0.6471\n",
      "val Loss: 2.4673 Acc: 0.6098\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 2.5166 Acc: 0.6392\n",
      "val Loss: 2.4589 Acc: 0.6127\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 2.4743 Acc: 0.6471\n",
      "val Loss: 2.4646 Acc: 0.6127\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 2.4695 Acc: 0.6461\n",
      "val Loss: 2.4567 Acc: 0.6088\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 2.4806 Acc: 0.6422\n",
      "val Loss: 2.4600 Acc: 0.6098\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 2.4763 Acc: 0.6431\n",
      "val Loss: 2.4545 Acc: 0.6137\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 2.4923 Acc: 0.6441\n",
      "val Loss: 2.4596 Acc: 0.6078\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 2.4888 Acc: 0.6569\n",
      "val Loss: 2.4613 Acc: 0.6069\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 2.5311 Acc: 0.6412\n",
      "val Loss: 2.4635 Acc: 0.6010\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 2.5046 Acc: 0.6422\n",
      "val Loss: 2.4554 Acc: 0.6078\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 2.4944 Acc: 0.6412\n",
      "val Loss: 2.4550 Acc: 0.6127\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 2.4803 Acc: 0.6539\n",
      "val Loss: 2.4610 Acc: 0.6059\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 2.4983 Acc: 0.6549\n",
      "val Loss: 2.4652 Acc: 0.6039\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 2.5404 Acc: 0.6422\n",
      "val Loss: 2.4694 Acc: 0.6020\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 2.5035 Acc: 0.6578\n",
      "val Loss: 2.4579 Acc: 0.6020\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 2.4968 Acc: 0.6471\n",
      "val Loss: 2.4666 Acc: 0.6069\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 2.5102 Acc: 0.6471\n",
      "val Loss: 2.4723 Acc: 0.6029\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 2.5208 Acc: 0.6461\n",
      "val Loss: 2.4722 Acc: 0.6069\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 2.5255 Acc: 0.6402\n",
      "val Loss: 2.4720 Acc: 0.6069\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 2.5115 Acc: 0.6363\n",
      "val Loss: 2.4673 Acc: 0.6118\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 2.4454 Acc: 0.6745\n",
      "val Loss: 2.4611 Acc: 0.6127\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 2.4731 Acc: 0.6549\n",
      "val Loss: 2.4660 Acc: 0.6029\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 2.4508 Acc: 0.6608\n",
      "val Loss: 2.4606 Acc: 0.6108\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 2.4795 Acc: 0.6490\n",
      "val Loss: 2.4631 Acc: 0.6108\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 2.4951 Acc: 0.6451\n",
      "val Loss: 2.4675 Acc: 0.6098\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 2.4941 Acc: 0.6392\n",
      "val Loss: 2.4593 Acc: 0.6088\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 2.4905 Acc: 0.6412\n",
      "val Loss: 2.4543 Acc: 0.6147\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 2.4933 Acc: 0.6578\n",
      "val Loss: 2.4572 Acc: 0.6069\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 2.5139 Acc: 0.6275\n",
      "val Loss: 2.4689 Acc: 0.6069\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 2.5129 Acc: 0.6353\n",
      "val Loss: 2.4605 Acc: 0.6020\n",
      "\n",
      "Training complete in 15m 9s\n",
      "Best val Acc: 0.614706\n",
      "\n",
      "----- Performing experiment with fixed lr=0.01 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6726 Acc: 0.0088\n",
      "val Loss: 4.6353 Acc: 0.0127\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.6122 Acc: 0.0127\n",
      "val Loss: 4.5764 Acc: 0.0147\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.5601 Acc: 0.0225\n",
      "val Loss: 4.5218 Acc: 0.0451\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.5078 Acc: 0.0461\n",
      "val Loss: 4.4702 Acc: 0.0627\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.4573 Acc: 0.0667\n",
      "val Loss: 4.4173 Acc: 0.1010\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.4130 Acc: 0.1020\n",
      "val Loss: 4.3640 Acc: 0.1461\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 4.3567 Acc: 0.1373\n",
      "val Loss: 4.3136 Acc: 0.1784\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 4.3205 Acc: 0.1735\n",
      "val Loss: 4.2670 Acc: 0.2049\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 4.2540 Acc: 0.2294\n",
      "val Loss: 4.2170 Acc: 0.2324\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 4.2261 Acc: 0.2412\n",
      "val Loss: 4.1688 Acc: 0.2686\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 4.1687 Acc: 0.2951\n",
      "val Loss: 4.1185 Acc: 0.2980\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 4.1278 Acc: 0.3255\n",
      "val Loss: 4.0732 Acc: 0.3255\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 4.0882 Acc: 0.3304\n",
      "val Loss: 4.0258 Acc: 0.3696\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 4.0430 Acc: 0.3549\n",
      "val Loss: 3.9778 Acc: 0.3794\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 3.9808 Acc: 0.3745\n",
      "val Loss: 3.9351 Acc: 0.3922\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 3.9535 Acc: 0.3892\n",
      "val Loss: 3.8898 Acc: 0.4088\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 3.9042 Acc: 0.4353\n",
      "val Loss: 3.8438 Acc: 0.4353\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 3.8674 Acc: 0.4167\n",
      "val Loss: 3.8048 Acc: 0.4157\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 3.8186 Acc: 0.4324\n",
      "val Loss: 3.7613 Acc: 0.4422\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 3.7751 Acc: 0.4578\n",
      "val Loss: 3.7209 Acc: 0.4510\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 3.7415 Acc: 0.4618\n",
      "val Loss: 3.6766 Acc: 0.4569\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 3.7153 Acc: 0.4480\n",
      "val Loss: 3.6415 Acc: 0.4725\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 3.6591 Acc: 0.4716\n",
      "val Loss: 3.5967 Acc: 0.4745\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 3.6209 Acc: 0.5000\n",
      "val Loss: 3.5604 Acc: 0.4716\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 3.5950 Acc: 0.4843\n",
      "val Loss: 3.5188 Acc: 0.4853\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 3.5683 Acc: 0.4765\n",
      "val Loss: 3.4849 Acc: 0.4912\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 3.4946 Acc: 0.5118\n",
      "val Loss: 3.4438 Acc: 0.4912\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 3.4726 Acc: 0.5275\n",
      "val Loss: 3.4147 Acc: 0.5078\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 3.4343 Acc: 0.5284\n",
      "val Loss: 3.3720 Acc: 0.5020\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 3.4258 Acc: 0.5255\n",
      "val Loss: 3.3451 Acc: 0.5059\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 3.3929 Acc: 0.5069\n",
      "val Loss: 3.3148 Acc: 0.5157\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 3.3377 Acc: 0.5275\n",
      "val Loss: 3.2758 Acc: 0.5078\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 3.3292 Acc: 0.5255\n",
      "val Loss: 3.2425 Acc: 0.5206\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 3.2892 Acc: 0.5304\n",
      "val Loss: 3.2171 Acc: 0.5186\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 3.2619 Acc: 0.5382\n",
      "val Loss: 3.1900 Acc: 0.5275\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 3.2244 Acc: 0.5529\n",
      "val Loss: 3.1575 Acc: 0.5294\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 3.1814 Acc: 0.5578\n",
      "val Loss: 3.1252 Acc: 0.5373\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 3.1784 Acc: 0.5471\n",
      "val Loss: 3.0874 Acc: 0.5461\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 3.1453 Acc: 0.5451\n",
      "val Loss: 3.0560 Acc: 0.5529\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 3.1065 Acc: 0.5725\n",
      "val Loss: 3.0389 Acc: 0.5500\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 3.0903 Acc: 0.5539\n",
      "val Loss: 3.0101 Acc: 0.5500\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 3.0351 Acc: 0.5843\n",
      "val Loss: 2.9770 Acc: 0.5510\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 3.0385 Acc: 0.5637\n",
      "val Loss: 2.9497 Acc: 0.5618\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 2.9951 Acc: 0.5922\n",
      "val Loss: 2.9457 Acc: 0.5598\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 3.0032 Acc: 0.5833\n",
      "val Loss: 2.9094 Acc: 0.5618\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 2.9435 Acc: 0.6029\n",
      "val Loss: 2.8845 Acc: 0.5696\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 2.9065 Acc: 0.6029\n",
      "val Loss: 2.8527 Acc: 0.5775\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 2.8872 Acc: 0.6167\n",
      "val Loss: 2.8313 Acc: 0.5794\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 2.8873 Acc: 0.5784\n",
      "val Loss: 2.8085 Acc: 0.5843\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 2.8579 Acc: 0.6137\n",
      "val Loss: 2.7868 Acc: 0.5853\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 2.8738 Acc: 0.5853\n",
      "val Loss: 2.7637 Acc: 0.5784\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 2.7714 Acc: 0.6294\n",
      "val Loss: 2.7252 Acc: 0.5882\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 2.7641 Acc: 0.6069\n",
      "val Loss: 2.7106 Acc: 0.5922\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 2.7484 Acc: 0.6176\n",
      "val Loss: 2.6927 Acc: 0.5971\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 2.7512 Acc: 0.5980\n",
      "val Loss: 2.6767 Acc: 0.5902\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 2.7321 Acc: 0.5971\n",
      "val Loss: 2.6572 Acc: 0.5853\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 2.6814 Acc: 0.6088\n",
      "val Loss: 2.6394 Acc: 0.5941\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 2.6973 Acc: 0.6010\n",
      "val Loss: 2.6196 Acc: 0.5941\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 2.6467 Acc: 0.6324\n",
      "val Loss: 2.6015 Acc: 0.6029\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 2.6433 Acc: 0.6196\n",
      "val Loss: 2.5805 Acc: 0.6049\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 2.6406 Acc: 0.6235\n",
      "val Loss: 2.5743 Acc: 0.6049\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 2.6197 Acc: 0.6029\n",
      "val Loss: 2.5696 Acc: 0.6059\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 2.5862 Acc: 0.6422\n",
      "val Loss: 2.5764 Acc: 0.6069\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 2.6080 Acc: 0.6382\n",
      "val Loss: 2.5617 Acc: 0.6069\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 2.5657 Acc: 0.6441\n",
      "val Loss: 2.5663 Acc: 0.6069\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 2.5760 Acc: 0.6471\n",
      "val Loss: 2.5631 Acc: 0.6078\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 2.5970 Acc: 0.6314\n",
      "val Loss: 2.5593 Acc: 0.6059\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 2.5645 Acc: 0.6392\n",
      "val Loss: 2.5525 Acc: 0.6059\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 2.6207 Acc: 0.6324\n",
      "val Loss: 2.5630 Acc: 0.6059\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 2.5826 Acc: 0.6333\n",
      "val Loss: 2.5594 Acc: 0.6088\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 2.6078 Acc: 0.6392\n",
      "val Loss: 2.5560 Acc: 0.6010\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 2.5945 Acc: 0.6333\n",
      "val Loss: 2.5552 Acc: 0.6010\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 2.5796 Acc: 0.6422\n",
      "val Loss: 2.5505 Acc: 0.6049\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 2.6162 Acc: 0.6255\n",
      "val Loss: 2.5550 Acc: 0.6088\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 2.5629 Acc: 0.6706\n",
      "val Loss: 2.5529 Acc: 0.6098\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 2.5399 Acc: 0.6735\n",
      "val Loss: 2.5442 Acc: 0.6078\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 2.6022 Acc: 0.6167\n",
      "val Loss: 2.5408 Acc: 0.6078\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 2.5471 Acc: 0.6500\n",
      "val Loss: 2.5436 Acc: 0.6088\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 2.5686 Acc: 0.6598\n",
      "val Loss: 2.5432 Acc: 0.6118\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 2.5217 Acc: 0.6451\n",
      "val Loss: 2.5350 Acc: 0.6088\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 2.5622 Acc: 0.6284\n",
      "val Loss: 2.5280 Acc: 0.6157\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 2.5934 Acc: 0.6343\n",
      "val Loss: 2.5430 Acc: 0.6010\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 2.5811 Acc: 0.6284\n",
      "val Loss: 2.5322 Acc: 0.6069\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 2.5588 Acc: 0.6294\n",
      "val Loss: 2.5229 Acc: 0.6059\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 2.5627 Acc: 0.6324\n",
      "val Loss: 2.5306 Acc: 0.6069\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 2.5484 Acc: 0.6480\n",
      "val Loss: 2.5203 Acc: 0.6108\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 2.5314 Acc: 0.6441\n",
      "val Loss: 2.5187 Acc: 0.6078\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 2.5808 Acc: 0.6275\n",
      "val Loss: 2.5258 Acc: 0.6127\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 2.5404 Acc: 0.6598\n",
      "val Loss: 2.5181 Acc: 0.6059\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 2.5157 Acc: 0.6598\n",
      "val Loss: 2.5103 Acc: 0.6069\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 2.5693 Acc: 0.6196\n",
      "val Loss: 2.5111 Acc: 0.6127\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 2.5544 Acc: 0.6343\n",
      "val Loss: 2.5169 Acc: 0.6078\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 2.5615 Acc: 0.6343\n",
      "val Loss: 2.5102 Acc: 0.6118\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 2.5681 Acc: 0.6353\n",
      "val Loss: 2.5110 Acc: 0.6059\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 2.5314 Acc: 0.6373\n",
      "val Loss: 2.5134 Acc: 0.6118\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 2.5264 Acc: 0.6510\n",
      "val Loss: 2.5072 Acc: 0.6108\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 2.5691 Acc: 0.6392\n",
      "val Loss: 2.5254 Acc: 0.6069\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 2.5201 Acc: 0.6490\n",
      "val Loss: 2.5167 Acc: 0.6167\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 2.4966 Acc: 0.6559\n",
      "val Loss: 2.4953 Acc: 0.6127\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 2.4851 Acc: 0.6402\n",
      "val Loss: 2.4977 Acc: 0.6088\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 2.4948 Acc: 0.6461\n",
      "val Loss: 2.5023 Acc: 0.6059\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 2.5561 Acc: 0.6324\n",
      "val Loss: 2.5060 Acc: 0.6108\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 2.5127 Acc: 0.6490\n",
      "val Loss: 2.5043 Acc: 0.6137\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 2.5081 Acc: 0.6559\n",
      "val Loss: 2.4953 Acc: 0.6118\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 2.5030 Acc: 0.6598\n",
      "val Loss: 2.4922 Acc: 0.6127\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 2.5056 Acc: 0.6529\n",
      "val Loss: 2.4917 Acc: 0.6137\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 2.5023 Acc: 0.6412\n",
      "val Loss: 2.4893 Acc: 0.6108\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 2.5245 Acc: 0.6510\n",
      "val Loss: 2.4871 Acc: 0.6088\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 2.5127 Acc: 0.6353\n",
      "val Loss: 2.4825 Acc: 0.6098\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 2.5012 Acc: 0.6618\n",
      "val Loss: 2.4802 Acc: 0.6088\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 2.5330 Acc: 0.6275\n",
      "val Loss: 2.4927 Acc: 0.6118\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 2.5277 Acc: 0.6461\n",
      "val Loss: 2.4845 Acc: 0.6118\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 2.5471 Acc: 0.6196\n",
      "val Loss: 2.4796 Acc: 0.6078\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 2.5264 Acc: 0.6578\n",
      "val Loss: 2.4777 Acc: 0.6137\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 2.5586 Acc: 0.6284\n",
      "val Loss: 2.4863 Acc: 0.6049\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 2.5003 Acc: 0.6618\n",
      "val Loss: 2.4773 Acc: 0.6127\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 2.5162 Acc: 0.6196\n",
      "val Loss: 2.4730 Acc: 0.6196\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 2.5164 Acc: 0.6431\n",
      "val Loss: 2.4805 Acc: 0.6137\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 2.5132 Acc: 0.6618\n",
      "val Loss: 2.4824 Acc: 0.6157\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 2.4918 Acc: 0.6490\n",
      "val Loss: 2.4735 Acc: 0.6137\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 2.5124 Acc: 0.6412\n",
      "val Loss: 2.4721 Acc: 0.6127\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 2.4964 Acc: 0.6500\n",
      "val Loss: 2.4734 Acc: 0.6118\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 2.5042 Acc: 0.6608\n",
      "val Loss: 2.4815 Acc: 0.6118\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 2.5076 Acc: 0.6353\n",
      "val Loss: 2.4763 Acc: 0.6108\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 2.5041 Acc: 0.6373\n",
      "val Loss: 2.4656 Acc: 0.6127\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 2.4646 Acc: 0.6549\n",
      "val Loss: 2.4581 Acc: 0.6127\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 2.5090 Acc: 0.6588\n",
      "val Loss: 2.4691 Acc: 0.6157\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 2.5068 Acc: 0.6490\n",
      "val Loss: 2.4598 Acc: 0.6167\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 2.5126 Acc: 0.6343\n",
      "val Loss: 2.4693 Acc: 0.6098\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 2.4619 Acc: 0.6569\n",
      "val Loss: 2.4611 Acc: 0.6147\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 2.5096 Acc: 0.6275\n",
      "val Loss: 2.4792 Acc: 0.6147\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 2.4714 Acc: 0.6588\n",
      "val Loss: 2.4720 Acc: 0.6127\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 2.4891 Acc: 0.6333\n",
      "val Loss: 2.4679 Acc: 0.6147\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 2.4688 Acc: 0.6529\n",
      "val Loss: 2.4694 Acc: 0.6108\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 2.4890 Acc: 0.6392\n",
      "val Loss: 2.4681 Acc: 0.6206\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 2.4865 Acc: 0.6578\n",
      "val Loss: 2.4655 Acc: 0.6137\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 2.5060 Acc: 0.6363\n",
      "val Loss: 2.4694 Acc: 0.6137\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 2.5068 Acc: 0.6392\n",
      "val Loss: 2.4572 Acc: 0.6147\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 2.5295 Acc: 0.6392\n",
      "val Loss: 2.4632 Acc: 0.6088\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 2.4798 Acc: 0.6627\n",
      "val Loss: 2.4653 Acc: 0.6137\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 2.4817 Acc: 0.6304\n",
      "val Loss: 2.4547 Acc: 0.6235\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 2.5104 Acc: 0.6373\n",
      "val Loss: 2.4788 Acc: 0.6108\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 2.4779 Acc: 0.6412\n",
      "val Loss: 2.4661 Acc: 0.6147\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 2.4981 Acc: 0.6245\n",
      "val Loss: 2.4762 Acc: 0.6078\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 2.4859 Acc: 0.6402\n",
      "val Loss: 2.4698 Acc: 0.6167\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 2.5204 Acc: 0.6284\n",
      "val Loss: 2.4643 Acc: 0.6157\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 2.5126 Acc: 0.6441\n",
      "val Loss: 2.4591 Acc: 0.6186\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 2.4826 Acc: 0.6451\n",
      "val Loss: 2.4555 Acc: 0.6196\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 2.5189 Acc: 0.6373\n",
      "val Loss: 2.4601 Acc: 0.6127\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 2.4872 Acc: 0.6461\n",
      "val Loss: 2.4679 Acc: 0.6137\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 2.4957 Acc: 0.6412\n",
      "val Loss: 2.4629 Acc: 0.6167\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 2.4916 Acc: 0.6451\n",
      "val Loss: 2.4665 Acc: 0.6059\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 2.5284 Acc: 0.6422\n",
      "val Loss: 2.4679 Acc: 0.6108\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 2.4562 Acc: 0.6500\n",
      "val Loss: 2.4651 Acc: 0.6147\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 2.5334 Acc: 0.6353\n",
      "val Loss: 2.4669 Acc: 0.6137\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 2.4922 Acc: 0.6598\n",
      "val Loss: 2.4614 Acc: 0.6127\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 2.5090 Acc: 0.6294\n",
      "val Loss: 2.4662 Acc: 0.6137\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 2.5197 Acc: 0.6422\n",
      "val Loss: 2.4749 Acc: 0.6167\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 2.4842 Acc: 0.6637\n",
      "val Loss: 2.4727 Acc: 0.6118\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 2.4932 Acc: 0.6618\n",
      "val Loss: 2.4662 Acc: 0.6157\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 2.4899 Acc: 0.6529\n",
      "val Loss: 2.4589 Acc: 0.6157\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 2.4673 Acc: 0.6510\n",
      "val Loss: 2.4637 Acc: 0.6206\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 2.4913 Acc: 0.6510\n",
      "val Loss: 2.4637 Acc: 0.6186\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 2.5052 Acc: 0.6471\n",
      "val Loss: 2.4672 Acc: 0.6147\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 2.5013 Acc: 0.6412\n",
      "val Loss: 2.4665 Acc: 0.6118\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 2.4961 Acc: 0.6549\n",
      "val Loss: 2.4741 Acc: 0.6098\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 2.5221 Acc: 0.6343\n",
      "val Loss: 2.4672 Acc: 0.6098\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 2.5023 Acc: 0.6461\n",
      "val Loss: 2.4767 Acc: 0.6088\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 2.5188 Acc: 0.6402\n",
      "val Loss: 2.4672 Acc: 0.6118\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 2.4619 Acc: 0.6559\n",
      "val Loss: 2.4628 Acc: 0.6147\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 2.4836 Acc: 0.6510\n",
      "val Loss: 2.4694 Acc: 0.6167\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 2.4895 Acc: 0.6500\n",
      "val Loss: 2.4580 Acc: 0.6176\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 2.5119 Acc: 0.6500\n",
      "val Loss: 2.4529 Acc: 0.6167\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 2.4666 Acc: 0.6422\n",
      "val Loss: 2.4608 Acc: 0.6137\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 2.4847 Acc: 0.6333\n",
      "val Loss: 2.4558 Acc: 0.6176\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 2.4838 Acc: 0.6451\n",
      "val Loss: 2.4597 Acc: 0.6137\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 2.5134 Acc: 0.6588\n",
      "val Loss: 2.4637 Acc: 0.6167\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 2.4713 Acc: 0.6461\n",
      "val Loss: 2.4532 Acc: 0.6167\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 2.4927 Acc: 0.6559\n",
      "val Loss: 2.4536 Acc: 0.6206\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 2.4815 Acc: 0.6549\n",
      "val Loss: 2.4522 Acc: 0.6118\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 2.4957 Acc: 0.6392\n",
      "val Loss: 2.4588 Acc: 0.6176\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 2.4771 Acc: 0.6549\n",
      "val Loss: 2.4560 Acc: 0.6157\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 2.4800 Acc: 0.6441\n",
      "val Loss: 2.4505 Acc: 0.6176\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 2.4965 Acc: 0.6431\n",
      "val Loss: 2.4563 Acc: 0.6186\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 2.4816 Acc: 0.6500\n",
      "val Loss: 2.4585 Acc: 0.6147\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 2.4823 Acc: 0.6363\n",
      "val Loss: 2.4660 Acc: 0.6186\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 2.5032 Acc: 0.6304\n",
      "val Loss: 2.4567 Acc: 0.6108\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 2.5043 Acc: 0.6480\n",
      "val Loss: 2.4649 Acc: 0.6078\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 2.5035 Acc: 0.6255\n",
      "val Loss: 2.4571 Acc: 0.6157\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 2.4879 Acc: 0.6373\n",
      "val Loss: 2.4544 Acc: 0.6108\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 2.4848 Acc: 0.6588\n",
      "val Loss: 2.4480 Acc: 0.6206\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 2.4907 Acc: 0.6402\n",
      "val Loss: 2.4599 Acc: 0.6176\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 2.4602 Acc: 0.6569\n",
      "val Loss: 2.4555 Acc: 0.6157\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 2.5013 Acc: 0.6324\n",
      "val Loss: 2.4612 Acc: 0.6127\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 2.5311 Acc: 0.6216\n",
      "val Loss: 2.4689 Acc: 0.6127\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 2.4902 Acc: 0.6549\n",
      "val Loss: 2.4696 Acc: 0.6127\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 2.4778 Acc: 0.6510\n",
      "val Loss: 2.4650 Acc: 0.6176\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 2.4910 Acc: 0.6412\n",
      "val Loss: 2.4629 Acc: 0.6157\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 2.5037 Acc: 0.6431\n",
      "val Loss: 2.4577 Acc: 0.6206\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 2.4951 Acc: 0.6382\n",
      "val Loss: 2.4696 Acc: 0.6127\n",
      "\n",
      "Training complete in 15m 7s\n",
      "Best val Acc: 0.623529\n",
      "\n",
      "----- Performing experiment with fixed lr=0.1 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6838 Acc: 0.0059\n",
      "val Loss: 4.6210 Acc: 0.0127\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.6140 Acc: 0.0137\n",
      "val Loss: 4.5573 Acc: 0.0284\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.5528 Acc: 0.0255\n",
      "val Loss: 4.4999 Acc: 0.0471\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.5063 Acc: 0.0382\n",
      "val Loss: 4.4476 Acc: 0.0706\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.4571 Acc: 0.0686\n",
      "val Loss: 4.3970 Acc: 0.1216\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.4003 Acc: 0.1157\n",
      "val Loss: 4.3453 Acc: 0.1578\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 4.3546 Acc: 0.1500\n",
      "val Loss: 4.2947 Acc: 0.2000\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 4.3016 Acc: 0.1931\n",
      "val Loss: 4.2440 Acc: 0.2441\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 4.2490 Acc: 0.2569\n",
      "val Loss: 4.1939 Acc: 0.2804\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 4.2124 Acc: 0.2529\n",
      "val Loss: 4.1457 Acc: 0.2990\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 4.1681 Acc: 0.2912\n",
      "val Loss: 4.0945 Acc: 0.3363\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 4.1077 Acc: 0.3255\n",
      "val Loss: 4.0510 Acc: 0.3451\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 4.0636 Acc: 0.3657\n",
      "val Loss: 4.0014 Acc: 0.3696\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 4.0371 Acc: 0.3824\n",
      "val Loss: 3.9577 Acc: 0.3814\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 3.9758 Acc: 0.3912\n",
      "val Loss: 3.9085 Acc: 0.4049\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 3.9360 Acc: 0.3912\n",
      "val Loss: 3.8657 Acc: 0.4245\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 3.9135 Acc: 0.4020\n",
      "val Loss: 3.8250 Acc: 0.4225\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 3.8360 Acc: 0.4412\n",
      "val Loss: 3.7778 Acc: 0.4529\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 3.8113 Acc: 0.4382\n",
      "val Loss: 3.7441 Acc: 0.4431\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 3.7753 Acc: 0.4520\n",
      "val Loss: 3.6976 Acc: 0.4588\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 3.7317 Acc: 0.4490\n",
      "val Loss: 3.6539 Acc: 0.4824\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 3.7037 Acc: 0.4559\n",
      "val Loss: 3.6221 Acc: 0.4735\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 3.6714 Acc: 0.4588\n",
      "val Loss: 3.5802 Acc: 0.4853\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 3.6224 Acc: 0.4706\n",
      "val Loss: 3.5417 Acc: 0.4990\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 3.5730 Acc: 0.4931\n",
      "val Loss: 3.5034 Acc: 0.5137\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 3.5439 Acc: 0.5127\n",
      "val Loss: 3.4679 Acc: 0.5088\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 3.5151 Acc: 0.5029\n",
      "val Loss: 3.4271 Acc: 0.5206\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 3.4842 Acc: 0.5176\n",
      "val Loss: 3.4021 Acc: 0.5196\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 3.4475 Acc: 0.5186\n",
      "val Loss: 3.3603 Acc: 0.5245\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 3.4114 Acc: 0.5235\n",
      "val Loss: 3.3255 Acc: 0.5402\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 3.3805 Acc: 0.5314\n",
      "val Loss: 3.2979 Acc: 0.5373\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 3.3171 Acc: 0.5353\n",
      "val Loss: 3.2631 Acc: 0.5392\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 3.3007 Acc: 0.5373\n",
      "val Loss: 3.2286 Acc: 0.5471\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 3.2847 Acc: 0.5324\n",
      "val Loss: 3.1919 Acc: 0.5559\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 3.2709 Acc: 0.5363\n",
      "val Loss: 3.1720 Acc: 0.5627\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 3.1996 Acc: 0.5775\n",
      "val Loss: 3.1348 Acc: 0.5647\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 3.1896 Acc: 0.5676\n",
      "val Loss: 3.1184 Acc: 0.5627\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 3.1518 Acc: 0.5833\n",
      "val Loss: 3.0800 Acc: 0.5775\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 3.1369 Acc: 0.5520\n",
      "val Loss: 3.0545 Acc: 0.5804\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 3.1292 Acc: 0.5529\n",
      "val Loss: 3.0348 Acc: 0.5706\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 3.0802 Acc: 0.5647\n",
      "val Loss: 2.9957 Acc: 0.5745\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 3.0544 Acc: 0.5578\n",
      "val Loss: 2.9702 Acc: 0.5775\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 2.9834 Acc: 0.5961\n",
      "val Loss: 2.9340 Acc: 0.5892\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 2.9784 Acc: 0.6098\n",
      "val Loss: 2.9160 Acc: 0.5931\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 2.9579 Acc: 0.5765\n",
      "val Loss: 2.8906 Acc: 0.5941\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 2.9821 Acc: 0.5696\n",
      "val Loss: 2.8796 Acc: 0.5863\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 2.8893 Acc: 0.6108\n",
      "val Loss: 2.8468 Acc: 0.5892\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 2.8855 Acc: 0.6000\n",
      "val Loss: 2.8084 Acc: 0.6039\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 2.8648 Acc: 0.5980\n",
      "val Loss: 2.7904 Acc: 0.5922\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 2.8380 Acc: 0.5961\n",
      "val Loss: 2.7698 Acc: 0.5990\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 2.8231 Acc: 0.5922\n",
      "val Loss: 2.7435 Acc: 0.6059\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 2.7902 Acc: 0.6029\n",
      "val Loss: 2.7200 Acc: 0.5980\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 2.7197 Acc: 0.6343\n",
      "val Loss: 2.6892 Acc: 0.6157\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 2.7520 Acc: 0.6127\n",
      "val Loss: 2.6768 Acc: 0.6029\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 2.7254 Acc: 0.6000\n",
      "val Loss: 2.6597 Acc: 0.6088\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 2.7085 Acc: 0.6167\n",
      "val Loss: 2.6340 Acc: 0.6127\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 2.6882 Acc: 0.6176\n",
      "val Loss: 2.6137 Acc: 0.6069\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 2.6455 Acc: 0.6353\n",
      "val Loss: 2.6032 Acc: 0.6157\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 2.6717 Acc: 0.6088\n",
      "val Loss: 2.5874 Acc: 0.6216\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 2.6210 Acc: 0.6284\n",
      "val Loss: 2.5656 Acc: 0.6167\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 2.5804 Acc: 0.6480\n",
      "val Loss: 2.5625 Acc: 0.6167\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 2.5720 Acc: 0.6422\n",
      "val Loss: 2.5617 Acc: 0.6157\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 2.5999 Acc: 0.6500\n",
      "val Loss: 2.5655 Acc: 0.6167\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 2.5924 Acc: 0.6402\n",
      "val Loss: 2.5553 Acc: 0.6196\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 2.6185 Acc: 0.6088\n",
      "val Loss: 2.5585 Acc: 0.6147\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 2.5852 Acc: 0.6431\n",
      "val Loss: 2.5430 Acc: 0.6196\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 2.5994 Acc: 0.6235\n",
      "val Loss: 2.5588 Acc: 0.6176\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 2.5923 Acc: 0.6255\n",
      "val Loss: 2.5570 Acc: 0.6157\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 2.5691 Acc: 0.6471\n",
      "val Loss: 2.5522 Acc: 0.6206\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 2.5873 Acc: 0.6275\n",
      "val Loss: 2.5475 Acc: 0.6225\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 2.6123 Acc: 0.6324\n",
      "val Loss: 2.5461 Acc: 0.6206\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 2.5958 Acc: 0.6373\n",
      "val Loss: 2.5458 Acc: 0.6186\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 2.5952 Acc: 0.6284\n",
      "val Loss: 2.5410 Acc: 0.6216\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 2.5321 Acc: 0.6559\n",
      "val Loss: 2.5286 Acc: 0.6265\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 2.5697 Acc: 0.6500\n",
      "val Loss: 2.5287 Acc: 0.6216\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 2.6061 Acc: 0.6157\n",
      "val Loss: 2.5318 Acc: 0.6216\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 2.5610 Acc: 0.6392\n",
      "val Loss: 2.5380 Acc: 0.6225\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 2.5826 Acc: 0.6618\n",
      "val Loss: 2.5372 Acc: 0.6186\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 2.5371 Acc: 0.6529\n",
      "val Loss: 2.5131 Acc: 0.6245\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 2.5820 Acc: 0.6304\n",
      "val Loss: 2.5362 Acc: 0.6186\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 2.5746 Acc: 0.6373\n",
      "val Loss: 2.5256 Acc: 0.6186\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 2.5568 Acc: 0.6324\n",
      "val Loss: 2.5209 Acc: 0.6294\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 2.5486 Acc: 0.6422\n",
      "val Loss: 2.5180 Acc: 0.6186\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 2.5168 Acc: 0.6471\n",
      "val Loss: 2.5094 Acc: 0.6265\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 2.5754 Acc: 0.6392\n",
      "val Loss: 2.5211 Acc: 0.6137\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 2.5623 Acc: 0.6559\n",
      "val Loss: 2.5153 Acc: 0.6235\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 2.5337 Acc: 0.6520\n",
      "val Loss: 2.5085 Acc: 0.6216\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 2.5357 Acc: 0.6725\n",
      "val Loss: 2.5026 Acc: 0.6196\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 2.5674 Acc: 0.6373\n",
      "val Loss: 2.5077 Acc: 0.6176\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 2.5498 Acc: 0.6510\n",
      "val Loss: 2.5050 Acc: 0.6137\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 2.5206 Acc: 0.6324\n",
      "val Loss: 2.5093 Acc: 0.6206\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 2.5421 Acc: 0.6363\n",
      "val Loss: 2.5050 Acc: 0.6235\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 2.5321 Acc: 0.6412\n",
      "val Loss: 2.5107 Acc: 0.6167\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 2.5165 Acc: 0.6490\n",
      "val Loss: 2.5059 Acc: 0.6196\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 2.5149 Acc: 0.6353\n",
      "val Loss: 2.4929 Acc: 0.6245\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 2.5099 Acc: 0.6627\n",
      "val Loss: 2.4996 Acc: 0.6225\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 2.5025 Acc: 0.6647\n",
      "val Loss: 2.4933 Acc: 0.6265\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 2.5154 Acc: 0.6618\n",
      "val Loss: 2.4840 Acc: 0.6235\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 2.5729 Acc: 0.6225\n",
      "val Loss: 2.4999 Acc: 0.6196\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 2.5443 Acc: 0.6186\n",
      "val Loss: 2.4926 Acc: 0.6235\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 2.5612 Acc: 0.6245\n",
      "val Loss: 2.4969 Acc: 0.6167\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 2.5179 Acc: 0.6647\n",
      "val Loss: 2.4836 Acc: 0.6216\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 2.5030 Acc: 0.6549\n",
      "val Loss: 2.4792 Acc: 0.6225\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 2.5270 Acc: 0.6333\n",
      "val Loss: 2.4863 Acc: 0.6225\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 2.5336 Acc: 0.6294\n",
      "val Loss: 2.4861 Acc: 0.6245\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 2.5016 Acc: 0.6471\n",
      "val Loss: 2.4673 Acc: 0.6314\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 2.5474 Acc: 0.6304\n",
      "val Loss: 2.4714 Acc: 0.6255\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 2.4759 Acc: 0.6549\n",
      "val Loss: 2.4598 Acc: 0.6324\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 2.4845 Acc: 0.6569\n",
      "val Loss: 2.4631 Acc: 0.6255\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 2.5076 Acc: 0.6431\n",
      "val Loss: 2.4785 Acc: 0.6275\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 2.5723 Acc: 0.6167\n",
      "val Loss: 2.4931 Acc: 0.6206\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 2.5562 Acc: 0.6353\n",
      "val Loss: 2.4750 Acc: 0.6206\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 2.4933 Acc: 0.6461\n",
      "val Loss: 2.4651 Acc: 0.6265\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 2.5219 Acc: 0.6422\n",
      "val Loss: 2.4649 Acc: 0.6255\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 2.4992 Acc: 0.6706\n",
      "val Loss: 2.4722 Acc: 0.6176\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 2.4936 Acc: 0.6549\n",
      "val Loss: 2.4625 Acc: 0.6118\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 2.4876 Acc: 0.6324\n",
      "val Loss: 2.4562 Acc: 0.6265\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 2.5169 Acc: 0.6284\n",
      "val Loss: 2.4694 Acc: 0.6235\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 2.5015 Acc: 0.6451\n",
      "val Loss: 2.4626 Acc: 0.6304\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 2.4571 Acc: 0.6647\n",
      "val Loss: 2.4472 Acc: 0.6314\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 2.4864 Acc: 0.6382\n",
      "val Loss: 2.4466 Acc: 0.6255\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 2.5224 Acc: 0.6402\n",
      "val Loss: 2.4566 Acc: 0.6196\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 2.5149 Acc: 0.6471\n",
      "val Loss: 2.4511 Acc: 0.6235\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 2.4808 Acc: 0.6402\n",
      "val Loss: 2.4555 Acc: 0.6255\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 2.5301 Acc: 0.6382\n",
      "val Loss: 2.4597 Acc: 0.6275\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 2.4620 Acc: 0.6647\n",
      "val Loss: 2.4570 Acc: 0.6275\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 2.4811 Acc: 0.6402\n",
      "val Loss: 2.4552 Acc: 0.6255\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 2.5076 Acc: 0.6422\n",
      "val Loss: 2.4542 Acc: 0.6216\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 2.4946 Acc: 0.6441\n",
      "val Loss: 2.4624 Acc: 0.6235\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 2.4944 Acc: 0.6510\n",
      "val Loss: 2.4506 Acc: 0.6255\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 2.4822 Acc: 0.6598\n",
      "val Loss: 2.4574 Acc: 0.6284\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 2.4447 Acc: 0.6559\n",
      "val Loss: 2.4490 Acc: 0.6304\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 2.4720 Acc: 0.6686\n",
      "val Loss: 2.4541 Acc: 0.6284\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 2.5135 Acc: 0.6529\n",
      "val Loss: 2.4532 Acc: 0.6235\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 2.5129 Acc: 0.6382\n",
      "val Loss: 2.4566 Acc: 0.6225\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 2.4914 Acc: 0.6422\n",
      "val Loss: 2.4531 Acc: 0.6255\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 2.5138 Acc: 0.6461\n",
      "val Loss: 2.4580 Acc: 0.6265\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 2.4714 Acc: 0.6353\n",
      "val Loss: 2.4571 Acc: 0.6265\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 2.4760 Acc: 0.6539\n",
      "val Loss: 2.4539 Acc: 0.6186\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 2.4578 Acc: 0.6578\n",
      "val Loss: 2.4499 Acc: 0.6225\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 2.5081 Acc: 0.6490\n",
      "val Loss: 2.4531 Acc: 0.6235\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 2.5079 Acc: 0.6265\n",
      "val Loss: 2.4391 Acc: 0.6275\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 2.4933 Acc: 0.6441\n",
      "val Loss: 2.4468 Acc: 0.6196\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 2.5370 Acc: 0.6324\n",
      "val Loss: 2.4618 Acc: 0.6167\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 2.5012 Acc: 0.6500\n",
      "val Loss: 2.4634 Acc: 0.6196\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 2.4852 Acc: 0.6441\n",
      "val Loss: 2.4584 Acc: 0.6275\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 2.5108 Acc: 0.6324\n",
      "val Loss: 2.4551 Acc: 0.6196\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 2.4540 Acc: 0.6618\n",
      "val Loss: 2.4519 Acc: 0.6275\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 2.4689 Acc: 0.6608\n",
      "val Loss: 2.4427 Acc: 0.6284\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 2.5092 Acc: 0.6480\n",
      "val Loss: 2.4478 Acc: 0.6255\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 2.4954 Acc: 0.6324\n",
      "val Loss: 2.4579 Acc: 0.6255\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 2.5340 Acc: 0.6265\n",
      "val Loss: 2.4483 Acc: 0.6216\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 2.5354 Acc: 0.6353\n",
      "val Loss: 2.4594 Acc: 0.6245\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 2.4619 Acc: 0.6716\n",
      "val Loss: 2.4581 Acc: 0.6265\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 2.5153 Acc: 0.6373\n",
      "val Loss: 2.4511 Acc: 0.6284\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 2.4236 Acc: 0.6735\n",
      "val Loss: 2.4315 Acc: 0.6275\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 2.4502 Acc: 0.6559\n",
      "val Loss: 2.4416 Acc: 0.6275\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 2.4622 Acc: 0.6588\n",
      "val Loss: 2.4547 Acc: 0.6294\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 2.4698 Acc: 0.6431\n",
      "val Loss: 2.4527 Acc: 0.6216\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 2.4719 Acc: 0.6402\n",
      "val Loss: 2.4433 Acc: 0.6235\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 2.4933 Acc: 0.6471\n",
      "val Loss: 2.4465 Acc: 0.6255\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 2.5100 Acc: 0.6500\n",
      "val Loss: 2.4509 Acc: 0.6225\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 2.4398 Acc: 0.6618\n",
      "val Loss: 2.4454 Acc: 0.6294\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 2.4914 Acc: 0.6608\n",
      "val Loss: 2.4502 Acc: 0.6206\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 2.4597 Acc: 0.6745\n",
      "val Loss: 2.4504 Acc: 0.6255\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 2.4940 Acc: 0.6490\n",
      "val Loss: 2.4574 Acc: 0.6225\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 2.4978 Acc: 0.6451\n",
      "val Loss: 2.4525 Acc: 0.6225\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 2.4993 Acc: 0.6520\n",
      "val Loss: 2.4556 Acc: 0.6235\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 2.5156 Acc: 0.6265\n",
      "val Loss: 2.4523 Acc: 0.6245\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 2.5003 Acc: 0.6275\n",
      "val Loss: 2.4532 Acc: 0.6304\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 2.4739 Acc: 0.6608\n",
      "val Loss: 2.4510 Acc: 0.6294\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 2.5165 Acc: 0.6431\n",
      "val Loss: 2.4531 Acc: 0.6255\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 2.5012 Acc: 0.6392\n",
      "val Loss: 2.4499 Acc: 0.6216\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 2.4703 Acc: 0.6618\n",
      "val Loss: 2.4505 Acc: 0.6255\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 2.4929 Acc: 0.6343\n",
      "val Loss: 2.4517 Acc: 0.6235\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 2.4904 Acc: 0.6402\n",
      "val Loss: 2.4516 Acc: 0.6275\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 2.5286 Acc: 0.6225\n",
      "val Loss: 2.4517 Acc: 0.6245\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 2.4563 Acc: 0.6451\n",
      "val Loss: 2.4308 Acc: 0.6275\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 2.4828 Acc: 0.6343\n",
      "val Loss: 2.4409 Acc: 0.6284\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 2.4968 Acc: 0.6510\n",
      "val Loss: 2.4425 Acc: 0.6284\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 2.4800 Acc: 0.6441\n",
      "val Loss: 2.4482 Acc: 0.6235\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 2.4583 Acc: 0.6627\n",
      "val Loss: 2.4458 Acc: 0.6235\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 2.4920 Acc: 0.6431\n",
      "val Loss: 2.4608 Acc: 0.6245\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 2.4762 Acc: 0.6549\n",
      "val Loss: 2.4446 Acc: 0.6235\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 2.4660 Acc: 0.6333\n",
      "val Loss: 2.4485 Acc: 0.6225\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 2.4632 Acc: 0.6618\n",
      "val Loss: 2.4530 Acc: 0.6245\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 2.4641 Acc: 0.6480\n",
      "val Loss: 2.4511 Acc: 0.6265\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 2.4969 Acc: 0.6539\n",
      "val Loss: 2.4594 Acc: 0.6216\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 2.4645 Acc: 0.6686\n",
      "val Loss: 2.4498 Acc: 0.6196\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 2.4586 Acc: 0.6598\n",
      "val Loss: 2.4390 Acc: 0.6245\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 2.4738 Acc: 0.6304\n",
      "val Loss: 2.4366 Acc: 0.6245\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 2.4518 Acc: 0.6686\n",
      "val Loss: 2.4364 Acc: 0.6304\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 2.4806 Acc: 0.6529\n",
      "val Loss: 2.4410 Acc: 0.6275\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 2.4819 Acc: 0.6284\n",
      "val Loss: 2.4500 Acc: 0.6275\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 2.4723 Acc: 0.6539\n",
      "val Loss: 2.4438 Acc: 0.6275\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 2.5020 Acc: 0.6461\n",
      "val Loss: 2.4570 Acc: 0.6255\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 2.4813 Acc: 0.6461\n",
      "val Loss: 2.4553 Acc: 0.6196\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 2.4526 Acc: 0.6647\n",
      "val Loss: 2.4489 Acc: 0.6245\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 2.4445 Acc: 0.6696\n",
      "val Loss: 2.4446 Acc: 0.6265\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 2.5059 Acc: 0.6196\n",
      "val Loss: 2.4440 Acc: 0.6206\n",
      "\n",
      "Training complete in 15m 8s\n",
      "Best val Acc: 0.632353\n",
      "\n",
      "----- Performing experiment with fixed lr=1 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6827 Acc: 0.0108\n",
      "val Loss: 4.6661 Acc: 0.0078\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.6365 Acc: 0.0118\n",
      "val Loss: 4.5956 Acc: 0.0235\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.5635 Acc: 0.0363\n",
      "val Loss: 4.5384 Acc: 0.0441\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.5194 Acc: 0.0539\n",
      "val Loss: 4.4867 Acc: 0.0676\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.4689 Acc: 0.0843\n",
      "val Loss: 4.4343 Acc: 0.1000\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.4224 Acc: 0.0990\n",
      "val Loss: 4.3836 Acc: 0.1422\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 4.3596 Acc: 0.1637\n",
      "val Loss: 4.3286 Acc: 0.1892\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 4.3257 Acc: 0.1706\n",
      "val Loss: 4.2814 Acc: 0.2275\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 4.2674 Acc: 0.2382\n",
      "val Loss: 4.2329 Acc: 0.2490\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 4.2323 Acc: 0.2275\n",
      "val Loss: 4.1847 Acc: 0.2804\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 4.1655 Acc: 0.2892\n",
      "val Loss: 4.1332 Acc: 0.2961\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 4.1309 Acc: 0.3137\n",
      "val Loss: 4.0911 Acc: 0.3304\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 4.0929 Acc: 0.3137\n",
      "val Loss: 4.0433 Acc: 0.3314\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 4.0325 Acc: 0.3500\n",
      "val Loss: 3.9948 Acc: 0.3451\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 3.9970 Acc: 0.3657\n",
      "val Loss: 3.9483 Acc: 0.3784\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 3.9415 Acc: 0.4069\n",
      "val Loss: 3.9049 Acc: 0.3912\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 3.9304 Acc: 0.3784\n",
      "val Loss: 3.8600 Acc: 0.3941\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 3.8653 Acc: 0.4186\n",
      "val Loss: 3.8199 Acc: 0.3990\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 3.8501 Acc: 0.4147\n",
      "val Loss: 3.7777 Acc: 0.4176\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 3.8024 Acc: 0.4461\n",
      "val Loss: 3.7325 Acc: 0.4265\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 3.7549 Acc: 0.4608\n",
      "val Loss: 3.6894 Acc: 0.4363\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 3.6925 Acc: 0.4686\n",
      "val Loss: 3.6509 Acc: 0.4451\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 3.6657 Acc: 0.4951\n",
      "val Loss: 3.6110 Acc: 0.4559\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 3.6181 Acc: 0.5029\n",
      "val Loss: 3.5692 Acc: 0.4549\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 3.5837 Acc: 0.4941\n",
      "val Loss: 3.5392 Acc: 0.4775\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 3.5604 Acc: 0.4961\n",
      "val Loss: 3.4992 Acc: 0.4902\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 3.5125 Acc: 0.5324\n",
      "val Loss: 3.4612 Acc: 0.4902\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 3.5040 Acc: 0.5098\n",
      "val Loss: 3.4324 Acc: 0.4843\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 3.4469 Acc: 0.5069\n",
      "val Loss: 3.3874 Acc: 0.4980\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 3.4101 Acc: 0.5176\n",
      "val Loss: 3.3564 Acc: 0.4931\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 3.4114 Acc: 0.5108\n",
      "val Loss: 3.3227 Acc: 0.5157\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 3.3876 Acc: 0.5069\n",
      "val Loss: 3.2991 Acc: 0.5196\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 3.3126 Acc: 0.5392\n",
      "val Loss: 3.2596 Acc: 0.5127\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 3.2860 Acc: 0.5382\n",
      "val Loss: 3.2180 Acc: 0.5284\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 3.2642 Acc: 0.5412\n",
      "val Loss: 3.1880 Acc: 0.5392\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 3.2347 Acc: 0.5343\n",
      "val Loss: 3.1557 Acc: 0.5422\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 3.2251 Acc: 0.5490\n",
      "val Loss: 3.1449 Acc: 0.5353\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 3.1220 Acc: 0.6069\n",
      "val Loss: 3.0985 Acc: 0.5520\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 3.1496 Acc: 0.5559\n",
      "val Loss: 3.0827 Acc: 0.5490\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 3.1216 Acc: 0.5647\n",
      "val Loss: 3.0431 Acc: 0.5510\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 3.0757 Acc: 0.5667\n",
      "val Loss: 3.0082 Acc: 0.5667\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 3.0488 Acc: 0.5686\n",
      "val Loss: 2.9830 Acc: 0.5667\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 3.0274 Acc: 0.5833\n",
      "val Loss: 2.9686 Acc: 0.5598\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 2.9691 Acc: 0.6078\n",
      "val Loss: 2.9369 Acc: 0.5647\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 2.9635 Acc: 0.5990\n",
      "val Loss: 2.9153 Acc: 0.5696\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 2.9425 Acc: 0.5784\n",
      "val Loss: 2.8851 Acc: 0.5814\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 2.9621 Acc: 0.5608\n",
      "val Loss: 2.8687 Acc: 0.5775\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 2.9179 Acc: 0.5922\n",
      "val Loss: 2.8387 Acc: 0.5775\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 2.8655 Acc: 0.5951\n",
      "val Loss: 2.8207 Acc: 0.5794\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 2.8556 Acc: 0.5922\n",
      "val Loss: 2.7934 Acc: 0.5863\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 2.8496 Acc: 0.5980\n",
      "val Loss: 2.7771 Acc: 0.5735\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 2.7989 Acc: 0.6000\n",
      "val Loss: 2.7463 Acc: 0.5873\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 2.8234 Acc: 0.5902\n",
      "val Loss: 2.7263 Acc: 0.5873\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 2.7248 Acc: 0.6314\n",
      "val Loss: 2.7028 Acc: 0.5912\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 2.7610 Acc: 0.6069\n",
      "val Loss: 2.6947 Acc: 0.5980\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 2.7360 Acc: 0.6206\n",
      "val Loss: 2.6668 Acc: 0.5892\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 2.6724 Acc: 0.6382\n",
      "val Loss: 2.6353 Acc: 0.6010\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 2.6879 Acc: 0.6196\n",
      "val Loss: 2.6248 Acc: 0.6069\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 2.6774 Acc: 0.6196\n",
      "val Loss: 2.6154 Acc: 0.6078\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 2.6075 Acc: 0.6520\n",
      "val Loss: 2.5862 Acc: 0.6029\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 2.5955 Acc: 0.6343\n",
      "val Loss: 2.5822 Acc: 0.6069\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 2.6415 Acc: 0.6167\n",
      "val Loss: 2.5842 Acc: 0.5990\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 2.6167 Acc: 0.5961\n",
      "val Loss: 2.5901 Acc: 0.6029\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 2.6357 Acc: 0.6147\n",
      "val Loss: 2.5851 Acc: 0.6069\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 2.6234 Acc: 0.6265\n",
      "val Loss: 2.5788 Acc: 0.6049\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 2.5845 Acc: 0.6245\n",
      "val Loss: 2.5707 Acc: 0.6098\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 2.6021 Acc: 0.6314\n",
      "val Loss: 2.5770 Acc: 0.6049\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 2.6113 Acc: 0.6343\n",
      "val Loss: 2.5774 Acc: 0.5990\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 2.5920 Acc: 0.6412\n",
      "val Loss: 2.5794 Acc: 0.6078\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 2.5828 Acc: 0.6333\n",
      "val Loss: 2.5682 Acc: 0.6059\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 2.5960 Acc: 0.6412\n",
      "val Loss: 2.5652 Acc: 0.6088\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 2.5993 Acc: 0.6314\n",
      "val Loss: 2.5640 Acc: 0.6059\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 2.5688 Acc: 0.6490\n",
      "val Loss: 2.5533 Acc: 0.6078\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 2.6109 Acc: 0.6304\n",
      "val Loss: 2.5553 Acc: 0.6039\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 2.5718 Acc: 0.6461\n",
      "val Loss: 2.5499 Acc: 0.6098\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 2.5837 Acc: 0.6373\n",
      "val Loss: 2.5605 Acc: 0.6127\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 2.5964 Acc: 0.6225\n",
      "val Loss: 2.5557 Acc: 0.6069\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 2.5793 Acc: 0.6431\n",
      "val Loss: 2.5423 Acc: 0.6098\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 2.5620 Acc: 0.6304\n",
      "val Loss: 2.5588 Acc: 0.6088\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 2.5734 Acc: 0.6225\n",
      "val Loss: 2.5574 Acc: 0.6127\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 2.5789 Acc: 0.6382\n",
      "val Loss: 2.5560 Acc: 0.6069\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 2.5477 Acc: 0.6363\n",
      "val Loss: 2.5396 Acc: 0.6088\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 2.5587 Acc: 0.6480\n",
      "val Loss: 2.5421 Acc: 0.6098\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 2.5358 Acc: 0.6382\n",
      "val Loss: 2.5442 Acc: 0.6167\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 2.5472 Acc: 0.6588\n",
      "val Loss: 2.5376 Acc: 0.6118\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 2.5574 Acc: 0.6304\n",
      "val Loss: 2.5433 Acc: 0.6098\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 2.5592 Acc: 0.6412\n",
      "val Loss: 2.5433 Acc: 0.6118\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 2.5960 Acc: 0.6471\n",
      "val Loss: 2.5455 Acc: 0.6078\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 2.5712 Acc: 0.6373\n",
      "val Loss: 2.5339 Acc: 0.6167\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 2.5952 Acc: 0.6353\n",
      "val Loss: 2.5424 Acc: 0.6167\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 2.5453 Acc: 0.6657\n",
      "val Loss: 2.5287 Acc: 0.6108\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 2.5488 Acc: 0.6529\n",
      "val Loss: 2.5365 Acc: 0.6108\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 2.5353 Acc: 0.6451\n",
      "val Loss: 2.5260 Acc: 0.6127\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 2.5645 Acc: 0.6402\n",
      "val Loss: 2.5211 Acc: 0.6069\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 2.6140 Acc: 0.6275\n",
      "val Loss: 2.5281 Acc: 0.6137\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 2.5413 Acc: 0.6510\n",
      "val Loss: 2.5251 Acc: 0.6118\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 2.5486 Acc: 0.6461\n",
      "val Loss: 2.5243 Acc: 0.6127\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 2.5314 Acc: 0.6529\n",
      "val Loss: 2.5063 Acc: 0.6118\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 2.5422 Acc: 0.6324\n",
      "val Loss: 2.5164 Acc: 0.6127\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 2.5901 Acc: 0.6255\n",
      "val Loss: 2.5229 Acc: 0.6098\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 2.5350 Acc: 0.6402\n",
      "val Loss: 2.5142 Acc: 0.6137\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 2.5611 Acc: 0.6324\n",
      "val Loss: 2.5156 Acc: 0.6147\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 2.5229 Acc: 0.6402\n",
      "val Loss: 2.5199 Acc: 0.6098\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 2.5334 Acc: 0.6441\n",
      "val Loss: 2.5146 Acc: 0.6118\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 2.5157 Acc: 0.6451\n",
      "val Loss: 2.5057 Acc: 0.6078\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 2.5200 Acc: 0.6471\n",
      "val Loss: 2.4935 Acc: 0.6157\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 2.5307 Acc: 0.6441\n",
      "val Loss: 2.5043 Acc: 0.6147\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 2.5794 Acc: 0.6294\n",
      "val Loss: 2.5043 Acc: 0.6098\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 2.5550 Acc: 0.6343\n",
      "val Loss: 2.5003 Acc: 0.6157\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 2.5371 Acc: 0.6490\n",
      "val Loss: 2.4874 Acc: 0.6147\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 2.5161 Acc: 0.6569\n",
      "val Loss: 2.4941 Acc: 0.6157\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 2.5184 Acc: 0.6490\n",
      "val Loss: 2.4873 Acc: 0.6137\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 2.4683 Acc: 0.6510\n",
      "val Loss: 2.4883 Acc: 0.6118\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 2.4730 Acc: 0.6598\n",
      "val Loss: 2.4905 Acc: 0.6108\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 2.5260 Acc: 0.6529\n",
      "val Loss: 2.4918 Acc: 0.6147\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 2.4946 Acc: 0.6441\n",
      "val Loss: 2.4914 Acc: 0.6176\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 2.5232 Acc: 0.6539\n",
      "val Loss: 2.4860 Acc: 0.6137\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 2.4998 Acc: 0.6343\n",
      "val Loss: 2.4832 Acc: 0.6167\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 2.5149 Acc: 0.6343\n",
      "val Loss: 2.4741 Acc: 0.6118\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 2.5170 Acc: 0.6363\n",
      "val Loss: 2.4754 Acc: 0.6176\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 2.4998 Acc: 0.6480\n",
      "val Loss: 2.4794 Acc: 0.6147\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 2.4860 Acc: 0.6382\n",
      "val Loss: 2.4593 Acc: 0.6216\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 2.4955 Acc: 0.6637\n",
      "val Loss: 2.4782 Acc: 0.6167\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 2.5204 Acc: 0.6373\n",
      "val Loss: 2.4906 Acc: 0.6157\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 2.4794 Acc: 0.6559\n",
      "val Loss: 2.4818 Acc: 0.6196\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 2.4573 Acc: 0.6549\n",
      "val Loss: 2.4597 Acc: 0.6127\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 2.5148 Acc: 0.6343\n",
      "val Loss: 2.4764 Acc: 0.6176\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 2.4868 Acc: 0.6510\n",
      "val Loss: 2.4739 Acc: 0.6147\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 2.5134 Acc: 0.6569\n",
      "val Loss: 2.4834 Acc: 0.6157\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 2.4829 Acc: 0.6510\n",
      "val Loss: 2.4723 Acc: 0.6176\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 2.5179 Acc: 0.6471\n",
      "val Loss: 2.4819 Acc: 0.6118\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 2.5262 Acc: 0.6363\n",
      "val Loss: 2.4868 Acc: 0.6167\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 2.5145 Acc: 0.6343\n",
      "val Loss: 2.4831 Acc: 0.6157\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 2.5227 Acc: 0.6343\n",
      "val Loss: 2.4722 Acc: 0.6176\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 2.4962 Acc: 0.6471\n",
      "val Loss: 2.4745 Acc: 0.6216\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 2.5225 Acc: 0.6265\n",
      "val Loss: 2.4764 Acc: 0.6176\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 2.4955 Acc: 0.6402\n",
      "val Loss: 2.4712 Acc: 0.6176\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 2.4943 Acc: 0.6598\n",
      "val Loss: 2.4708 Acc: 0.6167\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 2.5255 Acc: 0.6402\n",
      "val Loss: 2.4813 Acc: 0.6108\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 2.4900 Acc: 0.6333\n",
      "val Loss: 2.4782 Acc: 0.6196\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 2.5189 Acc: 0.6578\n",
      "val Loss: 2.4810 Acc: 0.6167\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 2.4751 Acc: 0.6343\n",
      "val Loss: 2.4731 Acc: 0.6167\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 2.5183 Acc: 0.6441\n",
      "val Loss: 2.4733 Acc: 0.6186\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 2.5393 Acc: 0.6422\n",
      "val Loss: 2.4766 Acc: 0.6167\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 2.4801 Acc: 0.6588\n",
      "val Loss: 2.4791 Acc: 0.6157\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 2.4677 Acc: 0.6520\n",
      "val Loss: 2.4730 Acc: 0.6147\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 2.4774 Acc: 0.6520\n",
      "val Loss: 2.4869 Acc: 0.6196\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 2.5136 Acc: 0.6333\n",
      "val Loss: 2.4880 Acc: 0.6137\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 2.5107 Acc: 0.6422\n",
      "val Loss: 2.4843 Acc: 0.6157\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 2.5188 Acc: 0.6402\n",
      "val Loss: 2.4792 Acc: 0.6147\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 2.5123 Acc: 0.6324\n",
      "val Loss: 2.4740 Acc: 0.6147\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 2.4805 Acc: 0.6382\n",
      "val Loss: 2.4756 Acc: 0.6147\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 2.4733 Acc: 0.6627\n",
      "val Loss: 2.4728 Acc: 0.6147\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 2.4612 Acc: 0.6706\n",
      "val Loss: 2.4686 Acc: 0.6167\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 2.5240 Acc: 0.6382\n",
      "val Loss: 2.4729 Acc: 0.6147\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 2.5261 Acc: 0.6373\n",
      "val Loss: 2.4778 Acc: 0.6176\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 2.5219 Acc: 0.6304\n",
      "val Loss: 2.4863 Acc: 0.6147\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 2.5301 Acc: 0.6275\n",
      "val Loss: 2.4774 Acc: 0.6147\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 2.5498 Acc: 0.6167\n",
      "val Loss: 2.4758 Acc: 0.6147\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 2.4908 Acc: 0.6539\n",
      "val Loss: 2.4771 Acc: 0.6137\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 2.5146 Acc: 0.6578\n",
      "val Loss: 2.4804 Acc: 0.6108\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 2.4773 Acc: 0.6451\n",
      "val Loss: 2.4672 Acc: 0.6147\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 2.4950 Acc: 0.6324\n",
      "val Loss: 2.4785 Acc: 0.6137\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 2.5164 Acc: 0.6471\n",
      "val Loss: 2.4834 Acc: 0.6127\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 2.5432 Acc: 0.6078\n",
      "val Loss: 2.4743 Acc: 0.6176\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 2.5119 Acc: 0.6451\n",
      "val Loss: 2.4784 Acc: 0.6147\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 2.5248 Acc: 0.6510\n",
      "val Loss: 2.4742 Acc: 0.6127\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 2.5270 Acc: 0.6451\n",
      "val Loss: 2.4747 Acc: 0.6167\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 2.5174 Acc: 0.6392\n",
      "val Loss: 2.4738 Acc: 0.6127\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 2.4991 Acc: 0.6333\n",
      "val Loss: 2.4791 Acc: 0.6147\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 2.5028 Acc: 0.6431\n",
      "val Loss: 2.4759 Acc: 0.6147\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 2.4976 Acc: 0.6382\n",
      "val Loss: 2.4700 Acc: 0.6157\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 2.5270 Acc: 0.6333\n",
      "val Loss: 2.4824 Acc: 0.6186\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 2.4761 Acc: 0.6500\n",
      "val Loss: 2.4742 Acc: 0.6196\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 2.4694 Acc: 0.6461\n",
      "val Loss: 2.4631 Acc: 0.6167\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 2.4980 Acc: 0.6422\n",
      "val Loss: 2.4703 Acc: 0.6127\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 2.5295 Acc: 0.6569\n",
      "val Loss: 2.4763 Acc: 0.6137\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 2.4489 Acc: 0.6627\n",
      "val Loss: 2.4658 Acc: 0.6167\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 2.5286 Acc: 0.6294\n",
      "val Loss: 2.4797 Acc: 0.6157\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 2.4623 Acc: 0.6402\n",
      "val Loss: 2.4693 Acc: 0.6157\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 2.5146 Acc: 0.6363\n",
      "val Loss: 2.4750 Acc: 0.6167\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 2.4471 Acc: 0.6775\n",
      "val Loss: 2.4635 Acc: 0.6147\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 2.4750 Acc: 0.6431\n",
      "val Loss: 2.4611 Acc: 0.6167\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 2.5209 Acc: 0.6490\n",
      "val Loss: 2.4838 Acc: 0.6196\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 2.5077 Acc: 0.6422\n",
      "val Loss: 2.4661 Acc: 0.6157\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 2.5045 Acc: 0.6520\n",
      "val Loss: 2.4730 Acc: 0.6167\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 2.5086 Acc: 0.6265\n",
      "val Loss: 2.4725 Acc: 0.6147\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 2.5202 Acc: 0.6255\n",
      "val Loss: 2.4629 Acc: 0.6147\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 2.4849 Acc: 0.6363\n",
      "val Loss: 2.4678 Acc: 0.6176\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 2.5059 Acc: 0.6471\n",
      "val Loss: 2.4723 Acc: 0.6127\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 2.4624 Acc: 0.6598\n",
      "val Loss: 2.4677 Acc: 0.6147\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 2.5146 Acc: 0.6559\n",
      "val Loss: 2.4734 Acc: 0.6167\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 2.5260 Acc: 0.6412\n",
      "val Loss: 2.4804 Acc: 0.6167\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 2.4933 Acc: 0.6559\n",
      "val Loss: 2.4715 Acc: 0.6167\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 2.4944 Acc: 0.6461\n",
      "val Loss: 2.4773 Acc: 0.6137\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 2.4801 Acc: 0.6422\n",
      "val Loss: 2.4681 Acc: 0.6216\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 2.5208 Acc: 0.6353\n",
      "val Loss: 2.4736 Acc: 0.6127\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 2.4832 Acc: 0.6627\n",
      "val Loss: 2.4701 Acc: 0.6137\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 2.5156 Acc: 0.6265\n",
      "val Loss: 2.4733 Acc: 0.6147\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 2.5080 Acc: 0.6549\n",
      "val Loss: 2.4779 Acc: 0.6147\n",
      "\n",
      "Training complete in 15m 9s\n",
      "Best val Acc: 0.621569\n"
     ]
    }
   ],
   "source": [
    "LR_SET = [10**i for i in range(-3, 1)]\n",
    "\n",
    "for lr in LR_SET:\n",
    "    print(f'\\n----- Performing experiment with fixed lr={lr} -----\\n')\n",
    "    \n",
    "    model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "    for param in model_conv.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "    num_ftrs = model_conv.fc.in_features\n",
    "    model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "    model_conv = model_conv.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that only parameters of final layer are being optimized as\n",
    "    # opposed to before.\n",
    "    optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 60 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=60, gamma=0.1)\n",
    "    \n",
    "    # Train model\n",
    "    model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                             exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation accuracy when using `ResNet50` as a feature extractor on `vgg-flowers` for the various learning rates are below.\n",
    "\n",
    "| Learning Rate Setting | Best Validation Accuracy |\n",
    "| ----------------------|--------------------------|\n",
    "| 0.001                 | 0.615                    |\n",
    "| 0.01                  | 0.624                    |\n",
    "| 0.1                   | 0.632                    |\n",
    "| 1                     | 0.622                    |\n",
    "\n",
    "From above, we can see that 0.1 is the best learning rate for using `ResNet50` as a feature extractor for the `vgg-flowers` VDD dataset.\n",
    "\n",
    "* b: (4 points)\n",
    "\n",
    "Given the two transfer learning approaches (Fine Tuning and Feature Extraction) and the various learning rate schemes, using Fine Tuning with a fixed learning rate equal to 0.01 resulted in the best validation accuracy on the holdout dataset. This isn't surprising, as the `ResNet50` model was pretrained on `ImageNet`, not the `vgg-flowers` dataset. Allowing for weight updates on the layers prior to the final Dense layers allows for `ResNet50` to learn features and representations specific to the `vgg-flowers` dataset.\n",
    "\n",
    "### Problem 2: *Weakly and Semi-Supervised Learning for Image Classification* (20 points)\n",
    "\n",
    "#### Q1 (2 points)\n",
    "\n",
    "The difference between weakly and semi-supervised pretraining is related to the labeling mechanism. Weakly supervised learning uses tags or labels, which can often be noisy, innacurate, and imbalanced, to train models on. The noise and imbalance in the tags can reduce performance of models. Semi-supervised learning takes advantage of a teacher-student architecture to select the Top-K images per class with the teacher model and train the student model on the subsampled dataset.\n",
    "\n",
    "#### Q2\n",
    "\n",
    "* a: (2 points) \n",
    "\n",
    "The model trained using hashtags is robust against noise in the labels. To test this, Mahajan et al randomly injected, 10%, 25%, and 50% label noise into the targets, and observed that there was only a 1 and 2 percent degredation in performance when injection 10 and 25 percent label noise, respectively. This showed that label noise is a miniscule issue to models trained on billions of images. \n",
    "\n",
    "* b: (2 points)\n",
    "\n",
    "Hashtags exhibit a Zipfian distribution, and it has been shown that resampling Zipfian distributions reduces the impact of the head of the word distribution when pretraining for transfer learning.\n",
    "\n",
    "#### Q3\n",
    "\n",
    "* a: (4 points)\n",
    "\n",
    "There are two models, the teacher and the student, for the purposes of selecting the best data to train on per class (teacher) and training on the best data (student). The student model leverages the teacher in the sense that the output of the teacher model is a subsample of images the teacher has the most confidence in per class, and this is the data the student is trained on. This can be thought of as a distillation technique because the more inaccurate or noisy images that the teacher has less confidence in are not fed to the student for learning; only the most \"pure\" training images are left after the teacher distillation process.\n",
    "\n",
    "* b: (4 points)\n",
    "\n",
    "K and P are hyperparameters for selecting which data to feed from the teacher to the student. K represents the Top-K images per class labelled by the teacher on the unlabelled dataset that will be fed to the student, and P represents the Top-P softmax probabilities per image that are kept for ranking the Top-K images per class. We use P > 1 to account for uncertainty in selecting only the most likely label assigned from the teacher, as well as to capture some of the less represented or infrequent labels that were learned by the teacher. A higher value for P leads to a more robust dataset to select the Top-K images per class from for the student.\n",
    "\n",
    "* c: (4 points)\n",
    "\n",
    "A new labelled dataset is created from the unlabbeled data by using the teacher to generate the Top-P softmax probabilities per image. From here, we rank and select the Top-K images per class from the Top-P softmax probabilities per image. It is possible for an image in the unlabelled dataset to belong to multiple classes with this approach, and Yalzin et al simply allow for an image with multiple Top-K class labels to be replicated in the dataset that is fed to the student.\n",
    "\n",
    "* d: (2 points)\n",
    "\n",
    "The accuracy of the student model demonstrates an inverted parabola shape with increasing K hyperparameter value because at a certain point, the original noise from the weakly labelled dataset is reintroduced with larger K values selected by the teacher model.\n",
    "\n",
    "### Problem 3: *PALEO, FLOPs, Platform Percent of Peak (PPP)* (20 points)\n",
    "\n",
    "#### Q1 (4 points)\n",
    "\n",
    "Achieving peak FLOPs from GPU hardware is a difficult proposition in real systems because you typically need to develop customized libraries that take advantage of intimate knowledge of the underlying GPU hardware, which most people outside of the developers of the GPU hardware possess. Platform Percent of Peak (PPP) helps capture this inefficiency by taking the average relative inefficiency of the platform compared to peak FLOPS.\n",
    "\n",
    "#### Q2 (6 points)\n",
    "\n",
    "The difference between VGG16 and VGG19 is the additional convolutional layer in the third and fourth convolution block of VGG19.\n",
    "\n",
    "The two additional convolution layers are 3x3x512 layers, where the input size to the first is 28x28x512 and the input size to the second is 14x14x512.\n",
    "\n",
    "From this, we can calculate the additional number of multiplications as follows:\n",
    "\n",
    "$$ 2 x (3 x 3 x 512) x 512 = 4718592 $$\n",
    "\n",
    "Adding this number to the number of FLOPs from Table 3 in Lu et al results in roughly 15365M FLOPs for the CONVs in VGG19.\n",
    "\n",
    "Adding the above to the unchanged number of FLOPs for POOL, ReLU, and FC layers in VGG19 results in a total of 15508M FLOPs for VGG19.\n",
    "\n",
    "#### Q3 (4 points)\n",
    "\n",
    "The measured time and sum of layerwise timings for forward pass did not match on GPUs for AlexNet, VGG16, GoogleNet, and ResNet50 due to communication and measurement overhead differences between TK1 and TX1. The authors keep GPUs iteratively running the matrix multiplication in a way that GPU cores can continuously perform multiply-add operations without synchronization, before recording the end time. After this, the measurement overhead is amortized over all the iterations, giving accurate timing estimates. When the number of iterations is large enough, the overhead is negligible.\n",
    "\n",
    "#### Q4 (6 points)\n",
    "\n",
    "The peak double-precision floating point performance of a NVIDIA Tesla K80 is 1.87 TFLOPS.\n",
    "\n",
    "The FLOPs for VGG16, GoogleNet, and ResNet50 are in the table below.\n",
    "\n",
    "| Model Architecture | FLOPS |\n",
    "| -------------------|-------|\n",
    "| VGG16              | 15503M|\n",
    "| GoogleNet          | 1606M |\n",
    "| ResNet50           | 3922M |\n",
    "\n",
    "The computation time formula is calculated as the FLOP counts of the operation divided by the FLOPS of the device.\n",
    "\n",
    "Using this formula, the computation time for the three models above is:\n",
    "\n",
    "| Model Architecture | FLOPS | K80 TFLOPS | Computation Time |\n",
    "| -------------------|-------|------------|------------------|\n",
    "| VGG16              | 15503M| 1.87       | 8 ms             |\n",
    "| GoogleNet          | 1606M | 1.87       | 0.8 ms           |\n",
    "| ResNet50           | 3922M | 1.87       | 2 ms             |\n",
    "\n",
    "Given these computation times, we can divide by 1 second to get throughput, or the number of images processed per second.\n",
    "\n",
    "| Model Architecture | FLOPS | K80 TFLOPS | Computation Time | Throughput |\n",
    "| -------------------|-------|------------|------------------|------------|\n",
    "| VGG16              | 15503M| 1.87       | 8 ms             | 125        |\n",
    "| GoogleNet          | 1606M | 1.87       | 0.8 ms           | 1250       |\n",
    "| ResNet50           | 3922M | 1.87       | 2 ms             | 500        |\n",
    "\n",
    "\n",
    "### Problem 4: *Optimus, Learning and Resource Models, Performance-Cost Tradeoffs* (30 points)\n",
    "\n",
    "#### Q1: (15 points)\n",
    "\n",
    "I worked on this question with Shawn Pachgade, Roberto Ponce, Nick Christman, and Wang Yao.\n",
    "\n",
    "I used [this code repository](https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py) for building out my `ResNet20` implementation.\n",
    "\n",
    "So far, I've testing on the following GPUs (this is for my own record keeping):\n",
    "\n",
    "- [ ] K80\n",
    "- [X] P100\n",
    "- [ ] V100\n",
    "\n",
    "Import the necessary packages and set up the ResNet20 module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, resnet_layers, hardware, dataloaders, criterion, optimizer, scheduler, num_epochs=350):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    metrics = []\n",
    "    training_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            epoch_phase_start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                step_start_time = time.time()\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        metrics.append({\n",
    "                            'resnet_layers': resnet_layers,\n",
    "                            'hardware': hardware,\n",
    "                            'epoch': epoch,\n",
    "                            'training_step': training_step,\n",
    "                            'training_step_loss': loss.item(),\n",
    "                            'training_step_time': time.time() - step_start_time\n",
    "                        })\n",
    "                        \n",
    "                        training_step += 1\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_phase_end_time = time.time()\n",
    "            \n",
    "            print(f'{phase} Loss: {round(epoch_loss, 4)} Acc: {round(epoch_acc.item(), 4)}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc.item()\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                       \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60}m {time_elapsed % 60}s')\n",
    "    print(f'Best val Acc: {round(best_acc, 4)}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # set up return structure\n",
    "    return_df = pd.DataFrame(data=metrics)\n",
    "    \n",
    "    return model, return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR-10 data with augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset sizes: {'train': 50000, 'val': 10000}\n",
      "Class names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "BATCHSIZE = 128\n",
    "DATA_DIR = '~/data/cifar10'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True,\n",
    "                                        download=True, transform=data_transforms['train'])\n",
    "\n",
    "val_set = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False,\n",
    "                                       download=True, transform=data_transforms['val'])\n",
    "\n",
    "image_datasets = {'train': train_set, 'val': val_set}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCHSIZE,\n",
    "                                              shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up learning criteria and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/349\n",
      "----------\n",
      "train Loss: 1.527 Acc: 0.4428\n",
      "val Loss: 1.1685 Acc: 0.5837\n",
      "\n",
      "Epoch 1/349\n",
      "----------\n",
      "train Loss: 0.9577 Acc: 0.6582\n",
      "val Loss: 1.0866 Acc: 0.6218\n",
      "\n",
      "Epoch 2/349\n",
      "----------\n",
      "train Loss: 0.7458 Acc: 0.7365\n",
      "val Loss: 0.8063 Acc: 0.7218\n",
      "\n",
      "Epoch 3/349\n",
      "----------\n",
      "train Loss: 0.6183 Acc: 0.7826\n",
      "val Loss: 0.7995 Acc: 0.7224\n",
      "\n",
      "Epoch 4/349\n",
      "----------\n",
      "train Loss: 0.5232 Acc: 0.8168\n",
      "val Loss: 0.6508 Acc: 0.7854\n",
      "\n",
      "Epoch 5/349\n",
      "----------\n",
      "train Loss: 0.4564 Acc: 0.8403\n",
      "val Loss: 0.745 Acc: 0.7593\n",
      "\n",
      "Epoch 6/349\n",
      "----------\n",
      "train Loss: 0.4014 Acc: 0.8595\n",
      "val Loss: 0.612 Acc: 0.7992\n",
      "\n",
      "Epoch 7/349\n",
      "----------\n",
      "train Loss: 0.3528 Acc: 0.8772\n",
      "val Loss: 0.6793 Acc: 0.779\n",
      "\n",
      "Epoch 8/349\n",
      "----------\n",
      "train Loss: 0.3118 Acc: 0.8889\n",
      "val Loss: 0.7326 Acc: 0.773\n",
      "\n",
      "Epoch 9/349\n",
      "----------\n",
      "train Loss: 0.2695 Acc: 0.9057\n",
      "val Loss: 0.6847 Acc: 0.798\n",
      "\n",
      "Epoch 10/349\n",
      "----------\n",
      "train Loss: 0.2308 Acc: 0.9174\n",
      "val Loss: 0.6987 Acc: 0.7922\n",
      "\n",
      "Epoch 11/349\n",
      "----------\n",
      "train Loss: 0.2029 Acc: 0.9279\n",
      "val Loss: 0.8253 Acc: 0.7818\n",
      "\n",
      "Epoch 12/349\n",
      "----------\n",
      "train Loss: 0.177 Acc: 0.937\n",
      "val Loss: 0.7976 Acc: 0.7908\n",
      "\n",
      "Epoch 13/349\n",
      "----------\n",
      "train Loss: 0.1683 Acc: 0.9404\n",
      "val Loss: 0.8161 Acc: 0.7812\n",
      "\n",
      "Epoch 14/349\n",
      "----------\n",
      "train Loss: 0.1406 Acc: 0.9497\n",
      "val Loss: 0.8411 Acc: 0.7961\n",
      "\n",
      "Epoch 15/349\n",
      "----------\n",
      "train Loss: 0.1259 Acc: 0.9542\n",
      "val Loss: 0.8968 Acc: 0.7847\n",
      "\n",
      "Epoch 16/349\n",
      "----------\n",
      "train Loss: 0.1067 Acc: 0.9616\n",
      "val Loss: 0.8808 Acc: 0.7949\n",
      "\n",
      "Epoch 17/349\n",
      "----------\n",
      "train Loss: 0.0921 Acc: 0.9671\n",
      "val Loss: 1.009 Acc: 0.7838\n",
      "\n",
      "Epoch 18/349\n",
      "----------\n",
      "train Loss: 0.0847 Acc: 0.9699\n",
      "val Loss: 1.0788 Acc: 0.7841\n",
      "\n",
      "Epoch 19/349\n",
      "----------\n",
      "train Loss: 0.0879 Acc: 0.9689\n",
      "val Loss: 1.0063 Acc: 0.7912\n",
      "\n",
      "Epoch 20/349\n",
      "----------\n",
      "train Loss: 0.0741 Acc: 0.9745\n",
      "val Loss: 0.9658 Acc: 0.7991\n",
      "\n",
      "Epoch 21/349\n",
      "----------\n",
      "train Loss: 0.0559 Acc: 0.9802\n",
      "val Loss: 1.0243 Acc: 0.8021\n",
      "\n",
      "Epoch 22/349\n",
      "----------\n",
      "train Loss: 0.0572 Acc: 0.9795\n",
      "val Loss: 1.0644 Acc: 0.7919\n",
      "\n",
      "Epoch 23/349\n",
      "----------\n",
      "train Loss: 0.0595 Acc: 0.9788\n",
      "val Loss: 1.0628 Acc: 0.8059\n",
      "\n",
      "Epoch 24/349\n",
      "----------\n",
      "train Loss: 0.0515 Acc: 0.9822\n",
      "val Loss: 1.003 Acc: 0.8072\n",
      "\n",
      "Epoch 25/349\n",
      "----------\n",
      "train Loss: 0.04 Acc: 0.986\n",
      "val Loss: 1.1236 Acc: 0.7976\n",
      "\n",
      "Epoch 26/349\n",
      "----------\n",
      "train Loss: 0.0354 Acc: 0.9876\n",
      "val Loss: 1.3127 Acc: 0.784\n",
      "\n",
      "Epoch 27/349\n",
      "----------\n",
      "train Loss: 0.0408 Acc: 0.9859\n",
      "val Loss: 1.0864 Acc: 0.8051\n",
      "\n",
      "Epoch 28/349\n",
      "----------\n",
      "train Loss: 0.0299 Acc: 0.9893\n",
      "val Loss: 1.2358 Acc: 0.7993\n",
      "\n",
      "Epoch 29/349\n",
      "----------\n",
      "train Loss: 0.0392 Acc: 0.9863\n",
      "val Loss: 1.0956 Acc: 0.8119\n",
      "\n",
      "Epoch 30/349\n",
      "----------\n",
      "train Loss: 0.0291 Acc: 0.99\n",
      "val Loss: 1.1161 Acc: 0.8107\n",
      "\n",
      "Epoch 31/349\n",
      "----------\n",
      "train Loss: 0.0229 Acc: 0.9919\n",
      "val Loss: 1.1326 Acc: 0.8098\n",
      "\n",
      "Epoch 32/349\n",
      "----------\n",
      "train Loss: 0.0311 Acc: 0.9891\n",
      "val Loss: 1.1434 Acc: 0.8052\n",
      "\n",
      "Epoch 33/349\n",
      "----------\n",
      "train Loss: 0.0334 Acc: 0.9879\n",
      "val Loss: 1.0613 Acc: 0.8137\n",
      "\n",
      "Epoch 34/349\n",
      "----------\n",
      "train Loss: 0.0217 Acc: 0.993\n",
      "val Loss: 1.1184 Acc: 0.8185\n",
      "\n",
      "Epoch 35/349\n",
      "----------\n",
      "train Loss: 0.0174 Acc: 0.994\n",
      "val Loss: 1.2097 Acc: 0.8078\n",
      "\n",
      "Epoch 36/349\n",
      "----------\n",
      "train Loss: 0.0185 Acc: 0.9934\n",
      "val Loss: 1.2093 Acc: 0.8129\n",
      "\n",
      "Epoch 37/349\n",
      "----------\n",
      "train Loss: 0.0199 Acc: 0.9931\n",
      "val Loss: 1.1797 Acc: 0.8088\n",
      "\n",
      "Epoch 38/349\n",
      "----------\n",
      "train Loss: 0.0198 Acc: 0.9932\n",
      "val Loss: 1.2405 Acc: 0.8131\n",
      "\n",
      "Epoch 39/349\n",
      "----------\n",
      "train Loss: 0.017 Acc: 0.9944\n",
      "val Loss: 1.102 Acc: 0.8148\n",
      "\n",
      "Epoch 40/349\n",
      "----------\n",
      "train Loss: 0.0161 Acc: 0.9944\n",
      "val Loss: 1.153 Acc: 0.8175\n",
      "\n",
      "Epoch 41/349\n",
      "----------\n",
      "train Loss: 0.0186 Acc: 0.9937\n",
      "val Loss: 1.1975 Acc: 0.8149\n",
      "\n",
      "Epoch 42/349\n",
      "----------\n",
      "train Loss: 0.0151 Acc: 0.9946\n",
      "val Loss: 1.2469 Acc: 0.8122\n",
      "\n",
      "Epoch 43/349\n",
      "----------\n",
      "train Loss: 0.0179 Acc: 0.9941\n",
      "val Loss: 1.3004 Acc: 0.8053\n",
      "\n",
      "Epoch 44/349\n",
      "----------\n",
      "train Loss: 0.0191 Acc: 0.9929\n",
      "val Loss: 1.2545 Acc: 0.8081\n",
      "\n",
      "Epoch 45/349\n",
      "----------\n",
      "train Loss: 0.0155 Acc: 0.9949\n",
      "val Loss: 1.2125 Acc: 0.8187\n",
      "\n",
      "Epoch 46/349\n",
      "----------\n",
      "train Loss: 0.0157 Acc: 0.9946\n",
      "val Loss: 1.28 Acc: 0.8121\n",
      "\n",
      "Epoch 47/349\n",
      "----------\n",
      "train Loss: 0.0133 Acc: 0.9955\n",
      "val Loss: 1.3631 Acc: 0.8036\n",
      "\n",
      "Epoch 48/349\n",
      "----------\n",
      "train Loss: 0.0147 Acc: 0.9948\n",
      "val Loss: 1.3233 Acc: 0.8086\n",
      "\n",
      "Epoch 49/349\n",
      "----------\n",
      "train Loss: 0.0157 Acc: 0.9948\n",
      "val Loss: 1.2608 Acc: 0.8171\n",
      "\n",
      "Epoch 50/349\n",
      "----------\n",
      "train Loss: 0.0112 Acc: 0.9964\n",
      "val Loss: 1.2712 Acc: 0.8154\n",
      "\n",
      "Epoch 51/349\n",
      "----------\n",
      "train Loss: 0.01 Acc: 0.9965\n",
      "val Loss: 1.3524 Acc: 0.8071\n",
      "\n",
      "Epoch 52/349\n",
      "----------\n",
      "train Loss: 0.0097 Acc: 0.9969\n",
      "val Loss: 1.2437 Acc: 0.8183\n",
      "\n",
      "Epoch 53/349\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.9969\n",
      "val Loss: 1.2603 Acc: 0.8177\n",
      "\n",
      "Epoch 54/349\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.9976\n",
      "val Loss: 1.3234 Acc: 0.8134\n",
      "\n",
      "Epoch 55/349\n",
      "----------\n",
      "train Loss: 0.0097 Acc: 0.9966\n",
      "val Loss: 1.3907 Acc: 0.8095\n",
      "\n",
      "Epoch 56/349\n",
      "----------\n",
      "train Loss: 0.0125 Acc: 0.9958\n",
      "val Loss: 1.4178 Acc: 0.807\n",
      "\n",
      "Epoch 57/349\n",
      "----------\n",
      "train Loss: 0.0218 Acc: 0.9924\n",
      "val Loss: 1.3662 Acc: 0.8161\n",
      "\n",
      "Epoch 58/349\n",
      "----------\n",
      "train Loss: 0.0142 Acc: 0.9952\n",
      "val Loss: 1.2805 Acc: 0.8176\n",
      "\n",
      "Epoch 59/349\n",
      "----------\n",
      "train Loss: 0.0116 Acc: 0.996\n",
      "val Loss: 1.2762 Acc: 0.8168\n",
      "\n",
      "Epoch 60/349\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.9968\n",
      "val Loss: 1.3991 Acc: 0.8087\n",
      "\n",
      "Epoch 61/349\n",
      "----------\n",
      "train Loss: 0.0046 Acc: 0.9987\n",
      "val Loss: 1.2609 Acc: 0.8228\n",
      "\n",
      "Epoch 62/349\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 0.9987\n",
      "val Loss: 1.2992 Acc: 0.8232\n",
      "\n",
      "Epoch 63/349\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 0.9988\n",
      "val Loss: 1.2805 Acc: 0.8223\n",
      "\n",
      "Epoch 64/349\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 0.999\n",
      "val Loss: 1.2984 Acc: 0.8197\n",
      "\n",
      "Epoch 65/349\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.9991\n",
      "val Loss: 1.3079 Acc: 0.8237\n",
      "\n",
      "Epoch 66/349\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9995\n",
      "val Loss: 1.3034 Acc: 0.8239\n",
      "\n",
      "Epoch 67/349\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9997\n",
      "val Loss: 1.2761 Acc: 0.8226\n",
      "\n",
      "Epoch 68/349\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9998\n",
      "val Loss: 1.305 Acc: 0.8243\n",
      "\n",
      "Epoch 69/349\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 1.0\n",
      "val Loss: 1.2823 Acc: 0.8263\n",
      "\n",
      "Epoch 70/349\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 1.0\n",
      "val Loss: 1.3111 Acc: 0.8268\n",
      "\n",
      "Epoch 71/349\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 1.0\n",
      "val Loss: 1.2915 Acc: 0.8272\n",
      "\n",
      "Epoch 72/349\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 1.0\n",
      "val Loss: 1.2823 Acc: 0.8292\n",
      "\n",
      "Epoch 73/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3022 Acc: 0.8288\n",
      "\n",
      "Epoch 74/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.2954 Acc: 0.8303\n",
      "\n",
      "Epoch 75/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.2997 Acc: 0.8302\n",
      "\n",
      "Epoch 76/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.2931 Acc: 0.8289\n",
      "\n",
      "Epoch 77/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3003 Acc: 0.8303\n",
      "\n",
      "Epoch 78/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3033 Acc: 0.8285\n",
      "\n",
      "Epoch 79/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3081 Acc: 0.828\n",
      "\n",
      "Epoch 80/349\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 1.0\n",
      "val Loss: 1.3161 Acc: 0.8276\n",
      "\n",
      "Epoch 81/349\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 1.0\n",
      "val Loss: 1.3034 Acc: 0.829\n",
      "\n",
      "Epoch 82/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3045 Acc: 0.8285\n",
      "\n",
      "Epoch 83/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3105 Acc: 0.8294\n",
      "\n",
      "Epoch 84/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3121 Acc: 0.8296\n",
      "\n",
      "Epoch 85/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3117 Acc: 0.8277\n",
      "\n",
      "Epoch 86/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3198 Acc: 0.8311\n",
      "\n",
      "Epoch 87/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3127 Acc: 0.8283\n",
      "\n",
      "Epoch 88/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3298 Acc: 0.8296\n",
      "\n",
      "Epoch 89/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3251 Acc: 0.8292\n",
      "\n",
      "Epoch 90/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3248 Acc: 0.8298\n",
      "\n",
      "Epoch 91/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3343 Acc: 0.8299\n",
      "\n",
      "Epoch 92/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3241 Acc: 0.8298\n",
      "\n",
      "Epoch 93/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3262 Acc: 0.8289\n",
      "\n",
      "Epoch 94/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3316 Acc: 0.83\n",
      "\n",
      "Epoch 95/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3468 Acc: 0.8306\n",
      "\n",
      "Epoch 96/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3247 Acc: 0.8305\n",
      "\n",
      "Epoch 97/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3197 Acc: 0.8293\n",
      "\n",
      "Epoch 98/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3399 Acc: 0.8298\n",
      "\n",
      "Epoch 99/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3365 Acc: 0.8293\n",
      "\n",
      "Epoch 100/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3311 Acc: 0.8296\n",
      "\n",
      "Epoch 101/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3295 Acc: 0.8297\n",
      "\n",
      "Epoch 102/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3272 Acc: 0.8287\n",
      "\n",
      "Epoch 103/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.329 Acc: 0.8292\n",
      "\n",
      "Epoch 104/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3281 Acc: 0.8304\n",
      "\n",
      "Epoch 105/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.324 Acc: 0.8288\n",
      "\n",
      "Epoch 106/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3322 Acc: 0.8293\n",
      "\n",
      "Epoch 107/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3236 Acc: 0.8304\n",
      "\n",
      "Epoch 108/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3302 Acc: 0.8296\n",
      "\n",
      "Epoch 109/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3344 Acc: 0.8288\n",
      "\n",
      "Epoch 110/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3254 Acc: 0.8291\n",
      "\n",
      "Epoch 111/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3255 Acc: 0.8306\n",
      "\n",
      "Epoch 112/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3305 Acc: 0.83\n",
      "\n",
      "Epoch 113/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.335 Acc: 0.8293\n",
      "\n",
      "Epoch 114/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3295 Acc: 0.8302\n",
      "\n",
      "Epoch 115/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3284 Acc: 0.8294\n",
      "\n",
      "Epoch 116/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3277 Acc: 0.8291\n",
      "\n",
      "Epoch 117/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3249 Acc: 0.8291\n",
      "\n",
      "Epoch 118/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3154 Acc: 0.8305\n",
      "\n",
      "Epoch 119/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3267 Acc: 0.8304\n",
      "\n",
      "Epoch 120/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3283 Acc: 0.8292\n",
      "\n",
      "Epoch 121/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3359 Acc: 0.8306\n",
      "\n",
      "Epoch 122/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3314 Acc: 0.8304\n",
      "\n",
      "Epoch 123/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3254 Acc: 0.8303\n",
      "\n",
      "Epoch 124/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3327 Acc: 0.8307\n",
      "\n",
      "Epoch 125/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3296 Acc: 0.8305\n",
      "\n",
      "Epoch 126/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3342 Acc: 0.83\n",
      "\n",
      "Epoch 127/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3373 Acc: 0.8293\n",
      "\n",
      "Epoch 128/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.332 Acc: 0.8301\n",
      "\n",
      "Epoch 129/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3358 Acc: 0.8305\n",
      "\n",
      "Epoch 130/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3279 Acc: 0.8303\n",
      "\n",
      "Epoch 131/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3179 Acc: 0.831\n",
      "\n",
      "Epoch 132/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3331 Acc: 0.8301\n",
      "\n",
      "Epoch 133/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3251 Acc: 0.8305\n",
      "\n",
      "Epoch 134/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.333 Acc: 0.8303\n",
      "\n",
      "Epoch 135/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3236 Acc: 0.83\n",
      "\n",
      "Epoch 136/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.333 Acc: 0.8303\n",
      "\n",
      "Epoch 137/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.344 Acc: 0.8295\n",
      "\n",
      "Epoch 138/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3286 Acc: 0.829\n",
      "\n",
      "Epoch 139/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3294 Acc: 0.83\n",
      "\n",
      "Epoch 140/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3287 Acc: 0.8299\n",
      "\n",
      "Epoch 141/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3324 Acc: 0.8315\n",
      "\n",
      "Epoch 142/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3259 Acc: 0.8301\n",
      "\n",
      "Epoch 143/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.328 Acc: 0.8295\n",
      "\n",
      "Epoch 144/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3383 Acc: 0.8306\n",
      "\n",
      "Epoch 145/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3264 Acc: 0.8293\n",
      "\n",
      "Epoch 146/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3333 Acc: 0.8295\n",
      "\n",
      "Epoch 147/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3313 Acc: 0.8293\n",
      "\n",
      "Epoch 148/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3204 Acc: 0.8304\n",
      "\n",
      "Epoch 149/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3264 Acc: 0.8316\n",
      "\n",
      "Epoch 150/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3371 Acc: 0.8299\n",
      "\n",
      "Epoch 151/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3316 Acc: 0.8294\n",
      "\n",
      "Epoch 152/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3334 Acc: 0.8301\n",
      "\n",
      "Epoch 153/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3381 Acc: 0.8298\n",
      "\n",
      "Epoch 154/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3315 Acc: 0.8307\n",
      "\n",
      "Epoch 155/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3392 Acc: 0.8303\n",
      "\n",
      "Epoch 156/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3365 Acc: 0.8299\n",
      "\n",
      "Epoch 157/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3285 Acc: 0.8289\n",
      "\n",
      "Epoch 158/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3289 Acc: 0.8298\n",
      "\n",
      "Epoch 159/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3321 Acc: 0.8298\n",
      "\n",
      "Epoch 160/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3375 Acc: 0.829\n",
      "\n",
      "Epoch 161/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3287 Acc: 0.8316\n",
      "\n",
      "Epoch 162/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3326 Acc: 0.8302\n",
      "\n",
      "Epoch 163/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3312 Acc: 0.83\n",
      "\n",
      "Epoch 164/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3315 Acc: 0.8308\n",
      "\n",
      "Epoch 165/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3323 Acc: 0.8312\n",
      "\n",
      "Epoch 166/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3387 Acc: 0.8305\n",
      "\n",
      "Epoch 167/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3327 Acc: 0.8299\n",
      "\n",
      "Epoch 168/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3275 Acc: 0.8312\n",
      "\n",
      "Epoch 169/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3235 Acc: 0.8314\n",
      "\n",
      "Epoch 170/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3296 Acc: 0.8305\n",
      "\n",
      "Epoch 171/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3379 Acc: 0.8297\n",
      "\n",
      "Epoch 172/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3357 Acc: 0.8308\n",
      "\n",
      "Epoch 173/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.336 Acc: 0.8303\n",
      "\n",
      "Epoch 174/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3264 Acc: 0.8295\n",
      "\n",
      "Epoch 175/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3356 Acc: 0.8308\n",
      "\n",
      "Epoch 176/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3302 Acc: 0.8304\n",
      "\n",
      "Epoch 177/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3336 Acc: 0.8301\n",
      "\n",
      "Epoch 178/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3299 Acc: 0.8297\n",
      "\n",
      "Epoch 179/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3393 Acc: 0.8296\n",
      "\n",
      "Epoch 180/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3278 Acc: 0.8301\n",
      "\n",
      "Epoch 181/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3275 Acc: 0.8303\n",
      "\n",
      "Epoch 182/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3386 Acc: 0.8305\n",
      "\n",
      "Epoch 183/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3363 Acc: 0.8295\n",
      "\n",
      "Epoch 184/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3338 Acc: 0.8306\n",
      "\n",
      "Epoch 185/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3376 Acc: 0.8301\n",
      "\n",
      "Epoch 186/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3391 Acc: 0.8311\n",
      "\n",
      "Epoch 187/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3265 Acc: 0.8304\n",
      "\n",
      "Epoch 188/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3346 Acc: 0.8305\n",
      "\n",
      "Epoch 189/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3362 Acc: 0.8304\n",
      "\n",
      "Epoch 190/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3351 Acc: 0.8311\n",
      "\n",
      "Epoch 191/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3307 Acc: 0.8295\n",
      "\n",
      "Epoch 192/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3422 Acc: 0.83\n",
      "\n",
      "Epoch 193/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3264 Acc: 0.8302\n",
      "\n",
      "Epoch 194/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3288 Acc: 0.8304\n",
      "\n",
      "Epoch 195/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3339 Acc: 0.8305\n",
      "\n",
      "Epoch 196/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3437 Acc: 0.8298\n",
      "\n",
      "Epoch 197/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3365 Acc: 0.829\n",
      "\n",
      "Epoch 198/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3413 Acc: 0.8314\n",
      "\n",
      "Epoch 199/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3323 Acc: 0.8306\n",
      "\n",
      "Epoch 200/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3366 Acc: 0.8297\n",
      "\n",
      "Epoch 201/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3396 Acc: 0.8297\n",
      "\n",
      "Epoch 202/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3377 Acc: 0.8314\n",
      "\n",
      "Epoch 203/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.335 Acc: 0.8306\n",
      "\n",
      "Epoch 204/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3369 Acc: 0.83\n",
      "\n",
      "Epoch 205/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3319 Acc: 0.8311\n",
      "\n",
      "Epoch 206/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3329 Acc: 0.8297\n",
      "\n",
      "Epoch 207/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3306 Acc: 0.83\n",
      "\n",
      "Epoch 208/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.332 Acc: 0.8301\n",
      "\n",
      "Epoch 209/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3427 Acc: 0.829\n",
      "\n",
      "Epoch 210/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3372 Acc: 0.8302\n",
      "\n",
      "Epoch 211/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3367 Acc: 0.8306\n",
      "\n",
      "Epoch 212/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3338 Acc: 0.8299\n",
      "\n",
      "Epoch 213/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3382 Acc: 0.8296\n",
      "\n",
      "Epoch 214/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3374 Acc: 0.8303\n",
      "\n",
      "Epoch 215/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3332 Acc: 0.8303\n",
      "\n",
      "Epoch 216/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3344 Acc: 0.8299\n",
      "\n",
      "Epoch 217/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3315 Acc: 0.8294\n",
      "\n",
      "Epoch 218/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3281 Acc: 0.8309\n",
      "\n",
      "Epoch 219/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3397 Acc: 0.8299\n",
      "\n",
      "Epoch 220/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3341 Acc: 0.8302\n",
      "\n",
      "Epoch 221/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3343 Acc: 0.8299\n",
      "\n",
      "Epoch 222/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3355 Acc: 0.829\n",
      "\n",
      "Epoch 223/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3332 Acc: 0.8305\n",
      "\n",
      "Epoch 224/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3421 Acc: 0.8295\n",
      "\n",
      "Epoch 225/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3418 Acc: 0.8304\n",
      "\n",
      "Epoch 226/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3358 Acc: 0.8308\n",
      "\n",
      "Epoch 227/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3385 Acc: 0.8305\n",
      "\n",
      "Epoch 228/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3344 Acc: 0.829\n",
      "\n",
      "Epoch 229/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3359 Acc: 0.831\n",
      "\n",
      "Epoch 230/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3403 Acc: 0.8303\n",
      "\n",
      "Epoch 231/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3351 Acc: 0.831\n",
      "\n",
      "Epoch 232/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3439 Acc: 0.8298\n",
      "\n",
      "Epoch 233/349\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 1.0\n",
      "val Loss: 1.3417 Acc: 0.8306\n",
      "\n",
      "Epoch 234/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3386 Acc: 0.8306\n",
      "\n",
      "Epoch 235/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3339 Acc: 0.8295\n",
      "\n",
      "Epoch 236/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3409 Acc: 0.8321\n",
      "\n",
      "Epoch 237/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3316 Acc: 0.8306\n",
      "\n",
      "Epoch 238/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3397 Acc: 0.8301\n",
      "\n",
      "Epoch 239/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.329 Acc: 0.8295\n",
      "\n",
      "Epoch 240/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3438 Acc: 0.8296\n",
      "\n",
      "Epoch 241/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3401 Acc: 0.8299\n",
      "\n",
      "Epoch 242/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3354 Acc: 0.8299\n",
      "\n",
      "Epoch 243/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3324 Acc: 0.832\n",
      "\n",
      "Epoch 244/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3393 Acc: 0.8302\n",
      "\n",
      "Epoch 245/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3322 Acc: 0.8303\n",
      "\n",
      "Epoch 246/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3401 Acc: 0.8297\n",
      "\n",
      "Epoch 247/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3332 Acc: 0.8306\n",
      "\n",
      "Epoch 248/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3363 Acc: 0.8305\n",
      "\n",
      "Epoch 249/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3377 Acc: 0.8299\n",
      "\n",
      "Epoch 250/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3361 Acc: 0.8284\n",
      "\n",
      "Epoch 251/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3319 Acc: 0.8298\n",
      "\n",
      "Epoch 252/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3437 Acc: 0.8296\n",
      "\n",
      "Epoch 253/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3328 Acc: 0.8298\n",
      "\n",
      "Epoch 254/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3299 Acc: 0.8307\n",
      "\n",
      "Epoch 255/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3345 Acc: 0.8302\n",
      "\n",
      "Epoch 256/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3355 Acc: 0.8301\n",
      "\n",
      "Epoch 257/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3371 Acc: 0.8301\n",
      "\n",
      "Epoch 258/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3396 Acc: 0.8294\n",
      "\n",
      "Epoch 259/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3344 Acc: 0.8311\n",
      "\n",
      "Epoch 260/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3459 Acc: 0.8303\n",
      "\n",
      "Epoch 261/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3417 Acc: 0.8304\n",
      "\n",
      "Epoch 262/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3403 Acc: 0.8295\n",
      "\n",
      "Epoch 263/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.337 Acc: 0.8304\n",
      "\n",
      "Epoch 264/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3306 Acc: 0.829\n",
      "\n",
      "Epoch 265/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.34 Acc: 0.8299\n",
      "\n",
      "Epoch 266/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3356 Acc: 0.831\n",
      "\n",
      "Epoch 267/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3444 Acc: 0.8299\n",
      "\n",
      "Epoch 268/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3351 Acc: 0.8306\n",
      "\n",
      "Epoch 269/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.339 Acc: 0.8307\n",
      "\n",
      "Epoch 270/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.334 Acc: 0.8297\n",
      "\n",
      "Epoch 271/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3409 Acc: 0.8292\n",
      "\n",
      "Epoch 272/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3379 Acc: 0.8307\n",
      "\n",
      "Epoch 273/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3301 Acc: 0.83\n",
      "\n",
      "Epoch 274/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3328 Acc: 0.8296\n",
      "\n",
      "Epoch 275/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3371 Acc: 0.83\n",
      "\n",
      "Epoch 276/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.342 Acc: 0.8298\n",
      "\n",
      "Epoch 277/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3336 Acc: 0.8299\n",
      "\n",
      "Epoch 278/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3379 Acc: 0.8295\n",
      "\n",
      "Epoch 279/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3328 Acc: 0.8298\n",
      "\n",
      "Epoch 280/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.329 Acc: 0.8306\n",
      "\n",
      "Epoch 281/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3338 Acc: 0.8294\n",
      "\n",
      "Epoch 282/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.328 Acc: 0.8302\n",
      "\n",
      "Epoch 283/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3318 Acc: 0.8301\n",
      "\n",
      "Epoch 284/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3359 Acc: 0.8297\n",
      "\n",
      "Epoch 285/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3347 Acc: 0.8308\n",
      "\n",
      "Epoch 286/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3393 Acc: 0.8299\n",
      "\n",
      "Epoch 287/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3401 Acc: 0.8311\n",
      "\n",
      "Epoch 288/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.341 Acc: 0.8305\n",
      "\n",
      "Epoch 289/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3354 Acc: 0.831\n",
      "\n",
      "Epoch 290/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3375 Acc: 0.8294\n",
      "\n",
      "Epoch 291/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3324 Acc: 0.8307\n",
      "\n",
      "Epoch 292/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3338 Acc: 0.8299\n",
      "\n",
      "Epoch 293/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3381 Acc: 0.8307\n",
      "\n",
      "Epoch 294/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3369 Acc: 0.8301\n",
      "\n",
      "Epoch 295/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.339 Acc: 0.8308\n",
      "\n",
      "Epoch 296/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3477 Acc: 0.8308\n",
      "\n",
      "Epoch 297/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3431 Acc: 0.83\n",
      "\n",
      "Epoch 298/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3312 Acc: 0.8295\n",
      "\n",
      "Epoch 299/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3348 Acc: 0.8305\n",
      "\n",
      "Epoch 300/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3478 Acc: 0.8293\n",
      "\n",
      "Epoch 301/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3428 Acc: 0.8312\n",
      "\n",
      "Epoch 302/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3343 Acc: 0.8302\n",
      "\n",
      "Epoch 303/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.338 Acc: 0.8299\n",
      "\n",
      "Epoch 304/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3395 Acc: 0.8299\n",
      "\n",
      "Epoch 305/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3279 Acc: 0.831\n",
      "\n",
      "Epoch 306/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3285 Acc: 0.8303\n",
      "\n",
      "Epoch 307/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3427 Acc: 0.8301\n",
      "\n",
      "Epoch 308/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3439 Acc: 0.8291\n",
      "\n",
      "Epoch 309/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3315 Acc: 0.8298\n",
      "\n",
      "Epoch 310/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3339 Acc: 0.8304\n",
      "\n",
      "Epoch 311/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3388 Acc: 0.8314\n",
      "\n",
      "Epoch 312/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3298 Acc: 0.8311\n",
      "\n",
      "Epoch 313/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3445 Acc: 0.831\n",
      "\n",
      "Epoch 314/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3286 Acc: 0.8303\n",
      "\n",
      "Epoch 315/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3316 Acc: 0.8312\n",
      "\n",
      "Epoch 316/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3308 Acc: 0.8294\n",
      "\n",
      "Epoch 317/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3261 Acc: 0.8303\n",
      "\n",
      "Epoch 318/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3358 Acc: 0.8303\n",
      "\n",
      "Epoch 319/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.331 Acc: 0.83\n",
      "\n",
      "Epoch 320/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3418 Acc: 0.8303\n",
      "\n",
      "Epoch 321/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3366 Acc: 0.8304\n",
      "\n",
      "Epoch 322/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3325 Acc: 0.8306\n",
      "\n",
      "Epoch 323/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3368 Acc: 0.831\n",
      "\n",
      "Epoch 324/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3408 Acc: 0.8307\n",
      "\n",
      "Epoch 325/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3383 Acc: 0.8307\n",
      "\n",
      "Epoch 326/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3335 Acc: 0.8315\n",
      "\n",
      "Epoch 327/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3386 Acc: 0.8309\n",
      "\n",
      "Epoch 328/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3308 Acc: 0.8291\n",
      "\n",
      "Epoch 329/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3359 Acc: 0.8313\n",
      "\n",
      "Epoch 330/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3336 Acc: 0.8297\n",
      "\n",
      "Epoch 331/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.339 Acc: 0.8303\n",
      "\n",
      "Epoch 332/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3296 Acc: 0.8306\n",
      "\n",
      "Epoch 333/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3364 Acc: 0.83\n",
      "\n",
      "Epoch 334/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3454 Acc: 0.8302\n",
      "\n",
      "Epoch 335/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3361 Acc: 0.8309\n",
      "\n",
      "Epoch 336/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3433 Acc: 0.8303\n",
      "\n",
      "Epoch 337/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3383 Acc: 0.831\n",
      "\n",
      "Epoch 338/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3421 Acc: 0.831\n",
      "\n",
      "Epoch 339/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3386 Acc: 0.8304\n",
      "\n",
      "Epoch 340/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3279 Acc: 0.8297\n",
      "\n",
      "Epoch 341/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.333 Acc: 0.8296\n",
      "\n",
      "Epoch 342/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3369 Acc: 0.8294\n",
      "\n",
      "Epoch 343/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3353 Acc: 0.8305\n",
      "\n",
      "Epoch 344/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.335 Acc: 0.8307\n",
      "\n",
      "Epoch 345/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3386 Acc: 0.8296\n",
      "\n",
      "Epoch 346/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3309 Acc: 0.8315\n",
      "\n",
      "Epoch 347/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3404 Acc: 0.8304\n",
      "\n",
      "Epoch 348/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.3336 Acc: 0.831\n",
      "\n",
      "Epoch 349/349\n",
      "----------\n",
      "train Loss: 0.0 Acc: 1.0\n",
      "val Loss: 1.336 Acc: 0.8311\n",
      "\n",
      "Training complete in 56.0m 30.752270460128784s\n",
      "Best val Acc: 0.8321\n"
     ]
    }
   ],
   "source": [
    "# Fetch model\n",
    "model = resnet20()\n",
    "\n",
    "# Set up criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 60 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "# Send model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Train\n",
    "num_epochs = 350\n",
    "resnet_layers = 20\n",
    "hardware = 'V100'\n",
    "model, results_df = train_model(model, resnet_layers, hardware, dataloaders,\n",
    "                                criterion, optimizer, exp_lr_scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136850, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resnet_layers</th>\n",
       "      <th>hardware</th>\n",
       "      <th>epoch</th>\n",
       "      <th>training_step</th>\n",
       "      <th>training_step_loss</th>\n",
       "      <th>training_step_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.318026</td>\n",
       "      <td>0.029508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.654434</td>\n",
       "      <td>0.031106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.825544</td>\n",
       "      <td>0.023048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.427489</td>\n",
       "      <td>0.021126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.182978</td>\n",
       "      <td>0.019686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.707961</td>\n",
       "      <td>0.022572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.193202</td>\n",
       "      <td>0.022811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.724806</td>\n",
       "      <td>0.022601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3.303929</td>\n",
       "      <td>0.018518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2.910753</td>\n",
       "      <td>0.022301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2.439167</td>\n",
       "      <td>0.018733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2.261326</td>\n",
       "      <td>0.018570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2.358150</td>\n",
       "      <td>0.021749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2.738165</td>\n",
       "      <td>0.022133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2.224427</td>\n",
       "      <td>0.021822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.853202</td>\n",
       "      <td>0.019373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.327466</td>\n",
       "      <td>0.018379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>2.159724</td>\n",
       "      <td>0.021769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2.157506</td>\n",
       "      <td>0.019047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>V100</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2.161107</td>\n",
       "      <td>0.024024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    resnet_layers hardware  epoch  training_step  training_step_loss  \\\n",
       "0              20     V100      0              0            3.318026   \n",
       "1              20     V100      0              1            2.654434   \n",
       "2              20     V100      0              2            2.825544   \n",
       "3              20     V100      0              3            3.427489   \n",
       "4              20     V100      0              4            3.182978   \n",
       "5              20     V100      0              5            3.707961   \n",
       "6              20     V100      0              6            4.193202   \n",
       "7              20     V100      0              7            2.724806   \n",
       "8              20     V100      0              8            3.303929   \n",
       "9              20     V100      0              9            2.910753   \n",
       "10             20     V100      0             10            2.439167   \n",
       "11             20     V100      0             11            2.261326   \n",
       "12             20     V100      0             12            2.358150   \n",
       "13             20     V100      0             13            2.738165   \n",
       "14             20     V100      0             14            2.224427   \n",
       "15             20     V100      0             15            1.853202   \n",
       "16             20     V100      0             16            2.327466   \n",
       "17             20     V100      0             17            2.159724   \n",
       "18             20     V100      0             18            2.157506   \n",
       "19             20     V100      0             19            2.161107   \n",
       "\n",
       "    training_step_time  \n",
       "0             0.029508  \n",
       "1             0.031106  \n",
       "2             0.023048  \n",
       "3             0.021126  \n",
       "4             0.019686  \n",
       "5             0.022572  \n",
       "6             0.022811  \n",
       "7             0.022601  \n",
       "8             0.018518  \n",
       "9             0.022301  \n",
       "10            0.018733  \n",
       "11            0.018570  \n",
       "12            0.021749  \n",
       "13            0.022133  \n",
       "14            0.021822  \n",
       "15            0.019373  \n",
       "16            0.018379  \n",
       "17            0.021769  \n",
       "18            0.019047  \n",
       "19            0.024024  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f'layers_{resnet_layers}_{hardware}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
