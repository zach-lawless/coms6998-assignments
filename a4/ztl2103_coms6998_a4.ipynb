{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMS 6998 - Practical Deep Learning System Performance\n",
    "\n",
    "## Assignment 4\n",
    "* **Name**: Zach Lawless\n",
    "* **UNI**: ztl2103\n",
    "\n",
    "### Problem 1: *Transfer Learning: Shallow Learning vs Fine Tuning, PyTorch* (30 points)\n",
    "\n",
    "#### Q1\n",
    "\n",
    "Using [this PyTorch tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) as reference.\n",
    "\n",
    "* a: (4 points)\n",
    "\n",
    "Using the `vgg-flowers` Visual Domain Decathalon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '~/data/vgg-flowers'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
    "                                              shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACDCAYAAAB2tFtFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAADfWElEQVR4nOz9eext25bfhX3GbNZau/n1p7/ta+q9KlcHbiqxiaGQTQQJEYmBBJtYtmLHRjEyjqIIB8txGTmRcYKREUmglEBBImPTBUOEMAqi5FiOHScxAVyu5vXvduee9tfsvVcz5xz5Y8y1f79z3n333Wqe672nM6/23b+zm7Xn6r5zjO/4jjFEVXk1Xo1X49V4Nb63hvvVnsCr8Wq8Gq/Gq/ErP16B+6vxarwar8b34HgF7q/Gq/FqvBrfg+MVuL8ar8ar8Wp8D45X4P5qvBqvxqvxPThegfur8Wq8Gq/G9+B4Be7f5UNEVEQ2IvK/+tWey6vxavxihoh8TkSuRCSLyO/91Z7P99p4Be7fG+NHVfWPAIjI2yLylfkNETkVkf9rXQC+KiK/4+YXReS3iMjPishWRP4zEXnrxnt/SES+JCIXIvKeiPwLIhJe+v4/JSJfrtv/WyLyufr67xaRn/okkxeRnxCRn/iEcxIR+edE5El9/CkRkY/Y5t9TF74/ceO1f6aCyfzYiUgRkVv1/Z8WkR//hHP+ioi8/UnmVM/Jf1b352dF5Lf+Iub0p0Tk6/UcfFVE/shL2/3KJ5zvj4vIT3+SOdX3f0f9vY2I/PsicvoR2zwVkUci8pdfet2LyJ+o18yliPwNETmu7+3Ptar+vKqugf/HJ9mHV+MXN16B+/f++N8BI3AX+MeB/4OI/CBABZB/D/ijwCnw/wb+/I3v/ofAr1XVQ+CHgB8F/uD8ZrW2fg/w3wbWwD8IPP7lTPYTzOn3Af/dOpcfqb/5+1/aRgT+DPDXbr6uqv9rVV3PD+CfA35aVX9Zc/4Ec/o3gb8BnAF/BPh3ROT2J5zT/wn4/noOfhPwO0Tkt/0y5/uxc6rXx78C/E7sutkC//uP2MY/B/ytj3j9j9e5/kbgsG6n/xWY86vxixivwP17eIjICviHgT+qqleq+peB/wC72QB+G/A3VfXfVtUe+AngR0Xk+wFU9Yuq+nzeHFCAz9ZtO+CPAf9TVf0ZtfFFVX36y5z2x84J+F3AP6+q76jqu8A/D/zul7bxPwP+E+Bnv9mPVMv6dwL/+i9zvh87p+rJ/Frgj6nqTlX/XeC/xM7Lt5yTqv6cqm5ufGx/Dn6p4xPM6R8H/kNV/UuqeoUttL9NRA5ubOM3Ygv+v/bStk+APwT8j1X1q/W6+K/quXw1/jaOV+D+PTZU9Suq+nb95+eArKo/f+Mj/z/gB+vfP1j/PX93A3zxxvuze36BWeQ/ill0AK/Xxw9V2uDLIvLHK+ijqj+lqr/7E875J1T1Jz7hnF54/6X9oVI4/yPgn/0WP/ubMav0373xWz+uqj/9Cef8tqp+5RPM6QeBL6nq5Teb88fNCUBE/rCIXAHvACvgz9Y53DzX32q+P62qP/4J5/TyOfgi5v3NlJvHPMJ/Eni5fskPAwn4R0TkAxH5eRH5Aze2dfNcvxrfxvEK3L+3xxo4f+m1c+DgE76Pqv7ZSgl8DviXgYf1rdfr838Tu6H/XuC3YzTN3845nwPrGxz3v0j1VL7F7/wu4N/5BJ/7JOPj5vQtj/G3mpOq/sn6+V8L/J8/Ynu/3Pm+PKdv9f4fBP6aqv5/PmLbrwNH2PXyKeAfAX5CRP6+X+acX41f5HgF7t/b4wrjPG+OQ+DyE76/H6r6C8Df5Jp73dXnP6Wqz6sV+68A/62/zXM+BK5UVUXkvwMcqOqf52OGiCyAf5RfGUrmY+f0Ee/N779wjL/VnCq98Tew4/7Hf4Xn+/Kcvun7IvIAA/c/wkeP+br4Zyvl818Af45f/nXxavwixytw/94ePw8EEfm+G6/9KAbS1Ocfnd+oHP1nbrz/8gj1fYCfw1z1X+myot9qTi+8z4v781uAX1/pgA+A/wHwh0TkL7z0G78NeAr89LdjznzjMf70Tb76pfd/sXO6eQ5+qeNbzenlc/BpoMWupx8D7gM/U4/xnwF+rB5zD/wX9Wuvys3+ag9VffX4Ln5gN9FnP+b9P4cpI1bA34W51z9Y37td//0PAx2mfvirN777e4E79e9fg930f/rG+/8G8H/D3PXXsQDm7/km8/gK8Ls/wf58qzn9E5hC4zXgQZ3TP1HfOwDu3Xj8eeBfAE5f+o3/BLMsP24eP263xyc6B990TvX9vwr8b+v+/PeA58DtbzUnzPj6/cAJFtD+MeB94A9+k3n8FPBTn3DO33ROGOd+gcUAVsD/Bfhz9b32pWP8T2GqpHs3tv2XMC+uBX4A+BD4LR8zl58Gfu+v9r30vfb4VZ/Aq8cv8wR+a3A/Bf59YAN8DfgdL73/Wyso7+pN9vaN9/41jGPfVHD+3wDdjfcPscXjEvg68L8E5CPm0NTPfP8n3KePm5MAfwqzcp/Wv7/hN+tnfwr4Ey+99hoW8Pumx6x+7ncCf+UTzvdj5wS8Xfdjh3k8v/WTzKmC+39ct3mFWc7/zMfs73+KqVQ+yZy/1Zx+R71eNsBf4KUF8sbnfjfwlz9if/7jOucvAb//W8zlFbh/Gx5SD+6r8V06RKQHBuBfVNU/+qs9n48aIvLfAP6Aqv72X+25fNIhIv9H4N9W1b/4qz2XTzJEpMEULj+iqtOv9nw+yah04V/HFv//iar+1K/ujL63xitwfzVejVfj1fgeHN+2gKqI/P0i8nMi8gUR+cPfrt95NV6NV+PVeDW+cXxbLPcaNf954O/DEi/+OvDbVfVnfsV/7NV4NV6NV+PV+Ibx7bLcfwz4gqp+SVVHLOj2D32bfuvVeDVejVfj1XhpfLvA/TVMPTGPd+prr8ar8Wq8Gq/G34YRvvVHfknjG0qw8lJSg4j8PqyaHk0Mv+6126coipayf5SS0aKIgENQFIqiWhDbhm1WFdUbP3Hzb++REPEx0sRI0zQ470yCpVo/p/vtuXmbdS9EHOLkers390LEHrj936pKmSZySqRxIk0TKU37OQliH0X2B0lQxKRLdgxU688IWst1lVLQrDgFUey52N8i4MQhIvRt5GEoTDm/dCbkxbnPvzEfu/rb8zxV569en8qbhXVt9oqWG9IrsWMoIi9eAC9t+8XDZ5/3zuFDIARP2waWy8BiEYhB6hTmcz1vQV+6oq5384XLYN4DEZt1fQa9no7M+yN2iuuxUVU+fH/Hh4+fXf+c2gGXl6oMaz238+/n+fjfmOM3FCa+vsxe2IFvPM71F176vM57IXJ9aTohdC3tcoGPgQIULft9+8b95sV7h+vrYv/3jff2n7h5rc6SRrWDJC9fOfX6alzgKAoiE07cC9daRsmiZCeot+te1e53yvx7hXLjeoXrcyrzfVjvLOccIo6S1e4drfeyuP0+z6/bubPt52LbFe8IIRJiQ2wiIQScc6hWfNJS72Ob5zRNTNPINCVK0f0RA8Gpw4uzOTmHiP1mKZmihnHzBSZg/97v43zNFzseFXLECavmgMePnj9W1dt8xPh2gfs7wBs3/v068N7ND6jqTwI/CfD6/Tv6p//n/0PyNDHuNgzbDdvLS7aXF0z9jsYJ0TnIiTyOlGkiCAQRHIrmjOYCZX62g69AOD6lufuA9f0HvPbaa7z++musVgtbOHKilISWCSfQtoEmBpyzO1Sc4JuIj41dPKlAthsFFcR7xDcQIvgAPpLHxObDR1x98JAn773PB+++x5MPH0FKSCkEEdoQaHygVtjClYwvEyFPlJzJKVNKQdTh1JGHxHjVk7YDfoImQUyCn5SQoHGBNgSiD/zsZ17jz5wNvL95il347hrQFFBB6wWfc6Ek+72cEjmXeiznBdRVoHZ42xoOwcns8inTMDKNAzklgvc0MdCEUIFbEQqaSz3OGVTr6xCCJ0b7zsHBAaenJ5yenvLZz57woz96xo/88Cm3bnU3Fle1mztnipYKKgXgerEXsRv2G25+h/iA9xGRUEGi2I0tAScewZMKpFLskZQ//Sf+Bv/ST/575JyZ41MuBrz3e4AvpaB1oXLOgSrjMJJzJtXjq3k2JsAJeO9wIna9VpBzdcZOHN4ZGPgKUpoLecpoKdUAsfKQGaUEz+SFRMavO04//xne/qEfYHn7lF4KuzwhIVIEcil7IC55BhYQp3V/MqqZXBLTNJLKRKHg3Hy8EqoZ1UKpn0tpIuVESRk/VYNDpRogClnRlHirO+U3Hzm2/ZeJoSGo4IpSBDZSuGqhP4hwukTbQM6JcrVFtgNlnNjlgVQKWpRclFIM0J3YeXXOzq0QWC0OWC2PSJOSkqLF4SUgOEpShmGi7wemMZFzZio922HLbhgJ3ZKzew948OanuP/GW5zdvkO3XFJyZru5ot9cUsaeLnocym6z4eEHD3n48BHPn1+g6vCxwzct4gJLWbAKS7pFS2g9SmY3bdjsLtjtrhiHHikFr0oo4JJS+pHcD+iUcFJQzYxpx5B6ConYBX7TZ/8efvJf/gtf/WYg/O0C978OfJ+IfAp4F/jHsKSIjxwicOe0YxqFTdixoaDTRB4GyDs8ikMRl3EhIZJxCnY7gnMgAUpKZBJKMSASR+wc7aphdbBguW5pF5Fm0ZiFrYWcJ1KOIIo2DaWJiHc4bzcXzlGcQVnx9ebag5wH5xHvQRxUC9wJBO/x3hOcI4hQFCiKEyEUR3TOvJNcyKWQEAYNOBVC8TRToZmUZlIYlHEDaWeruhazw5wXCEKmMJLImpmYbJ/SYNa883hXYUPFEEFBiy2CpWRKTgbuFYS0FFDBO3De4cXbDUv9bgVZVBEVvPM4L3YeqkXipLoWGJjHIIh4WxicAVzTBNom0nUNh4crTk/XnJ2tOTpaslg0eO+/0dzdWzMvPszSu+kP7Y31Cu6zJTl7atUF0r29Dtj5cU4IeHC2cNhCmG07zt2YykeLEZz3LFZLVJWUEtM4MQ3mwZWiFDCjRBx4u25Uhay69ylUFCf12Zm1WhzXlng17nIF7CxQvBJjYLFcIM4xpomJgtbFtJRiRkO9fku1XpEKwvvjUPb7JoBoMWtUC1ovoP37Ijjn8ao4D76ILRj1YdZ32Vunm7Hw+GpCSPiMGQ3O0XtlSJ4UA2H0+BCheCiJkgtpKoyTR1+wvgslKQnwriCSEFG0JEoScoKm6WjaBTE0CJ5pTAxlJJUdSQeS2n2zSzuGMuLawOHpEXfu3+Xeaw84vXWLbrGkFNjtBrZXO4btiORC6TNlnNhcXnH1bCDvHJ07pG1XtIsVIS5QEaILNE0ktgHnhawjTiZcbvElERGkJHwphKygiaKFkiY0J4IXvDe80QxjzpQh2zH+mPFtAXdVTSLyTwJ/ETt//6qqfrN6JYgIZ6dLhp3gdINOkPrC1CSYRpwWPIpItocrOFVcMYAPeLw4MkoqZtV58QQvNG2gXbasVgu6ZUfsIrFtQKCgpBwgJbO8mgZpGrvhnEMrzWL4UShmHuIraCLOHs4Bjmq+4Z0nhEgTItEHgvNkyRTK3qoRdXbxJ3MIkhOy8zTi8KrEXOjGQjck3E5IO0i9MJntRPYg5r5QVEk5kbOSNBu4T73ddCHgJBjoqaA671P1bkom50SaJvJU6uJhFIOPHofHSwCu3WPqZ1D7XHABkQIUo5ZyoojipIAUvHfE6AjB7y/U4IWubegWLctFy/HxmrOzA87ODjg5XbFctvgwW8c3aIH9olKt9lJuUD7zBeVmm30PBAbwFdQpSAV26mszwIvYNTUv7FJXtJtgp2qez43rHRVwzhl4iuCrBxjbSNM1jMPIMAyM40hJhVQUKbm66HXf6q6KmIHgBLwTnKhZ1l5AHDmX2SlBxSz2rIB4YteyOjjAx2CgTwbvzK2fPVuxa9s5W+bmxWu2yMXVhc4J4jylKLkk7EjPtOG8SF57G/Nr6EyvGrBfP5RBA5cpoKUgKeMVxAuTVyYPDEI7OSRF8w5LMq8lFdKYUQfeeYoqOWVSMvB3zo5bKeaBXumOi3jF4eERR4dKCM5uU0mkMjDmHcPUM6aRlBL9NEFsODo+5cGbb/Lam29yduc2TdcwTCP9bmC73dJvdpRhxANjP9BfXXH5/ILt5Q5JjlWzZLU6puvWiG/JWSluooRCkgyiZCaSy6gHiQ6PR3LB5QKaGKctw3DJ1F9BShA9XeMJrtAEs+JLyaDX1+BHjW+X5Y6q/kfAf/RJPisiHB0esvWeYbdh6wMxBNqmRVLCacFRkJxtZ0lmaajiquUY8GZte0EpOPF4H4ixI7YdoWvxMSI+IM5Xfh2c8wRvYO9CwDu78HTM5GyPknK1ZisFEAKEiPhoN5ynLggRCZ7QLukWI93ygMViTdddMqgwFiUVZZxNYBGKdyQHk4PJG7i01TrWnGACJfMCnyiCzny/uAp0eW935TyS0mAWtVTLTfw1LbMH6FKtnGq5T5l5N7041IPDEcTXGzobthYo2W5c7+1m8+Ipmih5MvpFMioGoj4GmujpukCMnhiEGD2LRctq2bFaLTg5OeDsbM2tW2tOjlcsVwbu1+hXvqnVTqXgzIKX6jW4vbXu5tDIfNyZqZyZJ7+2WgVbvFUcOL3mj6sHoTNwvWy1V6dIpLK3WQkSCCEQQkQdqDfrehonSnppUZqnMIO2U4pX1BWzIapHKG6evVm8iKA521zbSFivaJcL1MGUEolitJYWozmKGT4FA1VjkWyhLCXvj2+9tIwbx1Gqtyj1kN/wj4wmEkHF2bVavdRSjMaUYkYNIiQJDL4Dp3h/zY+rK9UzicTs0eztkGRPyR7NAaGtXqd5B3lSUnKUnPfAnlJmnCa0KDFGci6klBingcViiXOO2Apd8WQVUklonmhXCw5ObnH/tTd58+3PcHrnHqFdsNkNXG127LY70mBsgqQEIuy2l1w8f8bl83NSn/Au0vgIyehj8cYhJBnJOiHFvK5cEkPaMWlPZkTdhJQRpxPkge1wzjReUtIOSbkyBdVAchlpbBl11yfhI8e3Ddx/MUME1utDBGFzfk7wkegb2tgibTYrXQuSEkUnVB2qdiE6ERwB57w9vFkQThzOB2Joie2C2C3wscH5gHiPq0GYUAFzDpKJCGkcGXcTU99bUHQczdIAvAi+adCmg0bRECGIJVA3diN43yHtkq5b0XYr2m5JSgVSpuRMcg7xzox9b5Z39pCckoOgwUhtzRNlFMRninMUrWDubB52MwkVVpgDWmhG82QBqArWKsbh22VSb1wcoh4NCklJvnK4qnZBlequS7XaK6BrLva3mnXnxSzxUsQ+oopzZnUGLyyXRrscHixZrToWi8hy0bJYNCwWLYtly8F6xeHhAYdHKw4OG7rWV8aigrFe0ymzpeswvhZugI1oXbi1xguuqYY9Kul8vOYvvYBW++2JXCPui+CufENAVSoAV9omp4yKok6Nnw+Cx9O4Fh/N8twLBMrsHZpVvLeQHTUmUCAZQDoRipQ6ZUGc2O54R1x1dEdrNDimkhhyYRLwQZAEJdsxEGeUpN5Y2GydcPVY2GsG+GbV55JrANKsezvO1MXQ42ogtIhSxMyR68WXvVhhEs/gWotr1diCLYyJ4pWigZIdmuo9qR5Rj5OI87YoUQ0NcdTYR6mnL9c55RpDcKSU2PU9Lhg2LBYLmi7ioyM0jmYRSLnQHd7h9M5r3L3/OnfuvUbTLdkOE1fbDZdXG6ZhRHMiTwM6jow50W8vGMZLVHti42iCEpuMyo5cCk4aJAQ0JLJPKOZhT2lkmHpSGtBsi4vkEaYRxh1j5dVdmAE8m0dSPD44mhDNO/QfL3b8jgB3EJpmwdQk2nZB2ywY45bRRQOfGuwjFXRSdALJYpQu1X1Wj2apAFTj3yo417BYrDk4OmGxPiS0C3xo7UapwC4zMKYCOZMvd+yen7O9uGTa9Yz9YFYqjuAcY9MwtR0hRggWSJUm4trWrPHdjrLbsrvYkMZk7nK7QH1ARGjahqaJQFXXOBiDI0VhUYTDSVj3hfB0Q3AbShZyP1FkNL7VTMQ956ZFLWA3c5yaQVMF5Apvzqx3qfSBVL43OG/RfHUk8aQpkymV18eCYZr3brZWa4xS+XQM3L0TWzTUpue9mIXeOI4Ol9y+fcztWyccnxxwfLzm6GjFYtHQNJ6mCfURaZvIeuVoGzFa56bVPgO8KOKMY/eqqFZQmq+mPeBXYLSjZAhc9NqM15lYuKZobPuel6kYqRSdbarGHG784EwezRy2eRIJzRgd5xwSHE1oQNob7gTX8Z/yIphmnVVjdh5dVnCCxWArwM+LRIy0B2uWRwcUgTElJjLZO6RSSKp2DVsg1yg85Vrl5Jyvx8Csy5yM4stlqla/BcW10lqm/pD9NYUKzluwU51DJV87I9Vwyi4whQY/77+A9zCVQhK77nJSSlKCN6UJLlA8eLk+9kImxECIddvVM5mmiRgbRCxgbyDomUNOOKPMYhfpVh1HAiG0HJ6+ydHZAw6OTuiWa4aU2fUDu23PNI0UMpBJeWDoL8m7DWnYojLSLaGNJg5wrlC0B5eQkAhNizaO4jy5ZEhGB1cS06hfHJrFvLmUScUYBd8ILickJ7IWKILg8RJxIYD4j0XV7xBwBx8bYmxp2gVdt6QPG5wENAtpyORhhClBzkgueAWv1XqtHGLJQk5CSVVaKIpoZLk64ujkNqv1mrZb2GpfwdDufTFg7xPaD4yPL7h6+Jjzx08ZNluG7Q7Jheg80QeTVMaGEIItLM4hMeKbCN6R00RKI9t+x3azpRQhtgva9QGha+iWS9pFt3flixdSG8itZ1Ech6Oy2iS0e0rRJwxjYdwMFHZ7pYPhnVmGOlvTZQbBjGiqoG8Xk6C2DooFmk2tEVAPXh1BPZMkJhJTjVs4QyvD8RnY97x7vcExeis4odQ7yAEhCE3j6DrP0eGSO7dPeO21O9y9e8adu6fcvn1C10WcV7w3C1UreLRNoWlKBfeZiqlWp86WJczyxZet7vomsgf8slf/XHs587sC6uo2K6ib01u/e80z7613MOtxH1Ss25m/oWqHKWeKKkU9PnicDzhvXpuYfhUfglEa1WIvakqdPcDnKr0LGdeoGRf9QBonO/++ehZtw/L4kIOTYyQKpVrizhsAaw3WOufwIqRSwOX9QiQv7FcNtM+PeQ65GEBp/Z6UystfB6z9LEIQux5KsX2Sasi4GPCLFkEZxoEhJSKOrJPFB5LQTB5NHidmujpv3mUoZoRJKXjv8b6qiqTSTnPsoOR6Pm1ePgRiG4mxoagypkTAs1wtOTg85Pj4jIOTt2kWpyCOaSxstj3bTU/Oxb4vjjRmcjHaZNg8x+tEG4WuaegaJQQL6tpiDrEJhKYQfUuQxoQTKRGbjiZNdo7TRBlHJtkyJccoxYy4UtCckOSQbLEvhyNNgaF4mISUPx5Tv2PAvRRbwbyLNLEj+BaHgXuZlNRnSMU4drUd9eII6nDq7QbNQPKQasTeOUQDwbfE2OFcpBRHygpThpT3WnHGjG626NWG/uE5m3efcPnhY8arLeN2hyvQhmg0TLRH9r5y35gsMnrUOSYyk2YGzYyawXmaxZLuYM3iYE13uKZdr8AJRSq4d5HSBVoVDkZYXA5M2jBuC+mih3BJ1gqee9IVuxlnZUJV0lAKJZtipUDV0GPqGkzD61CznnAQQIpp510RXDYFjjBb4zct5zn2YDJHiwEU+1wN/KmDJjq61rNaNBwcdJycrLh164i79065f/8Wd++e0XYWiBXJpDQxTQPTOOD9dAPcZx4Y9pTBrPfd0ykzfcI3Ps+Yq2I7qfVFnYG6HgOx/TJqwvQs83aupZVU/rpuuoL9TOvNv1qKkqYEzo61V8WjBKHGPub4iOJSVVu5qs32Dh88Xpr9MoMqslDCqHhxbLc7hr5nSgZiWhLNesHByREHx0fsJEN0tuA6hwvO1D6lxv5nL8QJVIljyXMcwfTeWs+xg2pdzodjDqYXigg6A7uYJ+zrcbs+Lfb5UoPQWTNjHiglk/KIo5jlKomiGSmC5hHJAfGKaKa4jPdGb2kpuGKLYwyB4M0bLTUgGZzfLzbUcyHe4V2gqCOPBSXjQ6DphBA7Vutj1qtjxC/Z9gbqm82WcZgQcTTB41ym5J6sE1PaMaYtjc+oOLIk+uJwk1HBPrbEDlwXcU2iDY7OLShqCqVcY2+lFNKUGPueXbxgS8AlKH1iGgvjmNEEHotButiQnWPIhaFPhmMfM75jwH232TH2EzmBI+IlIhqQ4qEGWLw6ojgabytpxON1pmJAkxhdkyvoqH13Ggr9diQnGN2IpIIOEzomfFZ8ARkT+WpLvtxw+fAp2/eeMTw6J/cD9CNeHLGBNjrTwzfmTpoGRsmSSE5IogwURjI5OErj8V3DYnHI4ckZy7Nj2tNjmqNDCM6YAiek1pOaQFRoJwiXPWVTcI830D2nhEhCcGrcpQkaCkW04tK1gqUkpUwWSKWAurL3UkypU296zcYTq+UMiPe4AL4Rsit7qtoClQakpsKrljSlKhq0yhAzzllwq2uF1SJysG45WLesVw2rVWS58LSNEEOplk6p9Mms7w44KQQ/IWKu8DV4z6h6E8A/Bthv/H1tXV/TMXOIwgDXg3i0xiW4kWRznSRjYDEvGHve/SM8hxmUSzZteNFCVrMCxbt9MHTmkKnWPsW44hvrT7WIQYJHxLFYLYmL1nT0WsgoB7fPOLp1yvJwTU4DY+W+jYm6Tp6bPbsYPaMkUqmigZyprIUFcGfevc5vpt+0znkGa1ek8ueCijLNi4g4mhDICplEmaqkMQ3shktSmhApFg/A7a8DD7g0IJPDea3B8USWYsHdmvgTfWS5bGliwzSODGWsi725w1rPTUoZTQWZQMbClCZSLsQYado1KTmgwUtDTrC93HF5dcWu35FzJsZAbD2qiR41mipPqBRUilFb04CO2Rg/H2i6BZ3P5AYaVRayIPqMSECcPXABBdJUGHY92+BpnWMrgqew0cw2jUzThDohtg3N8gBxgdQPbPtLvgW2f2eAuyoMQyZNmZJhFjg6jTiNeI0ETQQRuhDpQiTiieKRzD7TyxFwmErDicO5gCuRNBS2Vz3BTZZYMWXKbkD7iZALMSsyJPLllnS5ZfPhU/oPnjM9uUSmhE+F6CNNW+hapc1KYwY5SQtJC/N/E4WRwiAFbYNxZF0gxgXd8oDl0Snt6Rnx7AQXA+JNMTNGxxQ9HohJ8asd8uElrB9R2o7sAwkhqOyDoteW0d6YNjzOhZxMl63Fgp6mOzZglyI1S9chEvYLhvdCjoJHKd4yYks2d1y1WnnMFIohv0PNsi9YHoIreK+0jWO5DKz34B5ZrSJd52gaCKEQXAYxgHfOPIkQvO2fUPnyxAsgvjcJ4RuB/VuBvIF7PWxVASRUudMe1BWP6LW0b8bw67zDGxmqL+vw60vOXVu7mgv5RqBU1OO8R5zbKx60Arvt+A3efn4/C1KU4qpM0rvKv1tgMTTR5JfBESRW6W0NnN7UQxejpfxNsDfzhDkx0jzDWU1VrXidOf56FGe2jELJBvBFxeR8NQYhzoHze+4dIE89qb+0gKEXSoI+Z5roaYOncQ6mLToWXANNE0lamPLIlDKbTc80TazXa45PHnCwOuDJ4ycM/UDJptTZbsyrKars+gEA5yMqsKkez3K1ZrE6oZSIkxaHY7frOX/6jIvNFUkV3zZ0sbWFcMpM/WDJacmMx1Ig5ZqDoMKUMlPekTc94WpHt+xZrg447CZW4QIRj28WNO2S0CxAPAK0jRKPWlbtIcPasTt0PFsUnvieS9dT8gRty+J0Qdut8duBizRWN+ybj+8IcBeBdnGAENixI40K2RF9y6pdkzVS/IKI0PlI5yNeBa8OkqIuU7wl3ohKTfU1KSSLI0qfuXj03NzGouiYKriPhKkQM7hholz1lKsdu6cXpOc75GoiqhDVsySyKg3L0uAnhy9ln2hiuURCEUcWE88kUaZJyX0i+5HL8w3SXZBiwzJEliESV0ti1yLBk52nD56AEAS0SeQQGETogUFh0jkI7PFzPEoxGZxaZqTDAF2T3YnFGWAUFQq53mjXD8TyT8V0b/hgFmPJSp6SfapasFqU6/+q5V4TZFDsWDglRlh0nvWq5ehwwdHhgoODjvWqYbkItA14X3AepAKLBehgny9QLUydwf0brHVM4jkHNuX6dUOdj7LyrymV69cwMMejWgGeKoW88Rkw+uXm119UzNzg/mtcY06h3y8mxbTZUtRiJDPn7oze2PMbcnNb9pSSQLFrex5FtRoWxqHnYh6CC/Wc1riJ1oD0nOI1Cwj2i5Ca0sn006UGi/U6h6Aez3lxk/3iVimrCvBil+IN6ov9782703pYN1Vs5cwQyyXVuE8gqOATyJhhElzs8K7gfEYyIAnnMk3j0DKx3Vzy/OkTnj97zjRmnPNMUyYnJRcYh8nsGhJjygzTRGhalosjTo7vcnx0l6Y9YLvZ8eTRMx5/+JDtMBDallUIROfxOMpU6DcD/VXPuBthTv4KDgnVYCiZaUoMKVP6zLYf2O48qblk9BMqntguabo1oVmCs6zaECLRe2LI+GWhcZE2HnJwULg877i8uLBFJPRojEib8F1B/HcFLSMsDk+AiLhL0gQUTxuWhIVASEibaMTRuUAjHpeBpOCL6YGDEn0g+oborRaED5HNIvJ0l7l8+NR45xnctwO6GwipEJPi+gzbHjYD08WWfDEgu0zjIgvfsKJhTctCG3SCMhZjZr2gDoIXNDjUebJmkggpWaxg0p7sL+hV2BblqK70y5OMOxIkBIYAGxVa52gdBHFMwE6VXVEGVSaFiDMLwFmWoapZdLkmDM2Zub6WHXAizNrtIL4+quVfCogFx2ZahL2LDVMBckaZbCGoCe8iGZw9+9mSdJYo0raOxSJwfLTg1tma23eOuX3nmLOzI46PD1itO5rW47zx6VknVA3Ar3FtAJmAhFRaZg8ys++gN/7mZqD1GtRvhD8/kjrZuzo4wFer3e0Bnhe+fyP39ca29kHWm1SNWOByP7sKjHPNEK0ZnzjjXmQ+5vXZxDwvLiRkEympZR6Zh4Z5jkWFKU+mkMnJPrP3cuo2vcVTZh/FqJ5s8lrcXl6ca8KRatnXLZr3eZZ67v23cr0vyqzIuXGc6gJru2nXY+vhIKh5Mh4SajSJUxpRGhxBlZBBph7JzpLeAoSixOhoY8tq0bHbXrG53PL+ex9wdbEl50LwkSa2gCNnoFhQd0yZYbQyDEeHZ7z++qd47bW3OTm5i0jDk8ePeO+dd/nw0YcUhdXhEcvVwrJzU2bY7thdbNhdbhm3I9EpeGeBzprMWFTxmpCc0eQYc4AEG38B8pSsggstLiwR31Ek4H1L1y1YdB0xCE4Trkw0TaI97Vgtoe0yF5dbVHZMRcgoEvM3uaavx3cGuIsQujVNdiyOdqz7TNuuKKsdZbeDXQ+7AZkSLikuFcqUyGNGpwK5erJqnG0g4ktAkkP7QtaBqR9eBPfdgO5GylQoqeD6BLsRtgNlM6B9xo8QoqNxgUYDPjtcrhme1XUlU0UWBqRBrLRA8Z4ihZSLBUc2AxMXFHF4Hwm+wbtI2yxxHeQGpuouW6DKkxFGZQ/sqf6c/fS1PeSZiVJX1TCyVxGY6++IEojO6s94Z+4gtSYKRvHuk7octWBZLhboYjIpmBoHLnsqxYKe4hTvoImRRedYrxpOjpfcvnXI/Xun3L17wq1bx5ycHrJcNSZzdAXVRMkDqQyAWe5WmiAhMuIkcc25G+80B/pm3r9Uq3KfYfoyfSNzwHO+1uzPmVay12ZLvdZxqSnus11fv1YvVfmGm+rl14w2m799g8qq72q1bPfXUGF/HmZE1RecDLve9vVwVEGN+85qlFmaJsbJONriI1mu8wHEzR7tTHjW68aJRU3E6KlShJIt0aaUYuu8isWwi76w27L/v9QkunkRu17UZH8qbAFzIoRpohl2FFcQ7y2/Q5TohYUXOnH4YsHnqNnA0gda59ll6NpIDA3LRcflxRUPP3jEow+f0+8SFGjjhKwssJmSoliGLQWCbzg6PuOtNz/N9332+7l/7w1iaHn+7Dlf/9pXefedr3N+dUXbLWi7SElr0rglpcTV8+dcPT+nv9qgU6ZZNLRi1HDEYjVOr6/PZAQroQQavURKJo0TUxoY8xVDEoZJgUDXdSyXS9rGkpQciegz0QsxOFxJrNqIOisER8n4G0Hjbza+M8AdIC4I68iBRNqDM8pmQ9ls0ctL0rPnpGfnjOeX9OOWcTswbnvG7UAZc7VIHLnJaAMpWACxKOxaR1oGXGf2ilNgykifKH2u7l9GhwS7Ce0TjBmXwBfTf3s8UhyalTzNgFAttqymQEQNJL2jceaqIYVcJopmpsH4uB7Prulom46uW5LXCVmrqYLUbjYVB+KN5kFI9ZEVstaiSVKM66ayxnNAVKqV7uZELbPyg6vg7izhS8sNoGCWs82a9ZpYIoVERnRCNAMJ1EDdO6t54pxJIoOHtoksF4HDg9bA/fYR9+6dcufOCWe3zHJvWkEk4ZwVnMplIKUdUPBVzYNLOJlAknkW1wVxmGWNegPsa1YDsi8EBntg31uv9TqbLWyu650wW+o1KczK0bn65essVW5uQl5A8xcoGgWkSLV2ZR/cgwriNUBrrryBsFSqveJlDWje/ImXqobqtS+FCKVY8beUEntRuKvLyQywvMhSzfSKc/XhBVfMm0AMtFGrFZTndPd5ganS2Ln42PxaqfTXNQFUk8pqIbQmj6xTRhpnxdkoJJRWHKsQWYWGUHMsFFfzLGpNJudYtC1t0+KdY3u14fzZBeNuIo/mQeKFknJNfrKF1OHM8Fgd8Pqbb/KZz3yW1x68TtM0PH9+zle++lW++rUv8+zpIwrQdAGRTMkDY79hGAb6y3Om7SUyDYSSiNkRRWlQo3qLCRn8qMRckx2B6B0LXeCnA4btlmGT2Owy2yGz6zO5QIgDi8WOJgrRZxYtBJ8JrrDoArFpCE1H0y1QacAXpugI7uPh+zsE3AVihw/KslmyOCiw2aAXl6TQsesLu/MdYxKm3cTmfMuw2TFsduhkiRlRAtqC6zwlzHUnCmPjyanBTVYRziMmgRwzbsjIlPFjRsZCGYsR2xlL7HGCd9XSFUdRSLnsk4BEuOYpq6acDC44GvEUUVox7i+PiTJMTDjG9oohdkyLA/Jqi18u0daDRmagmfX7Zf8wiz7vE17EsnZrQonM+nUneAkEiexTtNSUQx4rE+BwVtEvW8nT4pQsuWr2PThL3x7HgXEYmMYJ1YypYQqxatjbRojR0qLbznN4uOL4eM3pyQH37t/i7r1Tbt0+4uh4xWrV0DSC99ViLyOqI9bL2SR9phKRG+CsFfGqFFPmyLHB2p77V0VrkbL5cU3JfCOw26jbnT+zp2Lc9b91nkf9erXQVV4G+5e4eIyeQO077noD+23M4J5qhcu98/UxChykpv+XWm5B5hQsX+keOyLX5RbmoCl7RZVVU7SFIkuiuPr7GJUUgmWB5lLLbmjeVyrdn4e56mo2C19r8hWqZJGqVJU9NWglnb1tfyoshoJ3wqiFwdv1tJDAgW85jJ3p3kuhL5lhKiSXycHjXUMbrQhYGiY2lzt2m2FfCjs6R/SePNkC5300ViA0LFYH3Lp7n7fffpvXH7zGoltwfnnBO+++w5e//CUeffg+47hhsVgRgiVjaR5JI6R+Rx4u0fGKkLeEkpChx2dPDIHozTuQqVggODvwDd5NNHlBo57Sr8kXynixo9/COHjSaAlL2XtoleQT0Se0K8SQCT4jnSc32WSbq4amNXpY2kj87gB3wMWaIJORrJQ+US53jE8vuPjwCc/eecjloydsnl+we35JGkbyMOGK0PqIBEVdg4bK3WaQrAbe2wlNZZ/AIVnRKaNJccmsfhEoPlgqvgoihaAQm5bQNIgPqAjZcp6vs96yUdeZqtpJajRNVdMFhaYGrErB6J/LnuwvmcIzJmkQFfBKPGgr7yZW7lcsbhBCwIWABA+TWUe5WMlVr9kkY874f5EK5Grgfk0Z1zRuNSVyqXp4zVZR0EgKIYvRMyUru93IbjMxTQnvCsEXfGPVHNerwGodWS1blquW9arj5OSQ09NDTs+OOLt1zOmtY07PjirPLoibKrAPaBnRMiEYby+uVhcUv1+sdE/HlP3fehPU5/fl+r3Z4r3m6AFedmFnkiTX52ouq7v+G1eDhb4eP9k/641/182/SP3APhN23tpcjvimNl5VkVLpiwrcbv8F2G9Q1QS3VaevczCvJhDNGvm5jHDOmVQNghng9wtTBWW0kF0mk/fAPGdzOpF6naWaTHWd5zAn0e3r6+x59dmTNWWQm2v7VItdqBmtG8WdJ9yUiR1IZ6WYlxJZS8uBW9CIYySTpp6rNLIbJ1ILcXHEol0jCOdXV2wvJ6begsYCROdpXMOuH8midAtPbCKr1YqzO3d44603ee3BA1bLjouLp3zt6+/wpS99ifffe4e+v7TaLU0gNg7vihkfpUDZImWD5AuYLtBkjEHoAsvQsowtDki+MLjMOIFWPt27ltQvuLoM7J7C9tyAXaShUU8Uu99jViITQQdksyV2xWTDWZguBjbPBtJiw9HxGcv1CV3siDeC6x81vjPAXQCJgCUW6ZDIm570/JLdo2c8f/8RH7zzPpePntJfXDFeba3WRlai80gs+CgUnyAaByzZHl6VoglSxtdaFqJYdCqrZbpisrTilRKAWu7VI4TY4poGca66wgLBIcFbApIoOSsZ0zGXrJZVVouee6BB0GIZtAwFuezJxZOIjNksaw4awp1DQtPiTPxunoYL+GAPFwJk+51UCr6WGTBKxV1z1hoINPW+q9mVxdmCgds3+iArJSWkgodUTwCUnAq7bWa7tXKrTVTLmsfRRsd6FTk5XnB0vOb4eM3x8QG3bh1z69Yxp2fHHBwdsD5csjpYEqPHRCETRUdKGSh5sECqVG7XuWrdBcTN/MQM2tdgXpc2e12uXzNrvS4C++qPN1nrG0yKws0kcDtMZjrPVrjUCprQsWeY5cVnrt+xr9/cvM60xM0SFzd0I1ojBHWN2tP2sxNxvaVvZO9rMHa/aDhfMzYD5pgmUqWWRPzNGTB7MnNly6xmue+bvtRAsAEye+B3zq7h2SuROS5Q69cDUAopWSlalWIB4nnfKu3HTtHnwKgsThv8qkVCJEpDWzxxcjQ+osnh0kQaM1sm0uRZR4fThlIK4y6RRtPUl73TJlCENCqljIh4Vqs1t89u8eabb/D6W2+wPjqg77d87etf5xe+8AW++s47XF5dsFhGuuWCpmsRJ6QykRIE59F0RZnOSf05abehcZmTQ+XukXL7JHJ26OlaM6ascJlV6dTq2T9+4vjazuO1UMaBMhScF4Kf510IrtC6xDImjtaRk6OO48PAsvP0Q892uyPlnoU7ZyHQNgc0/uNTVL8zwF0xbm3MlM2OfHHJ7slzth8+4fzhI54+fMLzR0/ZPj+n7EZLPlLFFeMFi05MGUYXGH1T0+bNoAtgnFiWSkPOri2AccxGsyipJpIUyZR6YZZgSa9F5o4u5gL6ylVnp5SZq52LVQlQLLsvilRANQ27FvBDIWhP0Qv6sRjdc9xRztbopOAb2CVkzPhiWYneOyvBm9117e36bJwmNcgieI14LMMxl1wlbVKLf5tFrFkpKVNSqlxqrhy+0S85q9X4qDpe7x1dK6zXgZOTBXfuHHDnzgEnJwe1XswBZ2dHnJ4ec3x6yGLZ0Sxami7URJQqa9Tp+sF1nZhZKbO/HmQOmt4MRl4HJY0TliqHnLmHm0bvDXKZl/+cPYKZtZ4t92tLXysYg1pnoGto5Jr2mf9dXvCSjFYL+wxOKx9dF53agAPUYg61xIIr1/ulexL+GrMs4CuQrPSGy2JxIBxTchweHLFYrY3qKBb/8UUsRwNHqBrFOTxeRPFarPuRQPJmIER1xAJNdnRJGEpg1GSWtCZEC0ELPnlThTilBCEFqysfspiBoLxYeE7sUIcEi1FousDSL4lxQQne4la7wsRACHW/R2XqC71mxjTh40Qbk3kFRBbtgqEd6LdD9b+c1URSU4PFEDg5PubBg3u89tp9To4OGPLIow/f50tf+nm++pUv8fjZM8Q7Dk9WtMsVPgSyFqY0EHyi8Z6Sr9B8idcdy2bizoHjMw86Xr8duX/aced0wcGyJdbSyinn+lCmJHz1/Q7VwMWFcnE5MQzJPJkKv95l1p1yfAB3ziKffvuEu7cajtbCeulI05arq8jVdiSljPMbmhb6Pn0srH5ngDvAVNB+Il1sGB8/4+rhI55/8Iin73/I00ePOX/yjLTdEQtE5qChBUg1JaYpM4izgGFRc/Fdlf7VwlgiDjfXMwcQavVIb3VXgKm+XjC5VnK1+JRAJltDhZKR5Gu6uDeta3VDZ6WJWcvW8CKIIwq01frXSSFNlPGK/mqwNPKTJZyuKUNB2yUyFaSfkGRuZ3CeEAM+J1w2wZ6IGP0ktTqjq2WMJeKltUBYhlw7SE2u4MXKJ6dpIo0TJU/UKiDVirZAqaA0je750+OjwPFJw62zJa89OOW1125x//4p63XHat2xWnWs1wvW60DbKN5PoIWSxlpy2BQ2Rp0US8KhBj+rck9LqgWaJrPeK2/MXntSPzzXjq/W+kzLXL9+o46MwMzD35RPXgOxvAD6egPk91hf66hrtbLm0rfzd6Rub/4pFTGKT6F4B8FRxGqrO+8Iwco0TzUw7qpcxs+/ubeKpfZXEVQ8Wixpr3ORoArJSsqqF9arA/yi46pkSvS1nK7iUyZSTMGlStaEksnO8jOic2RvhetEHCF5YhYWJSAaGMhcMjLqSBaPJ+O00OSEnyB7JUUhNSaRbCbLNiVlNFmwNWuyYmVtYeGgaT0HB0ua1RqNHTtRNrueftszjsrUdmgIjNQ0fC3s0kQqV6AL1us1B4tj9BRcDjyZnqBFrfSACsvOuOnTW7d48803eeON1zk9PSbniYcPP+BLX/4CX/nyF3j86EOmnFgdH9J2HT42FGCcJrxXmpApxVHKFic9hwdwugh85sGCX/P2IQ9OI7ePIrcOG9YLTxNknzeQiyPlxDDCya0F2rRcbITzS2U3ZnLpLR7gYbmE+/ccb7+x4Ps+fcLf8UNvcOfU08Ydq66ANgx9y2a742ozsOsLRQd+9hfKx0Lqdwy465hI257N0+dcvf+Q8w8+5PmHjzl/+pztpZXcZEoG0FjbN1etg1omD02JMo0k542n9hZcisW6IYmWa5XCzLaKibes2YJVM1QsOFlUre6Lv2Zoi5u/l/dZldYSDSrzWlUGNTRXqs63WoI6p6SXDEMGGUkC5b3HlMMF/mqgXawQHOnZBWXbo9NUq/GplZEVtVpXGE0yd0Ca1S+eSKAlqRVay6Na6VmXSbXhSRqtQYfmhLiCF0VcMT20sxoxVA44BsfJScOduwvu3zvk7bdv86lPvcZrr92hawNN62pg1SrjhVgQGaFAmtg3UnA1YCiVI96joRiPnOciVTJZaQTHjSw8s82s4ca8MFzTMbz0kL1FXoG/grHWczPLCfcI/jLAU+Wu1bMzJqfqcuTm52dK5gbAV4ps7nmJq3RHMU8FtYCmn630mXfffx9bUGYlU12YtCSkJGLb4nJhmiaSCu1qYXXimdDo8Au7rXO25ilSYJpM9pod5GA0IvNvCcx1gcQDAUpUyEqmqpo04UNNHNRMdtlyTKSQVfZUUalemDrsOq0NVQRIJeFViBI4WqxwsWMgsJsmhquey2cbylbpuwG/aNgFx1gykxR6GbnangMNbbPg9OiUZbtC1DNsxxpHsN/rupZbt27xxltv8tnPfpqzu7dRBx88fJ8vfekX+OIXvsij9z+kH3e0q5b1wQoXrPRvTha3KWVWEmW8z6yWcLpY8Pk3jvg1bx/x5u3ASZc4aEZWsadxE75maJvXaVUquwgP4h2e5TVf+nrmK+9k4jmQlBCU9Qru3vH88A8c8Hf+6H1+zefvcLJONP45Xi+IbkdOE03MnBwr4iK5BIah8PV3b3ilHzG+M8BdQfuR4WrD00ePefTOu1x9+CH902f0mw2aMq0PEDKhWFU45iJZNSPVO090rqbDJ6MddK78olan5iaxOfvv6mqwqt6x3gI/PpggDufMggJQz5zZN7vuxRnlUPRaamZy5dqKT22hqF653bCz6KNyv2kzsPngKVuvpEfPcas1yXn6R48Yz8+ZtlvG3kqPzuU/VWpBpGBucNGCJqvm531DcB3KiJQJTY4sheyU7MxyTinXnpwJqx5qJVatJprDeSiNBVbbNnDrVsf9e2tef+OIt966xac+dZfXX79HCIJ1yLrBYVcet5D3PULFVdbXW79S58ILVnUG5t6dc8kBEVcpl5myuba8zaj/ZsA+d6mZE6AE9Lo5hqXTzxa7nZub7Pb1QsB+Idh3HzIZClplhgaMlaevl7OIN+DO1RtyzmqdM8c/dB//mTNXpZg3tpdfZqs5hLPXvReKN9VVyj1MxaiKruP41jHrkzX+sENbR4qOlCdb0AOUpHauRdHgKF5MNZMsWDqnqIl4MlYDXkNB48TAxOQmRBM+ODQlkhuRkBBvRbCmPDENoN5iFs5V2aanljBOoFZ3prQNsl7hFgcgkXFIbPqJZxcDTzYDwwitz8RYyD5ySWGnypgKebpiexnYHR/gV2/QHt9m8AvOx0zenePY0saJs1PH66873ngrce/eI9arS9I0so3vsZy+Srh8iLvc0gXHMh6wjGc0OGTqrZhX8hTvkEUL2foeLw8b7p8c8X2fPuBTr7fcWo0c+JGOnqgjUlL18A1DhMkMHCl08Tknx46jw5HgC5rNw+8C3Dtt+OHPrfkNP3TIj3yu4417CR2fouUSkQ2pXFJKIno4OLACZEwdo3jab9Gt4zsD3FHKMDJud5w/e87D9z+gf/KUcnWF7nooxZou54JLkxUGm+uK42pwwigQIUNJaJULlpe69Mwur3EotfEBsneBZ/7Xi6PU+7/Um3dWP9QN2syrNNHAW/c18QzYrVCnV6tm6WaArwyCzvXXdxP6+Jwh9ejTC9rVGprIeHFBurgi9z15GklpwpW07xRPpWKkVvWjWKed2HR00ayaNGVGl0CzFVIrRmtI3TnvHG3jWXSBxTKwWkaWy0jTuCrxzLSt5/79Qx48OOLBayfcvnPA4WFD22KWSq11rbOaZZZoVkA0a30+ttV6nS3HOva9OKuXJE6RKvGbFTAVVrkOls7bmWmluYLkdTbty9a87qmcWTt/bf3M4A/Xm1cqQJW8B3ZzwmTPi8v+b7vZrKJQresic9q/w1dw91Wy53AkenLW65otM/1fKb6Zii/ZKCslM447JGtVsQR862nWLX7VMsbCoJmM4oI10vBZUWfBUwmmqqJYXZRU1TCazMWc6gScV0qTSM7quDud27tNSFNwbSQ6D8OIDNb/k3qktdKDcxCZgvXmFSUdLpn0mG23oqTC+bbn8dWOh1cTj7Ny1YB0SmgLrrEErZxtoWx0oAzPudoesFFldXBCDB0H0458mViHnrunwusPMvfvPeXk7IJF+zOsIiyAe4y0jzeUr+2YgtKHliYcs5LbdPqMUjYMQ2FKniQN2lrrziZ4Dg873njDc+euslqf07VbGhmJqnitlpAE87h8ojBYddA0UdIzXBmIjDBC2pmEuBPltZMlv+Hzd/nRN4X7zXPaZ+/iy4QLQgmFrZpHFRromkzYbOFc8bsjXPpuUMtg1dus/OVAv+sZhwGZJqRmyvngcSVcXzSkvXvrKFCs+1BJNT23ys4yhews8FkL2JolJzeeZ2vsJvDPmYJQ3aybnvgNidv1FvcqaVPgYGVeK+A7vX6IvpiQQlG6IbG6ckQdaIsnNIm8HfDjhM+1G1Wlkcusda8lRO3Gu9Z5N7GjawqoZ/KZ6CZKmWpg13xmqcqN4DyLtuXwoOPoqOP4eMnx8ZLFMqDFvte2jnv3j7h//5i79444OTlgsfSIH6lJ8LPtbcdiT13tff79+RBuAvv1EZwTXYSA4vfH/LqMAC+A+p5v30siXwTym/+Wl9/bR9T37tSNBVv3mD/Pzsri6iyAuaZm9jST2yOz7GmajGiu9IRZxVLrn8fgaIOVO97pHPSeLYu6HZl9DrsWdcrgCl0b8dnS8DPWwWtKIz442q6hd5MBrfNEcXPVHCQ4XKH2AYbGCVOw3A/JiZwnS0iSwuAK6pQcMuqsFo6zJA98o8QuEF20cvBFCJMjZlvArsjXiVrBRASCUqZsdeW9Z/Keq2lie7Xh6eUFT3YbrqaJwUEJgoRADsFyPUpBi925XbBerturKx598D7bPuNQDmXg6Ljh/sExb97OvH7Hc7Aa8dMWl3s6V1iHwGLRER505M8kxsuJx5PgTyPr2wvGsuH8EuP9JyH4ln7ZEqQQFgkn0LSRg8OWw6NEU4r1bkZAoykvsNLN+GSeKY6YRsqzlt15IO0Cy+g5PTRP93AFp2eOu/ciy9UWkUtUt+bleqOVfb1VglQKelLYBWRzgOTvkmYdKVnRnXGcGMeRNBmoBS14EaL3+GhBIMe1xUw2CZ+WiZJNEILmmu7sSALZC7nUm5AaHGN/d+4Bfq7NPoP8PhXmBrU1g5Ylp7C3uJxcVySptKWpalC8GqjV9KT9d+YhRVlOBXaZIBOdjMSpUPqRNCZCrvvtXZV+qcku1ZKabgKVCkTf0jYmXRvDyOAHMtWaUoBsigI1XfOiazk8WHJ2esDtO0fcvn3IwUELDJTS0zRw994x9+6dcOv2IV0XaDuHk9quW2uZgBtqIb021fcWreyP4MyDV0Cr4O691cOf2SvVWVNu35vL0O5BXm8Aufgb/34Z5PPsKpnrrHMQ9nqR5Rse85NYchfXqns7gW4OJtS6DVVu6GpcQGvFTBGqvMWamnhPG83qzVVvPpdjnDn6mbO2qdTFr14z3jl0MCtenEKGabvDF6UNkRZTv+DB54KfMjEp0bUgyqiZMSWy83gXrS5TseJaU87sNLGTidFPJGfB3oiakEGh8QHvHblktsNA7if8VGizBVR9nKMdNbjv5/67QvCePiUudzt2m56riyuebzacp4mNFHIr1sc4NDgXrYiDJkoxIyl4AE/aTTz/8H12F+esG+FuN/Gp08JnjgJvLOC2FtptNWik4CXTBeHgMNIeBfQtYXdxyde3nnwI3T1494nj2TMhTQUtVjc+TRM53cjCLULjl3SNw00TolO97hrL05lVGW4CFxEJCCNjf8DuWYMMa+6eRoIUso4cHibefrvjjU+tOFoPtD4Q3BKfK9XnCk2O1mzeF6R4tA+Uy5byrKWM3wWWu6ry7NlTLs7P2e221tYqJ1w2GZ/Dgk/ROYL3ppDR2e2V/b04W3IWpDLKJRdhrCnVUmod7TIDu/0tuH0RJ8Wi3Tpz8+gNIL7WFjvcvvaG05qwXnl2C/bqnMuEYIoXt//+NX1j2CYsMvix4CQTZcKljJ8mfCp4Vcu+CwHRZPXoa/u2ms9SC4TNnGfAO/BiTXu9RJBSF0ajsmbls3dC27as1ytOTo65ffuM+w9OOTrqKGVLThtCSJyerjg87FgsAjFaoGkGdtWawVoXRsutcKBzzZYbFjw3gqL7JIwbR1iqK18LeVVlNS9q1+t5pph1jKvAfbNUwdyNx7O34ufOTvXZ4gQ3F4sXgV4AFStwpe5aZWUg7vbBYZm7cc2Lfm1A4sOceGAJRYLQNi2tb5AMY8mW2axaFzK9Ph5VZz/PbqY3ypSQyfIQCoU8FXZPL5meb5DNRBMFcYHgHDqOyC6zkMDZ+hDvHFf9lovdFZMqW/GUIvhiRsOQMppHdkyMbmAKmSCWXi+T4rK1TtQkTGNi2I6s3YKT5YJmV9hutvQ+k8QsfZMMY4SUE6LzXO4G3nt6jhsLw25gM05sgSFC6TzeN/jQEVy0468OLa426LCqjykru/NnTBePaZfK4Z2GTy0P+NxauCNblldbYhkN2L2VDXEusdCJZbtEbjv6TydON5GrhVAOdzx6OjGNBc1WqiA4qzqZkjV9LyUzjYmSAk5b0FW9piJIYwAvrrrnE8iWgidpT79ZM5wvaOl5657w2l0l6Tmr9RWf/WzHgzeXLOQKkSUueGQsVrWvTEQs9uQkI2ND6gPjecv40JPH7wLOXYvy/vvvsT0/5+rykjRNVo2w1rMQtU42XiB4U4cauKuVsNUq2XOulqDV/Y2Yag1zq7BqiUHWTccZ2HP9QOcMwpvgDlT3WKol52TuPjSzqW7ewh7Q3WxtqV6793XMeu45yBiw7krNqFia1IhODqcJn7PVqHBm8RXNpi+W2li7BnjnTMC5n6z9bsARKrjnKnMUrI77DO6erm1Zr1YcHR1xduuMO/fucnyyIE8XTGMD2rNcNqYkqpaUqzI+669au+hUKssSKW+QVXu9o6uNzG3xsYYYs2Uv7DMgcRhzHesCMAN6pV9mC12tX9QcPLVcg5mKkevvib/+vry4QIhY+rzs+fcXwR1GJklWBrcmixmAs6eSrHdAvRrEAooE80SkeiNSjJppupYokbQbGVMiFctzkBuB+v19gdRr3JKdfIJF6Lh39xQdEs/PL7jotwyPNly985Sj41usjldMTmhDgG3G7Qpn3ZpPrR/QxobH+QnvPctsphEnjixCVzwhO/pk8qaBgcGbJxsFYoLYZ+JgEteEJRAexwM+//pneGt1h/LhJV/9yleY9JxdGa2+S7ZYUxCxfRfP+XbDu0+2yKg4KRSBHhgxRVGMHU2zIEajfYobKTgojqlkNBjhWSZrO9kslNsHB3zqruPTtx3rnPFXO9ww2jXkG1vbxwl/sYFV4ShGPv9gxXpq+VCEh9NT8m7LuJuQYvdaSSObix2aRtqQyUnoe8+2HxlSxzIc1musAjvROHen4CxJccyF3aiM/QKXDjlqE6drwbeJSSdCu+NoOVLG55yP56iOdN2aKF3tpeAJ3hPjEqFQMoy9stkGdhvI3w2dmBTl/Pw5w+UVw9jXgFy92UrNnFTr+uPF3LvZytJZO15lYzNvaaIUC/GNmi2hwjlrVFHT+6W26BOtlsF88843WPUK9pyvyvUNzHU5XSdzL9EZ5N2eojFGQfdQQaV7btYYERV8Ns26jsX6SWbBScJRDOZCIDdtrfkxISWBzje/26eiizOWU9TYVicB7wKqAS9GcZV5OZI5oNqwXq04Pjri9MzA/eRsydQ3DFshpyva1nqBq07smzZTuwyVXC133Z9PveEFGWp5U5H4iASjbWTPubg9z611ARUJIC3yArgX5gbNqtk8AxVbEHXvV9VjW783C9LlReCuKbpmPMydqqolPytjjIKZoBHSmMxLE7VaQ67GzvasjCXECRacL1EJjccFb55FAe8b2q7DFcewGxiGcQ/eFjupFCGYV6rK3sfLwmF7yPe//Rn+rr/j1xESfPmLX+Rnv/ALvHf1DB7uiHcnjpuWPk20DvzgWeTIm90ZP9C+zqJpec9F4rDj2eYKcYXsHSscLZFRHVGtUFhB6SVZtcaUaQdH3GU8wlAKXbfiU69/H7/5h34jnzm8z+6rj/nPt5HNs5/j6e6CPAHJFtg2RhbLBafdIZfpGZdjQkZoG0G8MKG1HWBDaBd03QFtjEgq5OTJImQ82Vkj7bEkihtw7Ui39hzfdpzdg6PTRDOOEEboJ7tkosLoYBTAXmuCcvtkAdqRtxMfvH9B3lnF19Z5AoU89uRxy6IVVosVt0/XHB92tG2H8wuCX+KKoNlTUoASLOdFQCRQKExlZMoZpGXZdJysFzg/EbuEiy1xseZk1SAlonJsyjx/SHYL65+bdjgmSh5J/RVXH5zz/L2JzaOR6TIx5uljcfU7AtxtWKai96aXxs/6bZhrWQDXwOpq8pAvL9paVTuca53p4iy7NGnGlXLtOqvDqbNFo7haC6OWzN2TFnuYh2qZz6V9Ra+tOEvQqV6DGD8rMgdv7bsw8/VQo8J7EHIq+1rb1tTaWmk4b5USg/M0oUHbRCqJnCY0W8BuTw14b5y8eHJWpnRdGMwKN9W2Zf6GZyPQtMJy6Tk4bDg8bjg8iqwPAut1YPRWbCxNVvDLyWQSVFduIPHcCu+6B+e1dnyOWfg9yFrd8H3edf08e57dLHkrlyrSYL7QtaUODmvXJXUKNzl8o1HgumLmC6vznvaYz0m9wFS4zlSthgXFFr8YOHn9LpvzC9K2R7MZEQbyFeBRHLk2iXGUAFMUiA71Rj95CSzaFU2zYNqO9P3IsN0xR2bV0oBBhDTLXYMDCThxLF3Lp+68zT/wm/9+/oG/++/l1uqI977yNf7KX/kr/KW//tc4HwYWjzInS0/BkXc9Sw28fnjG57o3eVtvEyfPOk+EcskHgyP4idEJS3EsPBS3YIFVDE1jZuMG2uhYTMpyajmMDXmEi93Ayfouf/dnf4z/+lu/lrPc0h8d4O/1fLB9iru0cgFzBvWt4zM+9ZlPcXJ4yE9/9Wsm2PKgdvCMmnIO7xuaZslisaYNEcZM6mGUAi6jsWEcdlxOPcSJ5Yljeadj/dqC5jZw2JP7LRIT0hesZ+eAJGeuQQY04UKi6RyLLHDec/noCXl3QCMr45F0QvMVPvQcHx7y9htnfO7Tt3lwu+H0aEkbFjiEnAvTtpB60FLwwREXAb9oKG6BugFcYbnqOD1r8Hj67QWaNyzaTNcsibpgd7kkhwM0LBiHNVk9aRiRcccyjMR0Qf/0ive/eMH5ewP5wtHohsz4sYj6HQPuzktts+aIwaPBWx0YYV96dOaWrfKhq1l4MNMo1+E2awCQi5KwxB2r/WKfN+15MXAvdkMahz5bmQa415Yge1pmD/oqUEF9Tv/fK+KcWSwq1wBiBunMNVvAzElNT1dHyFZru2hBUrUoRXHegnAlRpAOlxKTH1FnCSHsAT7U1m1Wi2KzHcl5JOUJpNhCERI+TIib8DERS2Z9WDg8LhydZA4PR5rmAs2esY9MwwU5PaekrS1sNXgopXo+Utl0mcPFNbS/B+uKqnM1rFpa1qj2WllQzfqn6ipswQv2INrrOlvqsl9TZiXJHGxFMsKEVcWfUGrJ4JuLzZ48k+uHVlpnlqWqKUYscq10y46/8zf9eqa+J28HLp9f8PDd93n++Ck5jXgfjQ7LBVVHiA2+i7ilkDAqp2k7Ft0a7yLDZuD82Tnn5+fWRHsyKV0MVRqZErkUJDh8iJQallg2S9689SY/8MbnaXtrpP59J29z+l874I3VA7788F12mojbjjY2jLvAsjg+szrlB9vXOZG75MsN02Ph9IkjnXumTiid5yA2rHygSGYpjpImSj+ycYG2BJbqOPSRs+6A9mDBcAynt+7xI6ffx9v+LqvRJJRl/QY/cPgm8qznvH+GE+Hk5ITPvPkZPv2pz4Iv/GVnCW9eLAEwq10n1uSj9joILTE2Fuj1E95NFOfR0LHtey5Tpl0FFncOOHn7mPWba/LZxKYZiW4ghIwsQKaM5h0hge8cSAtJ0GTa/IvLwocfbHnvnZHdZcKrZWtr7mnixNntjh/+/AN+/Y98is9/5g6HrbKKStDEbjcwXI5szyd2m0TJnrZrWR0tWEkgLBpCXNOJY3nvgBO/5FHreedLFzx7/Ij+OTTLlvOnha+9o6R2Re+UoYyMGaQkWhKnbSGOl/RPLnj0tUSryp0Tz/17C/7W175L1DJNE5A20DaR1EQ0hho5D1adESEoBOcIs8RrVjNQQb1ylrOiQYp1VvciFLEiO7L/f+VkK/VTzb5rd35WKVTZo6uAfhPcrVhX7deq1nJvzuzEVSDa0wKVNphrjIhYlqmzl+YaJAaM+QbNZAHSGCp3H0bwDcUlo5ioHoQL4Dw4T78ZuNpuQRPKiLqMdxkfEz5OeD8iMoFkDg4zh8eZ45PCwdFIE59TUs+4c9aoYLxC80DBo85DDogPxueLrwd0lpNS+e/6J9xc8SweILI/Y6pW2XKOSogEkwwyA7uBuy0CRsFQnYZKYu/VOaZxnzAGdwB6A/hZJqogeJBQtx9Ajb+lzJmUFsOxmg7JPMnQcXLvlkUBijJudty6c8oHX3+Xxx88ZHtxyZAKbQhWVsBlkmZLukFpFw3L1ZqD9RFlUq6ebthcXDFe7SzQh9Xe9wiaLbkpNAEJVeo4TnZZqrKWFXcWtxgfbfnw6jknyyNeO7vP6gfX3Ik/ywePHqIqLFxHry1+O3DrInC6aWmdcPXBFXz1gubRwGpbOF0pYe05WS846hYkrzSTMg5b0nbH1kXaHDlql9xaHnL34Izjg2NycSxXR7xZTjnul3Q9TEPPybDgfjlkIyccOcdyueAzb3ya73vrc5wd3+LD7RO8szIcwZsHPBXFlbKvdZRHK4sxqYPaBxXnUS/sgNEH/MEBRw/W3Hr7lOM31jRngh5corHBtwGXE3kwi5opkybwqdCKQ0pHHgKbjePxVeb9J4kPn1qTahM6FMT1nBwEPvvWET/8/ff43Bun3DtqiTIgOpGHkakf2G1Grq4Gri4mtDi6lHGNo1074sITXYuLhbAulGnD7vkOLxvGq0uueod4R24KfVSmlWcXPFelMOZEILMOyrNYCNuR/vHA5ony5n3P0f2W259Z0Dz+LgB3Adou4FJgakMF94iLEZcyDUJEaNQUM1HcXjOOVntRr2kOe6oFwkg3uhfNRInsy4TOJcP3gU8tewDf0zPVYp8VKXvbT0whIzX9X2pmHlg69j4Jkhok3AP83ghmVoGUCn7W8KCG//aBUqmfFwgt6geySzfAzYMLqA8gnmEc2O52ljodUu1XWnDeLPcYEz4UQoSDQ+HwCI6OCwcH1t5Ly4ZpKOSpp6QBLQl1Ac0RXAOlqWtUtbTnAzLzTnuR+I0dlbp4VTrFqLZrgJ9r8yAeaxxcBaXqa9C2UjC185DUg2g/US1tmYAB1R3KDtWB2UOwH42IzItGAxptHlrXpDlQKwmRtN8vbbAF1jm6ZUO3jKwOWw5PVjx5+JCLZ8/Jo6XoixRcjCyXa/CCDy0lKc+fnLO73HH+8BnD8w06ZJw6GtcQVCBlyJkYotkbybwVqU0wvCrL0nAaD2iHiecfPOZZumT9ZsPJyQlnZYXPa7p2ScAzEOh3z2mGnhweMzQbLj54n92TpzSTsp4cpIFWA7cax1FZkIqi047NrmXadOwk0OaOs+aE15b3uH98j8PVIburnnIFzfsjwSX8KAyPe9LDDSfbhrfcLXaLJUcHh/zA2Wd5c/0GPgUuN0/xCp04Wm8dvwbNjIoB+zDSb7ds2oapmZCskKfaytIzAG695OT4lNc/e4+7b6xZHCmTXDBpQGJL0y3wZWLoCyWAqGPaKmmrSJmIcUV2K/q+49k08qQPXE6Wg9D5od4jI3fPWj739jGff+uEe8eBhWwputmLPByz+sx8wbynDTNCwWPtLIN4JOzIbU+33LFcKEE8V5eOIUV6abkKC8bDJbuuZSOZJInGFUZfKJLg3DM+EfzoWZ2suPXWAQdvLPD/+XeBFBK5rusSg6dpI7QRmRp8LjRizS+aYlUWvYo1GS61+4vWbkg1AxLnTEUBFsSb6QLZ+/RU5mRvw99MMLoZDLVyAsJe+1EzDudgqlBpilmSWaip6dc1bCxIZq/J9S4bBmplrJ1YdQNuqLNFXqjJ7YEggeIMpPbJIs4sUlxAxVdwusZVM+jFvAqsJk7XBRZL4fBowfqgY7lq6RaR2Cg+WAGx4qq2vB4365c5N29IFdjlOo6w58bLHtilUjLibwQ3Z1/IyT6YPS9U87AenrUMQc0+Fcn19frv2sfVrPYbMkihli2o/T51DrpLXXfmE3XN3dsqbPsrdSGSKsUvXim+kJ1xq3TC+vYhi4OWs3vHPHn8iCcfPmJzcUGaEi4Ii6ZDnGcYE+dPnnHx5IL+YkfeTDApUhxegxW1y3YROAIL37BLPVMecd6xaCLjlFn6lqO4YKmRhcL55cDm4XPON8LJpx3u0ZbVRebIBwsO546rMZIuNzy/+hqCcH51QZbCetnhRInbLZ0MnCwyh0tIWeh3jtvbQNk0jK4hsuD24QlvNHe5096h1Zanl4nLJ+fszh+SdiukOC7feUj/8Jz7qyMOF4GrdEmXG271C463LTpAdyUEVVqFlbeifhEYJyGlQh4mdldXFA9N01pJEWrCl4/4EDg5PuDO63d5+7MPODoaGXYf8MHXn3J72nJ2N3G4DpAdDRAWDnzDqJlpmOhHIHSwOILhgNJNlGVDPPbcwdEGBbfD+cJb9zs+/fqKe8eRZdyCXkAZCNIR247Gd3Reaf3IohmZUiE0ntXS0zYQnFVXFTK4Ht8MHBwG7tw+Y/eoZbrIDOdLhumQXVizCyt6iQwLxXWKBDNYpmlAy4ri1hweee69/ga337yFO+3RED8WVr8luIvIvwr8g8CHqvpD9bVT4M8DbwNfAf77qvqsvve/AH5Pvdv+oKr+xW/1GyjVPVd8cJWiaXCp4As0LtFIsoL2aiV8S7YiUyll49ezVk2xR3yN2NSu8t55/B5N6829j7PNSUW616HPenW5aeHXB4olSFXb/gVQqkk3WoFiLwecVRCVg54r9kotVzvH9NRYGbLWtnpilweCxQdQnHiChD24q9TaNRLRWRsuxm17Lzhfa9i7+VAXnIO2jRwctBwdrVivlyyXC9q2JYZMDFWznk3GV/aB0blWSEYm2R+LfQchuW6qYU2f50xY0//O6hizuqs0UoS5eTJ7Dr1KG3XipmTR+rfO2bAzsKeXwJ26FHtEAqpu7yVQ6r6U+htagOoB2dTsehCp148ds6RWzkKL5S9IyNZOcbGgOWxY3Vlz/OCExw8/5PHjR1w93bD74AkUzzhO7C539Jc9cxhAisdrIBZnFT6LEsVzuFhz7+5dzrfPefj8EUOaiFXGufSRpYvopufqw3Omp1dMj895fplYTB7ef0q+vGRzMeDFc9gtWU7CZjtx+fwxSTPJCd3xmrhYkbUQLhLLPLFYbAltiwahvRw4vlLYBbLzBLfgVlpziwMOpg43CIuNMD6dyI8es7k0b/P8/ALSxIPjMwZd8ThDuhjg/R10A4tlx3IMBFW6UujEKlUG58mt5WHsvJDSxHZzST/1hNiwiC0ShXa54PT0hNXtW9y6d5vjgzVH3UjcBc7fec6X33nOwWuJ2582DBEJtaGOJ+4cTI5UIpQFSRZMfsXy9prX9U30rOcsKAdxouglU3rM3XuBB7c9q/YCLwnV7d4BFcD5Ft9FvCzoFh251PhYqzTRGmOik3mTOoIO4ISDwwNOzxY8fdhzvolQlohfsh0L2zihC0/0oI2nIPR9ISdYNofcf/MNzu6/jfrAo4dfYRxuCgS+cXwSy/2ngH8J+DduvPaHgf9UVf+kiPzh+u9/WkR+DfCPAT8IPAD+7yLyOVX9+KryYkDnPcTocV2LjAmXCi4pUYVQBI91aaLWUpkmy2pNKTMlS7nGW+Nd8SC1Vonzc5LPDNCyr9w4P9xMMyhVKnmtlqmwc/3/GaQqIFUsZ5+EUq7T0+f35m4O+6xLndeaG6WvvFnrqVyDe6mctv2S1cP0EhAXbDGoIGrJFMYhOzfVvpgOXxU3rmZOmvQP2m4G9zXr9ZLFoqNtG4LPeGcWs3pPcZZEYouT1bQvOduiU7RWeKSCeRUQVu9JaskHNycNzZHWGmiWmX6pwYd93FO1ur/Wgk/2HsTs9s7WerrxuM46RV3l1TFaY6aAiuw7Z1GDp4jWRuJ1P/b8vz1UlTFNOLVM58Y7YmNqn5QTAnTrJd3pku5sSfPhgvd/4QMevfuE/vyKsZ/I2VLyvQuId6aMykLMQlMcjkzrPfePb/Gbft1v4HJ7yX/1c3+TL777Zba73q7NKZG3Pf3zSy6+/i7j8wvCkBjPn/B8M1GGkbzd8Sw9NWPm+JRYrMPPOCVUlLhYELqW4mAsE91uR7MbyVHYlonRO/JuoLkcOO4FQsD7wNEQWY8Rv1WYMsvBISmSzrdcXb7PdhqYHKyODohXBZmEg6ll7BV5MlC6DXoA6rKVr86FqIkYqqw5tvgIVxGuAuzIjEUpKrSxwS0Ci+MF69fOOLl7h+OTY5bRE8cd5emO7VcveDo+42obKN0S2pofIc482SR2ieRI1pYxBsYSWB6fcn99THsX7jcjJ7EHvWAYWpbLnuNVwvMEycnuUhdr8D2ZRNtjPQsWsaK+VXlFewP0krCaDwlKRvE0Xcdy3RDaBMHYRN86UhoYMzhprZdw8ChKX0aGcbAENn/Is8uG/os9Dx9fcHX+y6znrqp/SUTefunlfwj48fr3vw78NPBP19f/nBrZ+WUR+QLwY8D/81v9znLZUZxQhokyJjQVdCowWp9TVeuLWqYCU2LqJ8ZhZBoTU+2Xqs5DTRwRnxHvCU5pXCFaN4RKjsBsJRpYz0FT2AdLuQnu12SKJZbYhkopXOupbwYOb7zuqrJmtvSv/YN9xUMVIYmptSYHkwgTUmWc5j2UOTaAwxPwrjGr2jnT+rtg+y+zzHBW+c/VGjNFcwU0C2CvVgsODlYslx1NEwnB450VttJZReQ86so+3gBSs2LLHtxN6SjX4Orq+3UmOAuaVTH39Unf8/Q3L7iqjCExd/yx42uUkilgJmC0G8mOGnPzbhuWBGXPc7hWr0E+K3P26xwDseN1/el5ciJC2zVWhMuy5+jThEghtqZyKUBKE/FkwYPjt7h9co/n/jFf/8LXee9rD7l8voWiFElIdkgyOsqr0CB4PE0RWvF86sEbHB0fcnZ0RPz/On7uq19iO/TkcaC/vGTcXDFtNviU6IDSD2yuHhFjpAPylJlSz1URutBYBUrvkOihjYxktrsNm3HHot/hpq3VTR+29M4zTgUZlFUOhEYgCN0ghAGKTEybAS5HuhHKJKRdz9RvkHVL4zzDU+uS1o2eMEXKkx27/AhdXjKun0FWSIoLGZ8FvGPZNvhVJHQeFxQhMTnwbcvq6ICjkxPObp1xeueYO3dPOTg4IPdbnn71Ay6/9BWaDy84PYMzv8RvDuF8A9OE4phcZMIxFGVSwa89aWXVhqRTXMwQEzlfkdgQwo5ll1l1iSgTOhSKWhE2U9t4yLXchVBvylpThmQWug5mmNRAviVkWk6JBCh+IsmGSQrFg8RosYtg4ogmdIQgkDNFEn3acfX0GWV3xfvvvkvXKFfbp4ztpz8WU3+pnPtdVX0fQFXfF5E79fXXgL9643Pv1Nc+dogIi2VrHW+mjE7W2i33E3k3GDTlQpoSMtRG08PE2I+MYyalRMrFVmlfH5WeaZ0iruBc2Ss1LMNQLKlp5s/FveB2XYtc3N5tB9gnq1czU+eqk1x/WbTSEDOHCzPJg9wAd+rrqJBrnsUETCokHMVdOxdOZ7rINNNIqaDuKU7Izqxss4BLVdxY9qiWRCYhMlWqw+quL5cLlqslbdcSQm0+UhUtVN3/dWr9rOm3SeVa5U9qstI+w9JRM1TnpXCOjZRrvnwP1noN8MzBUvayRzu+1cuqvLtxGxXcGeyhFdznejX76j7zumFxFNWZZ7dmHpbQa3TSntaZvax9+r8yJStSF/DWZ7MNKImpTIxjIURPs7TmzU4csfO8mR6wDEvyVEjj+/SXIyhEcQTn8MkossxEKw0Bx9XunGePHvPgzh1++PM/QC4TaZr4yntfh36i311y+fwZeejJ/Q63HZBxwCdF02SB3DYyeiFNA31KNDHi24jvGsYA2zxwUQaGMuDKhJsGhl1mVwZ23qHZE0tD5wJRPRRPTAK9lXDenW/Izy6JVwNdVqIPbL2gXgje4QfIKSOjkjcjV5sN08WO5aJjc3cHiBU5WzQEsUYhE5mxwJgLOQguOtquZXFwwMmtM27dusXts1PunKxZR9DdFZdPHnP+3nvk83PuLZa8cXvF7dUCtxFyn9G8YCLyfEhsc0G9o1svaA5OELc0gzFt8F7oIuTtJZvhghi2VnTMTWgemHajNZX3XeUl1YLf2ts1J70Zcr4aH2WAVK32avSNw8iwmyh9Q0od2U3QXUE7kXRkyBMpO6QsaOhY+Ma6ZI1msMTO41eRIrDbKUwLJL0OTfexuPorHVCVj3jtI4khEfl9wO8DOD0+rAlMHt+2uLUy9RN93FLEGv6Ow0DZ9ZR+pPSTNZsYE2myfqIz2GhW8KVK2gStdT5Eyg33u9bSdr7K85wZvM7VFbjuys0A3x4muNZDU+WO+8+a9eqcIr4GXp1el/rlJshfQz5idu4kZrUnrZSMo9bO0ZpmX2kO53EuGrh7R/ZCcX7vOSiFUluiFU3kMuEq4SskwBFjYLlcsFotaduWEENNpa8AXhc/5zzqyz4DVyrgmarH1EJa92g2gmdr2/6uktRSG5RQM0L3RdRr2HkfsN2HRvYOke195cepHCbzY+Aa2OdLrUoc95ETG0UTqN0wc1tC5wGsfELZLy6yl12iSkngxRNpiJZ1Q8l1sXVCo5GmREL2iAglwuWDQidnvH3wGZZngfd/9h2uHo2EWuqgYAW+RgWvBY9Dd46HX/gCP3D/dV5/cA998/MM5wPj1UTpzym7J1x88HXiOJK5ZOe3uG7CZ2sA40NDu14TmobtODKkieXBivtvnNCsG54/+YCh9CwXgYt3v8754glj7hlFSAQ2/Yhvlki35kon6/9bGuImkx8linNsH/Wk3UgXFLpCdht2+pi4DEhX2F4IaSxshh2baUfysCkTcrXhw9XI6mTJ7Qe3uX2wYtj1nO92jCR2KdOHQAkdq+MDVqfHHJ/d4vbtW5wen3CyWnLoC7q95Opqy7MPPmB3/pyjGHj93n3u3T+k7TKbzTlliGQNbHLkvWeXXA4jofXcDQe0JSJJGTZX7PqJMV6iJbC9umTbnxPcluA8y04pkxlFJXqIbt8KVEejWma1Mg7zOJyVG2dKkMpe8jYMWzbbnpJaJK8JC8fBLWE9Fi6fXjHtetIQcP6Mrpyw0BYdJ8beurgdH61p1w1rCRy4Axo9gHHJI7f4WDD+pYL7QxG5X632+8CH9fV3gDdufO514L2P2oCq/iTwkwBvvX5fS844rKxmh6Pf9OQYGIBpmtjtdkybLWVn4G79P63Hp1VHFAP3kmuGZzZyQhNGcmTjocXhvaf4gA8efDBLv3pX4oV99x+RPajXF8zLKqWW3VXmIOoe3J2VNLBiZw6KXDM3N/67FlTadosYT5/EuuXM/y5ilMcMOsZhB6wCw3xlCdk5Si2fsLfaNVstbkmoZEvcchCip20blsuuBlIbq4MiMzDXWc7eTbn2OIyHrtmo86Kz3/Vrz2heGOcsUq1xkllBIMyFvuqCpPrCwyx5WxWFqpIR672qDFyDu6Uezh7QXMPGGq4H5sm5Goy2di55r8z03pq6kBWrm4PVuk+Qs9Tz5gliTZx9EXIakeJpQiT6QMRbYa1khsOkI8/9Oe6u8vrJfW7fX3B8q+Xdn/2Q84fn7M5HpgGamuMwpUxfhH4U3v3613n63oe8fnaftw5fQ9+G3bsXvH/5s7C74vnD91j5gGiPhh4tA8qEeGWxXNIcNbhV4PTomMkV2nXD8VsHSBjZ/pfvoX7i/pv3eLz9gMfvPeLOrUOOT4/ZjT3jZc+YdvRuIPgj8pAIG0Vzg145xK1I2y3qC+VQGLsCi0x3KrSHGb++QKZM2Uxs03Mud1sW6wNiu2bqC5OM3HvjLs0bK06Xh7z33vts338fxh3FFcK6ZXX3jNM3HnD7tfvcvn2Hg/Uhy7bFTwPp4dfZbTacX265ePQIKZmj0xNuv3EbfyQ83z1ld7UFVUaF8z7xzsUVV/2Org2UdcBdRnzynD97xjmOK2nYlIBOkPoNjivabs161VLaiIV+AroTdEzkvlhpBYFcEkUmnMs0rbOmN4IV/RrtZi7FUcYdU78BEm3sODw94HUXKd2WMZ7z/J0LJDn8uGCRoR08KSmlj6zaJadHHWdLYe09Xe5gs2C8aHnef5QtfT1+qeD+HwC/C/iT9fkv3Hj9z4rIn8YCqt8H/L++1cYUSMEaG7hs2s7eJ7Yhs42FvoG+EcYIebJuQkVmPtnAXYsFDMX5arlaWnPRbO4tGe/EOOpcKEH39TuKVm257CsDVMrmBlDJnoWt2zaAn0F3BhGpzYktvpdqpVmHBjHL1VupgTkI6Wqnp6KOoq7WgLe7XjGtvFNLoDHFDPaeA7wtaHixet2hyiLVgpEG8ImiCS+F2HjarmO9WrBcLViuOrquoWmCZfvKx1wscyB0n9xVm4Lf8IScF5NcWrJpXfTs+JSiKMkyW71HdLp2dsD2v0jVs1cPotZ+0dqdyapPGhWjNRsVEte+QzCvw6oQcc271yiLuNpEPO29K1y2IOvcBUqVXEMmOdfaPdSFt3KnqGUNe29Ulso1TSeqRM0clS2ZRHuwoFsfIyefIb51wPtffMjDLz4hvbuhbLNVUlDYFeVZnnjn/DlffPcDXr/3Gd44e8Dnl6+hd3+In3k0Ml6dc/nkAmlbOjKhmDY+pZ71esnZ3SMOzg5RKZzcXxLXkewn1D0ilw3evU9cCOv1gqZ9Qr/uWd+6x73Xb3O1uWSd1jx85yHbZ89oUHK/hVIYdkp+viHGQ+Jyie88Zj4l1icLTm8vkeVEkZ6TW4Xdsx2XX/iALl1y9+QeBzFz/mzDExKrWyccHByzahasPRx1HqeJwzYgJwes7t3m+P5dDk+O6ZoOcqHfDozPnnD1ta9ByQxF2G235HHH82big+fn0PeE3ROmi8fmjTUdU+wotyJNcUSvDLHnYnpGKwEpA620jDmz7TPZLUjSIKUBvyS2a9rO4/KW1GfyppB3iZJAS7HuU3lDZocPia4TujZYH9fRoYODKUDxOF9YNR7xHc1yjW9u0Z12NIc73OIREw9R6RlTZjEONJtEwNPJmpP1igd3HLePlY4B2RQmJoZB8UP7sbj6SaSQ/ybw48AtEXkH+GMYqP9bIvJ7gK8B/yiAqv5NEfm3gJ/BfOU/8C2VMhU5prWyG/7/1P1JrG3rlucH/cZXzLmqXZx9qnvfva+KyIiMcEY4HVnhdCFSNgKLDi0QNBBIltxBQkg0bOin5JYlupZogAQCSyBBD7DBDVCmk8yM0hEvXv1ufc85u15rzeIrBo3xzbXPs/O9tEjLerGe1tvn7mJVc87xje8//sXMzfDIdHhgnvfM7kDeVsqLjtqfkx8j88OR/DiQjzP1mKhjpk4W59ZJoPNGr3IVECWqI7hgkJiAOsO3a7MjyNUudFcrrhakeXmY6ZY/Of45af4zLPVNeHKYNYhBqj3JAkcwK+obXBQK6gMa1AJbokXkBe8Rbxx1JFBQci3UlnKpTallAR8WXFBQsmAzCmdSdRc9Pnq8dw0yyq24J9u9iNJ1gc3WszvbsNutWW9WrNe9MZScOxXak7DndF9MwNo5weKz3szSnPnaOL/QLxt0I8vya4uhtgGs6DKstWNkC4A3SKYKy3DTPsmCktC6dOwTqiOcwuFKm5vEBg21OMX3RFCLgMyJ7XKcujZwr+AbxKSFUlIbPi9zMDkVbTAzL9Tsp20WISclMmIeRk6go7JyE0edGHRkjIHxtWf34kO++d0rVt/6ii//8DP2Pzow3IyQCkFsN3ebMp98/Y4vP33LRzzn+aHjd8NHxN0Dn42fwrGYsMyridS04KXy8nLNN775nNWzHfubLxnvfkKtkeJH7od39Ds4W1/T90o9HIjyFbvvvMKvlUN3x9nzM571G6Z8z3T/hjJlZOo46zuidkzDTJ0H+s0rIHLMIzklOnfO2bNL6jow5j3nzwfC5p7u3Vsuysi3v/0R7vod+88+IWfP5H+H6pXZVerVGednPdu+Q843xGcXrK4uiNsNqJDGifF+z/RwYLq7YXq4J8bAXD33+yPD/oEyws6N7FcDWx4IdU/olVVXWV91PF+viZ2no9Dlkd7NeC30nSOEHVE3iDgOYQN5ghKRcEZhw5SU436kHI+UcUYy9N2KEAJznplypjDiwmiNSbb0YiZPHQLMEdFIOBM2qxUu7vDxDOnPCOszfCzgexDPKt5yc+3J6YgcH6lsCL1n6wLn68Czi0IvyQSMuVJKIB7/GXnuqvo/+AU/+td/we//XeDv/tMe9+duAvOmcqwT1/mW2/0byjzjQ8WfQVxv6J5tkP2E3B/Q+yP6MFIe7Z4fB+rBApWjl+buaMXHq8M3k7A2J2twRxuAtCGJSLX4PqlWrJoKLYjHu/DkALiQn09wiz49RtvSL1a/2L7CrA98QHxFgqKdQCe46AkdBPWIdDjX226jGAasTk9Bw1UqRYr1qlLJ3iARFUeIjhijFfdAe1+N/632b+dNBbzbRc7PG7d9u2a16oidtM59wcFh+cfP/6e12uZqaRVtYdR455+yZ/3pnYOax09t1rZUK666hCk72yUtLhDa6KjaCr/5xU8oM8pohV1HlGVgVTFbAQEX2tFZdIPvFfe2y/DLe2nf1iYwWzp3c7i0TjyXBhex7ODa4ZbWHDStln9vML9AgUFGViuldMLgElNNlL6nW215vfoWZ5eX3H98xyd/+AkPn95TH5VYHGMt3Nze8fbzNzy6V6zGwHYPH62v0Gcz83igr0ogE7TQNYroSgqhjEz7kesvfshhvuPi1ZbNZYB8h/pKrNe4eYYh8upyYjgWHvMjTpVn55c8fP05w+NXdH3ifLXlOI5IurNrwG8pCq5sKCVS3EyqlfvjA24fcE44kBjHd9R8S/ED3VkgXK3h7VvWHNnELW/3DxzmA2fbc3a7M1avnhEvzuieXRLPLV5ynGcO948cru+Y7x4pxxmmTNxtqKo83E+8vT+wv31gXsFVLGyeFTYbZbXxdJvK+XPh8iPYXHq2246VAzeADJXpEXJ2aNjRh9fE1Y5r76jzgTJGhqS8u80cbgeO72443txRx5ndasPL5x3nFyuqKCojqr5d/oWUqiltD456jMiMCdViIGw7xG0ph440NNfQ1ZrXrwKrzrPrIp9/cuTtFwceH98x5zM0r8kPnjyYbXG3nojbgaCFGDtWN7tfWlZ/JRSqCgxp4DAduT88cP14R6+ei83WFJRhxZlfIbOSh0Q+Jup+ojxOTDeP7L98y/HrG+owW3BwMv6xaMWphUfDaXz5HjlOTi9Al4K8TMQb/1CX7EhZkoDqE3GjAkVbYDdQzHYAVaq6ZmmgqJh3hrpKzfa4tVRqrtScCVEJwRFitNe1sFUaxKFOwVWzKPAKwTjbi3eCRg9dhOAhGi/ce7NDcN70A/0qcna+4cWLLS9enHNxeWYUyD4Sgjbjs3Kq5goGkbT7E1FwGVAutghmK2zduyDeIcFhPvwFp0bB1KqWPCQGwbhmIua88byf5hYN02q2BOhM1ZFSrWOvOqFMCLkt4padq83TxyyUOeH1p8V3eVeLYntZOMpsYQwlU0q2wW/bpdQlIakafCdt51bVXDerW8RS0XZMYj+bKEy+IsFEdJ0K2wrDNJuCVTquPn7F1bPXdLszPv3jT7j90VumdxPjNLA/3vJw+4b92Vsu5Iy1h80qcj6vOEwDKweRQigVT6GmzOPbd+TpkbEeeHv9GWEtoAekbji7DFAmDscZ/ESIjnUVbvZHfNzS9T3Hh0d++Oc/pByOfPjqJRf+ks/v31CHicpoIjn15DLgxLPdblifeR6nI9dv7ompR3YddTwn1sB65Vhl0EOP5B2vvvGXuXt+xugG9tHR73ryxYa021A3PTkIUjN1rszjxDwnUMe639KHHYEzkgpfvXvHu8MNd8eZkgPBr7ncveC731jxrWcju+6a0D/gd5nQ3RL9I5sY2PWO2AkleMoRjgdlykdKqIwuch8r96kyHwv3N3d8qQdCOjBc3zPcjkiqvLiYGQ6OVx8IZ5cRfIfzHaHLhJBxGMRbMlA8Xtf07pxQQUfPPHTcHSsPkz3vxYvC+TPPs2c9rmxZk1nniU+HW24PI9O45q7CbtdzvvN0HPHlAcfMqos4OfuldfVXorijyuF44HDY87h/5HH/iN/sWG/XPH/2ghebC662F3TSGX6dBYaMHhOHr97x5vs/4d3qpzy+u+V488iUjhbAUCuuyZ2VZU5qGLrQGBH6pCIVlcWPCqmN9CiNyidmI2y0P7EO/b3irgtov6RqnzxQrLBUbHdgbpUVnwspJWKMxFjpVh51sYU7GE1EgiDRtS6z7S4W+xNtnEOnSPRoF6y4d3MT6lrB9gF8EFbryMXljlevrnj56pzLyx3bzYquC4Rg3jMnyKktUNpWPW3tasWWxFM4hfjWuT9BM+LNrlkouCrUanMIVail0UadWRf44PA0LxlnhX2J0lNNUGdqnShlIJelwE+ozljSkakQXctcXRg27QBw4hkvu44lnR5s6FwTtc7UMrcM0Wy7i5OpTNuBtLlMqYqoaS5ESxOYqb1/b7OYUiszMMYVvXeEJKyzcFEtZ3POMFVhLI6ByuvvfkxJUI4zw3BNTRM345f8+G3HB8ETXn+H59tzqlb6g1CDp3cVyRVSompCqnK8O7C/vWWuo00hsnBfRuZDYbpaEdeKli3qI/tj4fERmHpWZ2fkO8fnD29IB+Fs85zt7iWu9MTtQNJALgEpjuo8WYVNXLE5u4KzyHh4w8PXB+IknL8+J7hz0v0Dab+m3o28vS+spmdsnz2jv+jpdp+xfbFl+/wF3e4MDZEDyjTsycOBCriihLnS4elCYOVsJ7bP8O6456v7B44ZOlnRd+e8OH/Nr330nF97caSrFZEjsx9JmgkV1gi9c8QuUlJP9IFaHIdD4XGeuS8D193EgUfScGB6uMbPd/R5RA8z033FJahzQesdc1aez2vi6khYJVbr2mxEFB2UPFZCcnT09PGcGJwFbRxX3L5Vvn4YST4zpoLzkWdXM5fnlfiBJ4ye4/WR+aBogjIreVJKjohOoI84DsTY4dw/o4jpv4pbVeWrr75mGgaOw4CK2ODv8pzL58/YrXasuw3R9YgGY6Oci5nr9z3z/sD4cM+cZ47TSE4O9YoUIWesWy7NJPY0GDVIQJWT6nVBk42fzpPTIwuE02CDxneRVrjeF0fCabT69EQALEWyWoe4fFeh4qhhouaIk2jCJ295ki46CM13pnnWmGVxbB09uOhwXUCCh6Wo+ybYDRCisFoFLi52vHx1xatXF1xcdqw3PTGKdR2mkGoF7efQphNMYVz2ZfDa5hHizVtfzG5YfIDgQJ8SiyjZBte1ufzVBdw3mbhr2wM5HZwKmqh1pJSRnAfmNFDq2IaqMyJK1CUKrza90ntDYX06Zqfi7moTmFWoFS3ZHC/LTK2ZUst7wqknz5lSC7kY80hraeEcLUZDaAud/buUwkGEh/6Mq7jiTDpe+p4P3JadrElZeciJd+PEl/mB2kXiR5XVY2bvVhx/9DX57shnDz/jT4LSXzke1lfsnCeGymYViBRqFVKplJTpQqCkSqmLXa4n54KbHfXRcXPIxB5i3+EkUcpIyms6fYGftxxvCjc3IyGck4rnXS5sew/r50xDImlP3+3QumKWQE+E2qFjwA8rZJ6RGhDfMz70PNwUxusZOXhuS+Ws37D1PW975eyff8bu4y27s3N87BjmxOHxkeuHB47ThCCsXGSjgXUWchKmrOR85Mv5HZ9+ec3b2z01O1bi6Vygj47zixUXzyp+dOSxoCUbRl6FVRWigpAJPrBaR87OIodZSYcDd3cT137iIEfKtGd6fMTNI2up9CVQmnfH3VFJVA75yP2U6Dcz3XpivZq52Cpbb6pjd1TLrw1K6EGIVA3UFJiPheE+MTLRxZlt71m5wraf2K2VF1eB51fKMCVkSOQOdtvK2UbZrpUuZaJMxOat9MtuvxLFXWvlxz/5ianWC6y3G84uL7m4umJ3eYkUxzEnfMm44gjqLYqr28B2hTtbE843uP0aOa6gTJCKqVzHjJZCLYpTXWZrJ3DhtGV3vNfptTSlJnt3Uk2BqYsXjDulLxlVDk4sEtyJQsiJnseyZJyKo8gC+ZhXi6aZ5EccxTjnBIJTfADpHDUIGrBQDrGfabBBgIsOHz0uOCRng2KCQTIhWIJS13s2m56z8y27sw1dZ6+h1IJYbI6FcWiCBXuui2e6b5DE4nvvTqwUxZ3YQkusnpysgBfTL9d+ThtQ2vZDG/XE/MfkZDKGs4XAJNwTdSnApQ2JsbhAVFjSppbX2IjxWGGX9lwLC6gVdimYEGph25h3vveuFXezHV4EbLVWSi3MrbuPIRCDnUilVuacqFSc2OA4e0fmkmEO5BQ5X73gN198iw9Xz9Bh5nE4cjOPfL15YF8Lh/UrjpcfMXz3li8/+CGPn76h3u6ZQuFhe2QsE8+nng/Z0a8inQuoK5BG5mmmFs+cC0LHym0B85iPboXDU3ImOse0P1DyaMzfnFj7c8KxY8ojHM8oUTjkiceQubjAfFg6Y45IvKBMjnGCup+ZeTR9hYeN37HKK/rjhjRHevccztbk1UjRwqNm3h0e+dodWXdnELfUoTDdP7LfH7h/d83tm7cM+yOuCgfxvJsy5TAhuRJxuOAZqAy3nnDXIXlms5pwcuQ+3fLp8DP6mtltb5jKQJkgZEGPAS89vkQrtAKrXeb1x4l++wUXF9dc3Xn+6KvEdDdTpkSvgcwZj5PjIFvo1iRxjOPA2vdcpQ1XY8dOEnr3jvnuU75xpnz7Wc9Hu8i5ZjY5sapv8PMjx2nLsNnhdMvV1uFy5nE/4R885YsV07xC145UEw/HPZvnA+s0cH8zs90Kz56dcXVxxvmZI1bFJSGnker/AhT3qspXb97Rh8j5Zsd6u2W13bE+O6ff7siHicMwIlMxrxk8W/H49RZZdbjdCn++xe03yGGN5sl8sOdsw7xx8eteMNllskoT5Tx1piwdKtpUxRZ03V6ouUYuxYh6wnelNrpdw30bkZKFK78g1vatpZ3URsUs5JLRNOGdUgOEZraFF1xwaO+hE6QV6xoFgkBwSHRW2IPg9oOxf7zi38PcYxT6PrBe93R9RFwiZRtUhZwIPuNdAi3mxrjAD87SmBaOuBN/6oTbRoRaTbHq6s8RD62oO/+fUbk+fQ60ofdicraoYm2eYFQ7k3GbNwda2yzF43F46QjS412Pc9F2DeJ5ms6+H0OmT7CMFJAJccnmAr7RTwmAR1xECJTS1LpOSDmTZuvw7a2FNljVU0CUE0FrxbnIpp4zP47c7GduzzLzZU/fX7Fz8MKNfBRnvhtHDiUxnmXKB6B/uXLzm7/D3ZdvOLy5QY4zz1Y7bj77iuluIteOVdebPoOeMK2Zh4k5V8aE6R/SCqkrQr+FssK5ns16Q9cF3j2+oYwHfHTk45HV5hw/K2nc0xczOdMayMGxdxYWst09Y9XtqLNnGieOKTEfDozZ8k9XuzXrsw3n3Tmb7QXrlx1h21MoTHVCpDIc9tx++SW+mG/6cJ0sHPvxwMPdPQ+39+zvHtC50PtISpXD/SOPt3fUXOljx2q9QX0kjxP9lOjczEWf2K4SRe652b/jzSPM22zwX4ZQoMyCZI+vnbGc+kJYZy6fF/rVyMVl5tlV5N0Md/tEOU7Ebk12a6oG5txTZcPsHffVcUyBMvUwryi+w88jZT5n2CdyHwl9Ry8T3XTEzUdou84hjfT9xFp6pFe6lFAN9JODh8BwKOzne+7HG4qb2GxHXofMxdWKb7yaOF85QgXX1NiZmadA9X/y7VeiuNuVHlBMWVbUU4hUiRSJjGXiOCXKMOPmQlCHhhVdKQak9x3sNrjzHW4YkZLJhyNTrZb24hzinmiNC4XNBDmGq2or9FKaDEZaX7pYFUDD5a1GWOfZOtFK84E3dahz3tg4Sye5gD3y3ttdmnpZfBTNTpda8TSWTOOyE62ASx9wvYPemwImOujcKa4MJ0gKOKfm4e6ssPtgA9uu96zWHX0fEEmkNFPKRPCJ4BO+Weg6atuZtM8smOWBl4DDGaOnLhqBNnuo+gRNqTTBgAM1FfDi5QJPzfVJSOQsXYuGrVY1qEUWNWpTli5e+cZZCqfiHqRHpEMk2ANWaJlqLLsE3rMDtiI/gyR7v07bQNRCuV2zVC5lmScoUxpJaQYUlxJVi6UJeQ/SESQYzbaCJGW7h3kQ8lT5dL7hP8nfZ38/8BsXr3klKzazp9t7zgsMBVIQ4uWOb/3mNyi/lpC5EOfC8e0tPw5/xvH+S/IhM+REVTGOu/e4rmNMA9CDrMgaCbKm1BWiG8RtUV0xDRD9C6qsKdNElA2dW6PzRB0mBCV2PYVILgWXO7bbSzbdFtQxpZlpHowBFQLSVULwJCY61xE3HdvXW7pfd9Qeqgpbv6aPESdXpIcrVu8+4f/+7u/x9f0j42CpVvuHR6bjiKuw6VZICNQ8MQx7Hh7ujNSw3uBdRVwiMrPdKBdbzwfPI9946bnaTfhamB4LQ650KugM42jGe5Jt8BxiIHqP66BfGx15cx7pz1e8vF1zfn3guL8hekcX7Xx62BeqzDjfoTtH1dk8X0oiT4m1z5y/uuCSgb43lX3NwpygjI08IaPRr6vgJdNpOFXeIBk3HRnLkeFww/F4g6wql1vPR682PH91ye6sJ0glHw7UMlmBr5k2IPqFt1+J4i5A1+0IPuD9GqRHpaNKR9LAMSn3x5n5MMKciOqI6zPOS7FC3ffI2RZ3PMdPM64UslaO80T0juL1FH7tXBveibQiswQi8xTKoWJdqDzdF5aIGRy2zrBqE7UsoW9NQOMw73SnJ6hAFwGQmMe4LIXmRPmrFLVc1Io2saU0PY4g0VtxX0fcJiCrAJ0V+XqiSypyMPtUo3S2HNY2WA1BLOLMC6Vkcp6AgeIT2WeCL7ZTETVvfedw3ttxkUhwAcHbzCCVVuChtMFyKYos8FezOWYp2o0qafz39xSwDY55OiZtqE1Tsy7KWrEzxSATh3fvd+09cnLFpM1PFq+YxTWytC3FwjKyuEEo7SP2WKh4xLnOFgtnO645J1LObSjedAi5oN4+HxXIWtCs5Dnh08xahTJnhnnirQjpMTH7wj4NfNuf83IK7I7KzvWE7DnmxHT7yMQ9Gdiut1xsztltd8SPA1990XH79U/ZjxODVtZOiF7w6xW+FGprNrIriKuo2K5HytQ8rCq5TFRNJqYTmKu5YhI8Whx5nqkoXR/ZdWs2zpEPj0zjzDgmtGAGc/1M7RzFO7TzdGdr0kXleDbxdfeAxkpXYa0ezfZYq5XSbzw3X77lZ8PnZvjXEo1yKqx8JDthVmU4HpmnI1BMCxIg+EwnB55dKB++XPONV5d8+HLFq+fK1cXA2WrPKgx0ZSaqp1YHScjZcUhiJ6kr7JzQi+I6JcQKLpP7wvnFlk2v1PSOzEyIE4FK5wr9ZsX2YosLF8x5pJaBIAPzeEsKmc2zNWtxdNHOr2nMTIeCH9qm0xdCNxPjSOcdVSqiSpoH0gBJlKQzLh/Z1EJwcLnbcPXqivXFFkehzBM6ZqRWpCo1178Yxd05z2/8xj9PcN62YHHF1eUVsT9nzp67feLr6wfGxwPkQu8C3fqC82eVVehwuzPWLwpbCeSuJ3crBudausuROmaYihVW756KyjK/0/e8AOW9jFbrXU+saXTpPbV13qaq1Sb6oZmIFXiCeMQwZ10Knep7DL3SHsrof04Vz9PdLfAQDQpygg/WrckqWOcenfHe1YaV9noKVY2atRhVipiZWCmJnD05T+Q8ojpQQiH4TG0LgeV2eise3lsuqIt4F0EdFYeUhCzUwWYvQLIuv2ZnKkEPVpBNwVq9bzsVh4rDe3caRspCdVHzkXFiw091oN5DxI4drQD7iHdrnPQIvW1zmgCKaqInastSxTj/yzGG9nk483lxNhXFqdkWOLHirm2mknIilWI6B+dsUQOCt6g0nJBKIWeLiOtSRfNsn7+rjE55WxPzIfGQD3wlZ3yYVrwYAs/TirPSsXErdmHNqjoeHo/cHR950DdcnF9ysXvJtz6GzaPj+uuvON7fMuZkoilfLVCqtoG/TxQ/UJ11nFoHSvZQKqXMKBMqxlba1x4EUpwoLlHKjNNK5z0rVwllIg8DdRjwqsSuo8bCxEzSQNidcf7ha86/+Zz+1RnTi8Dj84jzSlcCq9Kxnh08zjx+ec3d/dekaWI87NtcS4nB4bQSHNQyc5hGpuGIkNmd9UTn6YKnC5mX28xf+lbgN399xbc/PufqInK+seFvFFOE5mFEJ0iTUEalJE+dzSo7rgNxBeLbnCVmY9D5Std5VnFFJ4GSEqUeUQK9F56fCx98tGN3sSOVkeP+lv39wPXxyHQYmFczbutx3sLpj2NCh0KcoQt2dXdOjb5aE7Wa2eE8zKSpmHrbV7qQ6aKw6jou12vOYo+bCzkN1HnEFUUzNjxPBj39stuvRnH3nr/2L/xtRExdGsQTXaDzgfEwcPsw8vnXdxwfHpGqrGPP5mzkaq64rsOfXbIJK3S1xW3Pqf2GRwUZZ3QUGCZ0nK1Meiu0snSPy2uQpZEWfINivHvycTzRoxe4WEDFmZJUDXutS4GvJjpq0VAs+aGN28HSlC+MFK8OUU9QtZxYVYIqTm2VtnQoNSGVNzWqxGiYuxeKmsqyNDbKokxdePyGdze+fk3kLMxpIqUB1dH+LlRqrQTfAk7aB+O9J4YO5w2uUCziULINJrVKg5MU8QWfvQ2zaQwS/7Rj8j4grtlFvAeRLeyUBUZZBpzOVYKHxcdeq7fCS0Rch/c9Ij1oaH9qS6uWbJ4rOqENfoGEvBfCsuwSWF6HepQAtK6d2Iq9WU2XUpBgk5RSS1uOfeO8Z1KeSSlZUlOtzAxI8ATviA5mrdzkzCEPvK0rPpl7Xow9H0wbPpg3vC5bXrkLLuOOflpzc5t4e3fPYziwfzXzou948cE3yCkxjwNzGil5JlAgLHF8xc47n4yX3vz9jfGUbaGTGVxGUcbckdWOn4jgoy2dSGEaE5rMJK0LlSpKckcmVSbvcefnbL655eIvvWD70RWcrxnWjmn7jGfrLZdlxfZ6Jn35lvufvWP/9Q1HOeK2SueEECKCUEsmu2ZrnWamcSSlydTU/RYvgpbKpkv8+kfwe3/Z89u/FfnoQ88qZqJkegGvPekoHFNgGpQ0F+pUyclgWZwn7j0uQpaCrxBWheAKtTqkVlah53y7ZZozuWSqJvou8Pxc+cYLz7OXPbUKjw9H3nlhvClMw8BwmJDtGcGvKLmSZsudcOKo4gkBnBeolVIn6gx1TjAnmMwjy1TrZmuxdZGVRHyGMhxJ00DKCSfCPBXSaLvEWv4CdO4iwgevv91YKHb51VTIcyJroWhHkZ7qs6Ws9D21W5Ncz+w7SjRhjFSI4lkprMeRzTjRpYAbHtDRdmZVTnrFBlNbIacVci/OiihG4zOb3VbQW9ctrhUIVcqJUl0bvLJ41YgVejE++hLwLC1RSYwAicMGU16NBUTO7Z4gOTQJmgSygxKgPmW8Am1QXKg5UUqmlnLq3FWX4iUtuHvp3IWcZ1KaG2+8YdK6wE+OqmYQ41ywC9HbXaszpa0rLOHVtZga1flKTYXqpXVEi9mYa+pVj1TrlCvSOvelyC68dAEMilFnQ20TSnmoHY4OoWHsCxyDawPXJTHJaJTKYMXdWYF3CIv5miUl2F00/Pz3WkC3U4fqTM6FWgulVDOpyxnvhDlkIxdNlXkcKKWYAVuARz8TnbcGIENKlSnDvXputONNXXMxr3iXtzzME4fDwOHwyGt2PO8uONeOEna8G468++ItddOx02QYshcKlVKN7x8jiKvkOVHKQMV2WdLM5GiOmstnITVRtDJJx6wWCuFdoI890XeAMrcmIHgPQZgp7MtEjpHN60suf+1jzr/7TfzrS9JFYN4Kdb3mrL9gnXvS9cj1D244/MlXjJ+9syL2POC25nTYh0AthblktKS2dSs4MWuAVQys+oBmY0ddXTr+xu8If/13HN/8ZmW7u0fyhKZE0IArvQWUxxW+d9SukEMip8w8F6ZcmLLjODu2CTY1svWOuK6I65jHRO/WfPPD1+ScGcYDw3jEB8f5OtH7B3xOeMls4566y8wvlEeUDtisAn3n0TzhotBtIhuJhGj05KqFeUjUeUKTIhmCE1znSRSmpEwjDEPFh8T6LKMbpcyVNBamVChZOexnpjERnCMX4ZfdfiWKO0BOvvGi7UKuRUw04tbsLl7ywUdKSYkYAuuu5+rqGWF3zuw8Q54Yx4mcKzUEZLdj+/IVz8URtKc7CvWQKTlT69INVzPkEgvcxi14vJlWWa/YuO4sePz73uxY8APLQBRQ63Dej8hYIJsqgkW+CaLFuml1uOrxpRAXjB7wDpLThjUXglMLWuodLkU0ZSRbvJm6aiKcNFFyoqZkBbeata8TIXi7W4aqFfKF+gfesPjgCF7fS21aPg9/YrosBPiFDmrmYe4k3jHPi0IpDorg6vvFvXnsO22LXPOlafbA6BKYjS1gaoul2cFYjCDaIdojrLAOO1jXXmiLWaY2RaupWQfwE+iMkBo01nCn5fGAJ4tgPXX1NBqk+bkncs3UZvG7hLSUYjbBaZ6ZxgFQuq5HvGPSmZCAuZCOmXGYmZJSXcT3G4aYGSkkzahXSkg8zHe8vX/DB+GSq80l4jtisIV6/7BnziMlTeZLFMUWHZJpIqhkl8h5wmtC6GxBbC6e5oxqcW+qyTj7PhjDCEfFM1OaxdEKrd64/WqajBIEznacf/iCF7/+bZ792reIr18wbCJzH9D1irja8SptOXxyzed/+jOG73+BfHlPeMw4Fea+thCeDD6gJVPnRJpG8AGP0AVjVnkHJc9QKmdnO77z7Y7f+e13fPfbld35gRD2aE5Nz9LDWJEacQS64FmvHTlXUp4Z5plpKsy1x8UV/banzo7xMDDOB/bjxNsvrynpitcvLui6wDQe2B/uUTKbbSHkG+ZHLGheE5sw8tHLnrHfwZi5POtYBQ/Jsdpt2e48awKakimWszDPlTIVZC54NWsUCuSipBnG2RyTwiqxvVI2JVJLJGXPMCXSXHk8VlJydHFF1QVP+CfffkWKu6DV8j/VhQVxRhF8VM4uXuD8Godlf666nvWqp+t7ckoMeuBxNkc/FwJuu2UDsOpxydHdzsj9SBknUkrUlJFi1LpOzOME53DeEX04MTsEMRbM0rE37NsolGD4rjRXx+YWqUrRp+LeNE6tV69Q2mBVBVFv5k9NQOQanS4vtgNSjTUSwE0OnQOasyXBZIuaUynUNFHmkZztvdUW0oEGs/gNzdBrKe5SLZ1HTDofohJDteLeRgN+oS+2O2BCsOUNtaINi0umUqopcH02MzNZnBIxDxpzg7QC7pC2aNjnqrU2Fo6am2bD3Y377s1YTSOiPegK0e6Es9vf1uYNM1N0ojKgHEEnxM84SbaLqha9JidLCrB9WrDXIZyGu9Bi9lKiYlCTX0iubdhaiwmccq2IGOtpqjDUiqSMHmbyw8B8P5Dnig89/Sbjt8LUKXdkvChzHNltK7u58NX4wOXdV2xrYOM6LrZneISqM10v+F0PueOYzYY2F8W3PFhpAidfG9mgfT6UamK9gn3NBcqMCx5cJBUhlUwNntL3aAwMSZmrIl1gd3XJB9/9Fq+++y02r18gFzvqbsPqbIvrollz30+4n/2Ahz/9Ce++/wn56z270hG0Y56VYZzbHCxRxGYXJWXylHCBZkXR9BfZ4jODczy7uODXvvucb35r4OLyDhjRmi1+MRo1t9QFPrNZiQtK7BPdOhNLIQNCoJYNadwxPAh3d5nHIXNzd+TLzzJ9HzjbPuPyfEPJjsOxUvJA1RlNR/I040KjFquyO4uw2lKOE9u1w1eQ6Nj1a858T8jK4909Q3WMpSdry/DVjOSKKwpJyQ2hmbPFbI4TzCmg2lFIpOqZijAVx0SgiMe5XYMRf/HtV6K4C7Dqt23FDnjx5JxJbjbBju/YbM4I3tH3kb4z2bl3jnE44scDDB60CXmAdeeJ2xV+qnSPM64Iw8MjejgwHQfSOFGm2eIVG9PElKE27Fu46a66U/fuadh6adv0XIx10i7wUitF6yLRQUWaI8GTS6LyRBMUKSf/w0KhaKE0i95SvS0C1Zl4pxaDHprNKzlTydSaqXmiThM1z2h6svo1WKX51oTFhdE6YnNvDHhXCbESg/nRLO/VeRsQiWuL7rJSFdqg2DV75Sa7L4q0wl5CQYp9bovi0xaIp+J++n6Dt2y3YaErQsFLMZ6/g8XO1yaHPca16xohxug6NZvyN9eZqhNFRlQGhAnHjLqluHsr7ry3AzmxaWz3RNMq0Hz00zwjwVZ11WrWxtVoj4u3/ULvzLlQtHJUh9MOL66Zv1UkTciQkWFA95D6CV2tGN0jXxXHynuunnW8nALPHgoXNzPPB6Hc7zhbX9Ctdkj0dL1js+khdcx1QEsyO2I1z/oF/tPS5jxFqdnhi0dKB1mQrHiMESJdwIdI7la4Vc8cAgNC2qyJuzMuX73kg29/zIff/jZnr15QusgoUGKH92dMY+bLd++4+cEPOP+H/xB/t2czmWFcnh372hHjjrAK1FxJY8arMafKbL5LVSx0R7MNvmvOlLmwWXWc7854/eo1ff9TyOalo1g2smhBp4l5hHEyCmKaJ+aUmEn0556rbc889UzjhpR7rr9Wps9H7h/veXd34O6h4vuZDz48QHlAi2Hx2y6RXSLNA9N8hDojxdTjAK7r6LyHVUdOM3OBbfCstzaYTfsjh3zk6Dak/gxCQXUyr6Q8UHNCZ0WTXUPqsfoVV+BXVDrmOjLjyC6Qo6B9j8QOjWfW8PyS269EcQdhvd40X/CAd96wMh+IIbffsPDsvovEaFvoqhX1EI89/hgBxbdEoUCP1ErM0I0VR8Rd35Bub5nuHkgPMGkmVbXi3hQ4JxtXrACdoBk1sF5UyCWTSibn2eL/2va8LMwRkXZBY/8WOQleTm9GGiXQKUWq8dybR/3pa4uF0+ZrflIM1YIWQWui5AbJTDZgqzmf2CchetbrFefnO87PtmzWK7oYCL6pS/X9gWbrshunX5rkegnFXjL/Tj5cenojp7s2eCbnVtyzsWdo1K/l8a3rXxY4adBB6/5zsWLrDEP3gv296NNzScuKPT11Pe0inlBIPVFOVRaOu93t/S3LN++9r0XEZqZnFtZh+gNXnRUfrfjgUA34JlwrJVuASzUxWnWeGjpC6Og2Du8yypZcH+FhRA6Z/PjA7PaMXqmbDr/dsFpveIwde3Ec5kpaZfxYcI8PPOzvCd0Z2/WKs1UkeqXbrkBn0qAtQKaN66s062QbptakSPG4Ku04OoIPbDYr+m3Er3e47Tnu/Blzt+EuKykVNhcXvP7Od/jwO9/m4sVzpIukECBGui5SxXN7u+cnP/uU7/3ghxx++H1+78vvc4mC65nVc1CYSrQiqVumIbG/OzB1c7P/KCcRcS6VeZrxgmUupMoshVrBuw5xG5TeICUtlCX46AglR9RvkD62WUumW2d252ucbHm8c/z4R4/8+PuPfPmFMAwJF/aETuk3K569dJztKiXf8Hh/h5OC1ISWmTRP5DTbLOs9au8wZmYcUTz7Y2LjPJuzDa6DnCf2857HKTGteiRcAjO1eopTaqhI9ThR8KCpkFOhqrAOKwg9RRxZoAazr9aklKRUcfhoGcq/7ParUdwFo0S9R49DHbUuqUOO4D0hWPanD84651oIncP3Ht9bdigB2645wUmgf37Gtnji+gz96oz56xVzHxm8ksiUVEzFKtJqhnXxWSFinG+n9rUgOFEylUQli9qBwhaC6g2nlWamhW+QhnMs2h4TMj2xNaT5xEeNBDpcF43Lvgr4PrSv3u5d25kYL9NsFXKizok6z9Q0o5obZdLR9x3n5zueP7/i+Ysrzs/PWK9XxGgJMcUGBfZZsihS22sWmynU0pgiClJtKKzvF3la/J4zo55aKikrkgTX7H9do2JKe+8nm4Dl71ruq3Xu2a5YtzA7wElEXLMwXox8hOYEJ4hEHJFARKp5aENEJCA+t1hCC/72LuB9G5q2QapW10JCGpG1ZsPUs2vKZfPQXtxFvQTwZjVctVJTorRZh4gQuxXr0BusV22XU7ueuiqMYyVnA4KiDwgQ5kBQhwyJQ5kZUuHNlPkhlc2ZsNk5Xs/K86ng94XzsuZy1bPrHX04w6069DjAOFOOM2WY2Aboggm6Ui64GMlir1ec4+z8Ere7Qi6e0V+cES529C8uuK8T+XBHjMo3//Kv8fqbH7LarSgUKImVV4JT6vGRw2dfcfOn3+f++z+i++otm+GAqxOjahtuB3x2xGrHAHXMU+bwOBK6RAh2TXchQHXkVJiGQhBT/ZYMXgrzmJhnpdYzVC7BB4PfspKzJ9WOXLakFJmnzJwrIcLmzLO+MBwf7wifzXz15sCf/HHieICPP1rxrW9d8PJ8xYsXme1ZppY7xmlGtOJFkVooc2mh6q0Rcda05dlICEEc0z6hnWfeBpJOlDozlD1jrSQ1haxdMzNVbP7kQoN5xXZaFCiq5KqM88wwHamScZ2d6qoFIrYTc6cO6xfefiWKu1EDi3VTrXN0rhpMIE1d2YIonLNtu0hBXEZCxUcl9DaARbIxCpwVWX+xZr29ZP3qQ8rljrSJTNGxl0IpM3mYSClTtCXzeKVQifWpuBvvnOY2opSlqPuW7KSy1GtTNIaACxbj53zANS40i6nVqXu1fwsOrx5fA7Hr6NYr4rrDdQHXB+KmJ2w6wqprBmHGgqhUczIsiZqzDVOl4pwNUFerjrOzHS9ePOf582ecn29Yr3piqOTintKqitHovKvmtNj8yVXNprfkcqKELlyjNlv9uQUKDJrSDNLqs/NqFgr+PSx7gWnkvTuGt+fciisz6uZ2cnTGTXaZxQnzJJAS33RjESQiJYIGVIMVdxcQ1wJZnLewY7GfqZrf+6ImKLUxdjBYqxTb0ThnOyutpR0zE0uVWijNliBn444H36yeS2oxkBXX4vo0OrSLlGQMKecDnYtmNZGUepwZp4l5ap0ixr6JPvDxPPPhwyM6Tlztdnzkn/Ph+SUXfo1znjQbG0a84FYdo5oxWnCB2gVc6IjrNRKCXRfbc8rqQ8rlK/TqnHLWM247jmXPw/zITXpAH96g04pnu3NW0bPdRIJz1McDt198ws/+4I/48e//Cfuvr+lxnPcrJq1MpeJViE4IXYfUQI0tBrNATQLe4WOkDz1dFynZwsDzbA1QSZYzHZ0yDon9Y2Y8npF7UBmpbqaIUl2Hhg0P95mvv7zl+vqafjXx+hueTR94mPZsXGV9ccF3/tIL/vzP4AffuyVPng9eXvHtj67YnAkXZ++IqwNpnqhNJW75DUrxdsqBw/mIEklZCc5ZKtOcKFWZc2I/HNiuhZUrSJ/x68pYM/Nhwkm2QCC1c7zkycRIxXbSMRhceDzuub7J6OqM7VUgxkRyE74mNs7omDmNLMEyv+j2K1HcFaWW2bpdZz7kprA0GlQIZoDlTquVcawtF7XYBDu0IoUJXwxe8bjVhthdsnq+YhuFHIW590ydI3We+eGRvD+i00xSYcKi3rIouShBKt4pXhWPNGuAxbTLn9ZO12h9zntCjPhoHaIPwbDrVshZvgKnAo8xZ1CPix1u1SF9h19FQh8J646w7vCLcMlhcE017L/mjC53X/DeaIexsxDs3dmO3W77XlZqgyka1q3UZp/QcmGhwSQNxxbfBqzhBKPIezsPadtU676N9iXJTM98Umg0UPWu1XJZ3j2tettxayuGlkrFWjfbhSWTXPuM+MVIvxmSOYMaJBi/SSSgNaIagQ5xte0ajIklLuCcUR1Rw98V11KgFgaUwWEl2wJX21zB2Da+Df3FhChzImeTggfvQQKqyjQdycn4yB5PVI/3SuwddXZNITzjg23nypxI08w0TszTSCnJCALOtB93x8RZ2vDsfMezV694/cEHvDy7oC+Qbx+AivSRmCvMieP9I1krfefwPpCcJ257uvWaKsK+VLLOdGUgz4EwF9ycCbuO5/1LhtvEj37yEz75+lMun+24PN9x1vd0CPnmgfuffcbXP/gJ9+/e4lNGYs9RC4dsu7zOCavgbXGpgeo9yQnBR/puzarv6fuO2Nkge54nxqEwjUowEQGlwOGgfP75Nd/73s/4zVdweO3puh2IcjjMPN5l7m+PfPX5A19/dcdxGHj5usAKVhdC7BLV71k5xfc7Pvi48pu/DY/X8MEHwm4LISS6OBP7GS+Z6lsP0YxSXZvN1BoQOlRMXOW80TlrGQCL3juMicN4wK+h2yoXgO4rhzHhdMIz4pkQN4FPtiMwqgbeebyYorV3mVVIbNeCdpmYEzGaHkHVkyZP90+p3r8SxR2FcXgkhIDI2oJmMb6yVtv6LPyV93goRhWkGM1NZ5DcZpXGofZeccHUXzUK/ctLLteBeHXO7vULXnz9DR7fvOXhzTvG2zv0OJKPE+TSwjWUWiEUGw5Jrbha8V7wIT4JnRrTJjR/cR+tc3fenTje+v6bXW6tyBtMEaxweBuc0EFYB8K2J6573Cqa3UCjPlat1GJScq21BYOXExMmhkDwNn8w2qYV6pQEZTbVZTZeuHcmsjD4pBW0ZnPrJCOYaEqdx/kOJ405okLViqseJ9lCytuCUYpR9cUpvlq+q6uL77varNSJMWjkiVHjvD/BJLUqpeHfIhb0LZJwbrbPq2Hottjp09CWgNQOh2VMGuvJ1LWoiaGUgNZg2a2lsZ2WRujk8Gmde65KqrZbCI27nhVSqcy5UqsQfECsA6EolDyRckKTom3R8XhbbHxFpZAarEZWypyNp55yo7Fy8sqnQeWbFxf8zu/+VX7713+dDy6eEXNlur0nbdfos3NWCl2F4f4R3rylNL/wUpQ5ZXJjRVWEqRZyHejSgbMauFitWV1sufjwGR+c91zuX/KP/+Qf859+70+4/oO3BFHO+o6NeMKckf2APyb64AmbwNxmUdlDkEDnI1PwdM4Tw4rV9oL+ckVIkfVqa/BgZ3OyeR5JcyHNhZzAxZZ65mAalS+/uOUP/yiz0xUfvupZb2x3uX+cub0euH038u7NI4+PEy5Uvp1gcwVnLx1n5475mDjqI6Eorz4o/NY/F3n3RWW3Pp7oDM4NxGDSf23zdQpIdlTncC6QS0etPVo7aMZyuTibvTUr8KKQKWhwdJ0QOqVoQacEOuF1xMuEj8n4c2pQzKKPUe+QXjhbC2cbZb1KEGdCyHTR6pkD8iR04WnC9E+6/WoUd5Rx3NN1HTG2oGNayHNtyiN8K+6lYZuLwWzzD9HU4JqlsNO43ZXqKzUI8fkZq5eX7D54ybNvvGa6vuH60y94+8ln3H35NePNHdPtPTrNxsfNldpgiZqz8ctROu9wIeKChSSH4PAhEEMkBI8LjWmyKC+FVvS01fbG5z418M6sHmM0mMdhjeXK47c9Yd0jMUBoUEw27/GaE1qM9qilgZTOVG4xBIJrA8s2FLVOUyl1ttDwklmMs0TMMkBodM5akVJIZBCPC/a4EiOONi2uagOiYp07mp/ySHMlOQWpVG/e95axGvBV8QrqneGaYouKDdQ9xbmmfDWIRChIydAKu2pCNLEEbxhEo60oL7TGztwtZREtNQtgdU1Q5qjVU6tQWqiInop6+3+xHUXRZfhtswJjBxVSqaiIHfvYWeFXNeocZn1QtZKrJTb54s3emYzzheoq4zBSpoxm27EI0oK37fyRdr749YoX3/6If/5f+pv89q/9JXrg8O4WomNzsaMrlY14OhzT/sjm3WuG45FxnHh43DPuDxxRio4UVVzfsdp2bK527D54wQff+ZiLD54Tdh3aC+FqzewzQxm5+UfXfPbZp5z1PRerNVscXa50qM2Vgu2G1DlGAe8CmUjxHW59zvMXH/L6w4+52SnhB39Iv1qzWq8BmOeJaTaBjmBwoncm8PFiUvvHxyPf//OJm0+E3VroV+Y3M4/KeFTmUZknG3ZvtuB6ePGNyOv9GtcLlBHJhfN+5Oyq58O0Yp4OpOM9w/xI7AUoDQpuqFuBmm13Tu0ouceljjlF5uwYxso8J+Z5JqfJ/P02nm5tNgdhBSFmNFR2vSLRYBjXinugIs0xIytko1AYrNbDrod1V/Deksc6ZkIQXJxxMoF6vPsLAMvAQimrT5O6n5/aNZS34fGLSyNPnjA0/1njHldEPUWVEirZC3nlkRCN/hcdvVNi59Hg8dsVZ69eMN8/Mt8/IscRP0zIMFIOA/VwJA8DeRgt29W5Brc4uxCDt/8OwZKIliHqiYRihUdOkIc2DK8N8ZygnUM716aPUINQo4lSS5szUOopyDnnTM7J8N6xUSHn2bprZ4Mq5731nmqD0ZwqKRVKtQSfUrNJ/EMrrF5Onji1VrLmE+sidFbIxBtF01VsPlGL0SarFXDFOm4tametuJbGZMW9Bl0OKZw6WU6wifcBirc0oWrpVyJtxlKzFXWdsVO3DVaNxmQdPB7U/GFsh2fF3nx55TTVNpcAoRShFncq7ovo6smewl53iMGG5m2R1GrQVgiBGCNdiKBKSolSCki2xYXaONg2ZNZsjxmiWQvnOVGctJ1Tc+HERF6lWGRkLYXucs2r73zMB7/2LeLVjjJM6NmKtbti6wKhVGIx+4ouZeLrK/aPe25ubnl49w7dRh4fH7m+veXxcGR3fsFf/92/xm/+1u/y/OVzNuc7usstqc4cxz0zmW/92nfoz1asNj1/9Ae/T50mfC6ElJE5k0jUZDCoX0W61cqYHRIoRUjqCds14fULtt/8kCGO8KNF32C8+mmemecZEUe/6okh4LQgasUuBLu2S8mMk+Hwem8Q2jxCTu/txhQOU0U6ePYKXn0YCN0ZnRekHImidCQIpn5O1c6tgpCzxeOFSLOfXpTrATDxU3Wm0J5K5fE4cTxacddq8In4Dt9mYiqW1OUcrKLi+kKWhJaEoxIbp6AqpIoF3tOC7leO1crjY0VkssZVlSBqhR1pQr+/AH7uwOliep+Stly7T72UtJGm4e7CsjO3DkeL/U2tNnTT6im9kqMjr+KJBeJFcFuTK5/3HeurS8owUYeZOozwcEAeHqm3DwzXNxyvrxnuHhj3e+bjsVkBm2mUeAsSEO9P2/IWUWG35X00Zom9ZnnK+VTAW9ISUcy+V0BbEEIRK7LShntW1GcbPqVETjNpmsjjaBz3EBsEU09Ml5ILOZdmOyD4tvsxdgwnlpL3DpqfzeIzXwuoOkKcid1M8BGW5CrhBDs59xS5V3WJyzMMu7Rd1pLqtPDHkYDzBnYbLGNdKz6gPjTh0pLQpKhY1wszi+rAPlxvTJplIO8ioivbYcjTHltOlCWgmgeQVuXU7UuLDly+evv74IQuBnKtTM3oSRrGHmMgBo9zGI2ztmGZzifIrNbaOnPTTQQXbcfnA7nvcOpbsLozyq1CzYqW1JivSrdZc/H6OdsXz6CLqIPOnyO7NR2OOk5twO2QXOgvd9ThjOl8w3bXM93dcvx04qef3/HTTz9htdnywe/8Hr/1csf2O69BKylUcIGuW0Px9JuOV/4Dfu+v/XWeX13x5osvuHv7Fh1nVs6z8YGgQlAhOjv/D021PB5GDvd73uQj9fpLbikM2+W8Mj1ILpkpzcxpJgajOdMF6pwoOZk/SxP2hQghKFIwFlM1WKTWFg7jKqVYsb2+g599mnn+ItF351xdetYxWrBKmcm14DqlW0MUUKfMCeJoBdHF96i63gb+0uI9E8pYMoeUGFImZ8W3yDN1jlyFYSwESYhWVtHcIkOXmXWmiC1csfVxix1K8A4lIrHD9YLvAi5yMr0SMbiKUCElMzrVvyCdewi94djNVhawgyaLfChw6rwaMdYGXyZeoprvip3hNMqeCU0SMLlqgcZifyu9USx939Gf7SAVmDKMM3p7T131JAFNE/NwwE8jvnT45rboxJl7ZOvcF+rjE61PG40OQE/GZKdbg2m02vCwZKUkm5yb86AnJbsbNAGiagU9z+S5de3zTJ4myjQZHS8aZFBSJqfcirrdS7HiLlJ4MvpfuvZgpmG1zTWWBSJXaoEQJro4kZun+7J6iVv+3lOrx2nAaUZc89rB8H5pC9mynqmCcxnvW+KTGKXS1GIB9RH10QzJnDs5W4pkkJkGIGGPFpYuwC5ECXYxEGCZ0ZyUxq1d0iYM04JFApoFgdktBJzY0FSAoJWA8d+zmpDMIXTe03nBqfm61JSg2FwmNcfMhblpebR2Liw20xXFhwCdQ4qYdqFYk2J/6xGNOPEMc+J+PJCDUnvbfbrOU4aJlCpzngkIslrhETQX/G5F13scM/vjHV+lA58c7/np4Q4ZD/yHv//3iK+f869e9nzwwSvj71NBo4WXOHAx8OL1KzabFc8uL7l585YyTXTiWPlgeTKKDRhjx+wjpSoPt/d8/eXX3Fzf8vn+hp/dvaPuzGgv1YQvjlwSuSbmNONchBiI3hbRWpLpFMQKX/AOqi3Q0Ud83BG8J2Wb+yjZYNQMc4Gvvp750z89sO52yHfOiVcX0Ge03gP3rNaZdWcBM1SYJ/CDXcILqcsWS2sksxamqoylsV+0kp0gXUeIHt8HKpVhnKCMBuFuhbjr6QS8z1SfWQJnnGtzhWjCpZI7KhvUdWTJZLxh/SEhsRiTLwpogOQoU2m7x19SU///KcT/5d+EfnVG8IEQzOnPtuJ2AQsdaLDCo6Vhowptgq01IBrtwm1QjVPBqf18qgUtMz2BJRnII40qp5wwkmJJHUULKY2M48AwDQzzyFQSmdo6axugmoqzGWI1FknFOq2qzW9Zl4HwMjWgqSr1SeWqlXmGNGAUMe+JMeK0WFHpIifkOCdKminTTJ1mK+rTRBmtuNdVoBYTEpVcWoFf7tICKJooaBlkij2nD2IiWLSlK9njZK2EMJHiSHAttEOe3rPzRuUsNeDVUzVgC1LT6ip2AVZTwZp/jOBcIYRWeFsKk3iBuhT3rlE7F7aRSeytc6edA409hWsdfOO9S1jKKMLi12+ri6AG7WhCNcHy6cqyA7G7YvTPjoovM9RK1ITUQnCelVQCxRgT40hN2RZ5agtObzvKdtwtOtahRckl2VyDZWG0XUSai2kvkol4zI/OcXN7z6dffsZhGtjstrYoOHtPrge0kHJhCiZQOqSJIU3cTkd+evOGf/zDP+dPf/R9fnb9hgc1V8t/9IM/pm468lr4V/6Vf5nXr19Zy1SLdZJO8H3H2ju6rqMqxL4njyNeoQuBII7g7DnFeVIRS3TynkkrrDseHx755LPPGB4S9ayQy2Q+O7WYK2QqTKjlDrgeHwQh4kXwLhO8ZxUDZ2Lnl8iKEM6Ys2NKiVQnqlRcqtTRLq+HB+WzTxObbk8eI/prF5yvdsSwMfjRGTMmqA3+S4E0Qwqt6Pp2fklFSaQCYxGmoky1MGsmi0FpxGDutB3ghVwS41DwWnHVgjV6JxANfmukq4bAerT0SN4y5w1Tdtwd79kcFD/1rM8jrqvQt/yGwZNT4XhMlL8IrpCI0HW7xpawFBwkG14sFZsu2ku1mtmKuCgmRDEzKUc+dceumWapBlIxqqUIBOeIgLblWU1WaSdamtFxYDrumfaPDPsHDsc9wzQw59m6Gm+4swvuxGF3Tby0DE1LMUhDW4FvRgZPA7JS0VLJ2dgRc06MtTBqRhrrpus6XM74UtCuO9kPl5KtW29Yexon+zpZtGA56ym5klMmtfs8J8Pas6Wwa9D2+b0/2tC2aMKyx7D3Yna+aZ6YxhHBE3zE+9iEKQbvIJyooF5920radqWqNijGdjTYXqXRyOyzUN7Lp3XB/Npjb+hPwz+NJ28X2+nUWWYaRHvdYlF7shiEPZ1iPKlSK0LEyYRKeO/nYl27D1bc1Ub4PhvOKap0Wuk8BA9RKq7OlJKJdUZqhiaGkrzMiOQk/KotZKHOxbQAxRGkw9WAFsPYc6rMQ27CGRoVMjAeR968/Zo3777m1auXxL6nTMbCij7iQiTNM/OcuH2859NPP+XTzz7ns88/50c/+Qk/+NGP+OrNG/bDQPVmSnczPPCPvveHTL5wqCP/0r/0t/nGBx/Sx2CZBGISexc7Qhc5qwUVYTwcqCW3BC3fGgOzDXGDzWDWac0un+E3HdurM+7ynvl4R9VCqjM12fWRa276hkzKZmDXh0AXOoj2npwza4aVZlxuezWnqM9UZoQRwoSvGYIVa6pwPDo+/WRG5wOSenbdBRe7nvnYcTg4eu9x0XbwlbnNpYTkxCiqHiMESCEpZLHgH/UKQVFtkE0IuK4ndJ4QAi5nSqkcjxM1FaM8OmeDWrOuMpdqZ+iElhXzuOaQOx7Gwpd3ma0X/PMOuYysnOCigjdbluOxcr83sOGX3X41ijug0hvfWB2lOkr1zZvcqGSLP7g4zGebxUGxJ/oNfdfYHw2oX9gX6poJWamIV3NjW9wIVcnTSH48Mt0/Mt3cG2Pm7TXT23dM17ekR8PZyYmA4oMneN8waluJFzexk8qyZkrN9pxq9gWmuWkMk2LsmzQZXj5OE8dpZJgncEIIgb7r0O2But8wdcbEcM6hOZvgoxX3PE7kyYp9TZn0bEOWwjzbPc2JNKVmBxtYnClpZigGP9cmVrKiv0A2J/igFOY0I8NAKRBDR4y9dWuybF1NFezV2VSqeecg0hgpsFCEToWuzQVqrq3LX4aZHu97kI1BJg0Ssa5cbQuuaqlYbpEg2eDXHsOj0hkvn8VHX9pzKFBO3b3ZATf2lXCCZexu54iOR2MwNZFSDMG27zWBYkEWwZHVkdPMmEsr7ib4ojZ4KxXKXKlJoTibbRRrG7UIdVby3BY8XTB4w3urKre3N3zvz/6Mly9e8ur5i9PcZi4W3n3/+MBXX3zNj374Q37w/e/z/R/8kE8//5yHx0dyMaGe6zwlF5LadvJ6f88//OM/4Prhjrd31/zX/9V/ld/6jd/kbLOmFpsreBGons05uOA5rDoOhz3TPNr62XnUO0qpdF6IvieXDanMhG3Hqq45uz/n4XqgPhZSmclqhma5FtONOBOF5ZxYRU/XdwRxzM0rqhOI82xwYcX8n2pG64j4ibCqBGe7mDwBKSClY//g+XSaKcMdQQMvrxKOiemgXJ719LstXYyIuwcdGr1RTpcIAWoTLRIU33viBrrikdnjsGshxv5kfSJ1pKSRmoU6gY8F+kTsbEDsW2GvHlBHroGH7Lg5FN7dTXx5O3MePKuD0s0BXwM9oBmmY+Fxr9wP/i+O5a8asYpSDVcvxVF12Vq3iwRb6ZZhnQDOd4S4oYsV7wqLamuJb6vOXBqpFVdNZRpo2+VayMOR4faG/dfXPH71hsev3zBc35Ju78kPeyRlXC50CBI8sRmWve9FLq07PQ2L2klKXVgx5g1vExSllkxJiTRNTMPAeDxy3B84HA8AhBiYu46yXpPXK6OIeuuQaAtDSYk8TYa3z4kyJWou5A+fkXtjxaSUSVPr3Od0ktPDUmelMWOao55rsFE7ZxbubalG+6p1IOdKH60z6WJnn4M3vr/RUH3b0i6LnjvF8GlZhEoG05TydBe08eztGLvQIVhxt/zUfGJUneiw8F6uqm87PrsiRTqgt++fmgOjVlpxt+7cBqkF3KKQbtbEEsyGRZVyHFqXJeCFGo0Gq66FujTP/D5CdR4nMOXaKJY2v9BcWwShDT1xDoejFqWURBoLZapIdkTf0XU9rnrynJmniZQnrt+94+///b/H3e0tH3/0TV69eMnF2QUlZT7/7At+/MMf86Mf/Iif/PgnvLu+5nG/J6VsGowY7CoSo6GixhaJIZJK5k+/92d88eUX/PRnP+O//W/8G/ytv/7XeXH5DB+jBWkUszDYXlzgukiisp+O4APrbY9zjnyYWIv5ZnYxsj3bEki8e7jj3eMN7+6vqWJ6Cy1Pc4fQN29zp0+NEaHZeNjcyjvHehXxAYpGJHS42RbLyWz68AHWAeiEMkXquCWULfM48enP9oz7PS+eFa4uE1cX8OzsghifEZxQXGqwH82ZdPGlNaMw520H1btI9QEXDSf39PSuow8e1cw0mXevL8Ww9uhItfA4KWtX6IKp3W0Hb881TYXrx5GvbwpfXw88jAlJgUMJDNqzUdPaaCkMIxxnYa6x1cVffPvVKO4KY9t+Be9N1ouY34twcleU95Sd0IoPzjp7adTHdjFZB2fYllehE6EXsxRwxTjraRwZbu/Zv7vm8c1b9m/fcXx3Q3p4pA7GjQ0K0Tk65+m9pzuJbuBEv6zS1KJmB6ClFR811NY1ho5vrA/Xulay+VmXaUbmGT8lQHG5QDMSGseJEgKzNw74IlYy17zZhqgpW6hBMZ8TjZbSUlJlnjPTODNNiZwCtbQw8FMBX7D1trtoYdVLrixYgc8lWweetfmwWOcfgifg20CyZdU2auJC819CPVy1YIj6HmNGayv60hByRxugdohb2QPUqTlAJgtCaUVenKkIbVXoWOiyZknQgfYswRuizi5eWYq7DesFbzYW0qwNpP29tCQphTqbOKw0j/kaPdm59lmZbDw22waPsvKes/WKUm24nXOhSCFJJZNJpdFZLQkQVwOrGPBxRaCj9yv6sMJLIM+Z8Tgy7guPj/f8wR/8Pt/7s+9xtjvj4uIZu82WNCWu395wfX3DPM2s+zU4R+y6BjHZlteKqlGFa3MvneaJIpkQIg/7R/6j/+f/i5/8+Cd87+/8Hf6N/+Z/i9/4te+y224bNmwDV4kB10eSKPf3Nwxl5Or5FZdXl+weheFwxMdAHzccj/f85POf8Sff/x4Pw4HuG2s6Xdo1u8Zzk4KKx+CHrlJdpmrFSyYi9A7wEek8ntBgVasFoqGJCM13yqkje0cWxZUZ1ZFxP/CzT2fubpSPPujYdC+Ypg8YxktyUFjPuN4hnQdfmTHGjlclSMDHQOec0RgZ2TnBaSLKRBRvaQApk8pAdgOQjefRKbMUZinkoKx9ILpILZ7jIbF/rOz3A3ePIzcPhbspUUMlr816OYUtQ4Hp4cjhds94O5InZRU9/pfX9l+N4q4o++FACJ6+69HYIc2gx4n5rVdr057+pl3HBSWr3eeSSanJwdWK2NoHzmrH1gVWIkStSC6Uw5H5cc/x+pqHN285vH3HeHdP3h9xcyaqEGMkInSt2zdhhTx1kO05VGiF3XBkbYEgDivqy9ApNidDs8k1i1xNBUmFWFpHt4SJ5IKU0TjsLUBEHFgodz3ZDtSS7fdry2Yt1fjiziCKnG2BGMeJefLkFCnFgkhMjWpYb0rWVXvvUC+nxcsyZV1jutSWSGQce9c8YcwuYsmLbbCHUyvwSIPIPMFbYbcMWWmd83tmZNWGua4VWOu+lzPEOP5VzQPGcm/NQdNR25B4aQDacBUzEKN5yPBznXs7l94r5rjy9D2kvVAsbam9BkTt+Z29Luegeo/GSAwNpnId2y6abUHzqS/ZdlPTODEwWQZqzTic4czeEyUQrHThJSOlmjCtzninSHDkmtgfM8M4cH17i6hjGmdKKsTQszs7Z7vZ4vYHUqmkPFBzbdYdNnNCPKLmgS5NiJdTajBl4vs/+CG3N7f8+Mc/5b/xr/1r/M2/8df4+BsfstqsSWkycsJ2zbOXVxw/P3L7cEu36Xh5+RwZheomwnpL7yvD4w0//PQTPvnqDaHreM3GRFCtichVOdTKhCJBCBuHXwlopqZKR2Ulnk4csw/Utls0e20HLhKqJ3SRruuJMRrDyWeSmyjzQGQml8TDo5IzbNY9x/E5N3fnpLRhtRKi2xC7QiiBmgrjPBk9MlbOdx0Xu45VB8EdEHck9IWOShQltFMlO5hQRq+k1ifUoEbk81DE7HyrO2OaAl+/2/PVF3tu7kfGrIxVmQT6NegmwnZL8Wccpsz0OHL3xYSfEhsVVi6bo+Qvuf1Ti7uIfBP43wIfYLSEf19V/1cicgX8H4HvAD8F/nuqetv+5n8B/JvtSvqfqur/7Zc9h6py/fCG4AOrfkXf9XaRqu3su9iMw04wiC1ZgjDNM2MZmetEqjNJM0VzY0TakK4D1iJ0xVgPZRiZ7x4Zbu84vrtheHfLdHtP3R9xUyJmpRNH7x0RIWJormuFpra8Um3dD+8Vd21+66JWpDxLgbc7C4ekgmse6K5UokJs6LG2X6gNw1+ew1AUPdEoazW6nYESrhl7NYMsF0HFuv9hYjiOTFMkJeOu267GPuNSKqo2fY/BQfRtGG3eJurVFKONBbT4zZivjA0PragZpi3Odi02a1gWCNvxuEYiqq2406x6tZrTnatKdWDLT9fOj5YJq67h97abELWirtSGxS+7haW4Lx7w5iNjp++iK18GvgIyt6+ZE8fzdG5CmtSiEIPHNcm31nLaedixrgjFPjeXqTKCOFvgndD1Qh8DqwB9EMYgpL6c0riszZ+QWk3t6GzhrC7hZaZ689JxYj44RdWSt4oFeKOO6D0VeNjvGQ4D4ziRUrG4SGyRdpYTaeSCPpAU5mmGqq0wCuMw8Omnn7HfH/jiiy/40+/9Kf/y3/4X+Z3f/Su8fvWC7cUZ3SYiQbm9v+Hw7pHru2vOVls+8B+yurziWEa+fvM5//BP/lP+wR/9KZ+/u+EbH75ulh3OGiXnQApRDT51nSN2ER8C0nz6kWZSZ2RwpDOvJjCuONGcLkMX6WJHjAFBKMGYS9kJc61IKKivzAUej4k3N/fkXNmu9vRrx2o8wmpgmmb2h4lhNNvpq2fw0YfnONkQXCQKxM7Ru4QrA5pnUjltUAlrWK2gU04GhtlBcYKXDvyaNPc8HuHNTeWLrwu3t5WkkASzb5iEFx9sEX3OcBCGMsMknO+e4+OB8njg7nYkpX92KmQG/ueq+o9F5Az4RyLy/wD+x8B/pKr/roj8O8C/A/zbIvLPAf994K8A3wD+QxH5TdVfLKdSrXzy5feblL91QO1/3jliDHSdcWBP/i1uieOrTOPMXGaqUyQqIZogxjnPeuNZB0dfwKWZMmfmhz3D9S2HdzeMb2/Jd49wGIhTplfoxJI6O8DXim+xfMvQlGImXVVr08Q8DVNrC0g2SMaKZEDwLYsVtQLnGh4fWqF7miLYMEfbbkRb6HUDTE/kP23u8LUNatVZ6LR6OXXuoo6cKsMwMQwd07QiJ4vBW3BxxQKIS8nGjNGASDPBYhlML74rjb5ZnPm8OMGXxpJptDyRhp83Js7yGCLG9qkLOlItn3XxYddlDoK9KT3lmoKSUDUfmKJiAy+tiHvqpmvb6j9N3Vt0nob37rVVibafXWCjpcgDtKSqpcgrkLKz+EQRgwx1YRqZv1EVszBw1bz7a83ksrf3tFBFG70yRvOh2fSekiy4Yh4SaZgoRdHiUR3J1VOzUqbmyukqEmKDiXLj0YvBJeKowDBN5hfU3CjFebrO4WgNQSnUXNBcWIYfTpWgTcQm0fxwmr3t/f09f/CHf8gXX33BT376E/5rP/6b/K2/+Tf4S3/pO5xf7Nie73j1+gVzHri+fsefP/4Afb3h2dVL3j3e8/t//n3+47//D/jhzz7jmCakiYOW3GLfUrYCBQ2WquRiEwJqbbtyK3yuFoNUW/SiimuF3o6ZCwGJHm3nrZEoTIFO8LjOE1YFsjJp5mb/wJxH+tDR9Z7VWKhh5ngceHicGCaLHLx/EEqZEZfxsuJyd070PVN6QMpsuQVtB2czuAWybbXNOSQGvI8IPalsGKeOd/eJr6+Vr94Jd3eeXD1ZhUQhu4qPE8HfEMIIec/lRvnt75yx7T1lhuJnTnkEv+D2Ty3uqvol8GX796OI/BnwEfDfAf5O+7X/DfAfA/92+/7/QVUn4Cci8kPgbwF/7xc9R9XKF29/dBLEuIXzfCru0STesWurcyT4aMZYLEMrPdGyFhOn4AOxU3pRulQox4m8H5hv7xje3HB8e810c0+53yOHiVgqXZUlgpmotA7ChCvaim0pyYp7LU+CxxauYXBBG/aiLZ1T8K0+UxRXrEP11eTiVjdbR3gi69EGwY1SiTuFftjdfOcbfG1KOidtyGd0Qn2vcz8eI+O4Zp4TJVuReYJKMrU2fvbCkKn1hKtbm7tAnNLslKWZWsmpLi4DZWoxccuSWNN+V8Rh2alQm7d6aLRDg2jabkDaoLSCql2kpXhK9saialJzEROwibqfG7rThCdPMIxRNp9ET8vNisvpd1qOKgu9smFuuQYKNpjPxSAO8frzi5hbdg6KaqFKfnqu1sHZ5x3xPtDFgERHDoHOKzkImh2ShTRkjg8HDg+Gry7H05g8WOpXsYXKO4c6y3PNJZl3O4IXf+qAtekcakrUFi8pCCUnnA+svNFIS64GE8XuxKLKpfL1mzcc/sGBn336M77/g+/xt//23+Kv/tW/wocfvOLFy+coNg/46Q9/yuc/mbi4esmbu7f8vX/8D/jj73+f/TTg+kDoQ7supLlF2NDd1K2g0ZS7ONeCb5rNcrUOvtZErZ5QBfGO4rCZmwjFuwbTGnW3aiFjC28NAbcqrM4LOlcIyiGNzHU2bXLwhMlUruNcGEZlMvkDU1aKTpR6QGukvt5xvlmh84jXtcFovuCxLWmplVSVggMfTEgnEXxPzT1p7Li/hy/eFb689ry773h4EFKOpKKMaWbMiXF65OZ6RPRA7zLf/nDDR7sVly96Nt0GvxuIN/8lDlRF5DvA7wH/CfC6FX5U9UsRedV+7SPg77/3Z5+17/2SmzLqW6Sa/NotGC8GvfjscdNT4Q4+NHtMs6BtPBqCs87eDLyCUZzmHbvpnI1bkQ8DaX9kvn0g3z9SHg5wGPCjBRjGqnRFiRXr1s3erxX3eiq7ki3mjveK+1KWl5VbGiQjtYKWp8LXhp+0DEWvgkcoDe6w0Gtpg0cTOOWTzN5R2/MZL6g07xXTYjocCevYXLOeTSkzDJnhGMxOdk7kHAHXRGOY+RdLRysnlszytNoK/AKJLWwh7+3fy6Bb64KLZ/Px0mWZ862zWZ7DNczdFmnnTRFql7trnU81ry9t3i/FUUo4RRxWFTzN3VG8LQK6jFSXop5Pz/fUTjVwH+D0+pbfWV7fknxlM5WUhYKZ1gkVFwQftGH9erIDznVZXhLezW0G0Z61WdsUCQRvjIsQOrabDfQRzUInHZ1bMR1m3n11QxpnxsMEVcl5oqjgO2mfuS2Ite2qDLqz3VspRl9NpZrDac5tltMYRrLkwPo257FFC8wrR4InlYKKNpGeYxhHfvjjH/PV15/z45/8kD//89/jb/6Nv8rv/pXfZrNe8eEHr/jy83f8vf/PH3H7OHD9eM2Xbz/nfhipweOisRvcsnC3na3NooKpL0PAhdhgTvNmyctgHuPE55zMYwVvYrs2+PfVjrsUmktqWxycQnQ4OjbOQS74BplMxRoRSoaMucBiO+nqDf67fYQhJR4PDzw8Dry73vHhqy0vLrZs4tq49jWTp5lhGNgfj4yzzUf6zZr1dkfJyqhKHpXD3czNl5WvP0+8vXU8TDum2jElxzjNjLMpVq+vM3f3I6tQuTpTLmPi7Rd7zrRwtXbs+kuCi7+0qv4XLu4isgP+T8D/TFUfnpgr//lf/Sd87z+3fxCRfwv4twAuLnYUuUVaF6KumUa1YZRd5Mtw7im82vH0bxEr/AbdRLrW6Uv3jIv4kp07pxxGpsORdPdIud+jj0fkOBGmhEuVWJVYlZAbY6W0LWyxLsy1LlVyNQVBtZxPbU2jtPb2ZP5E6/wxJo0URVOx9PdccFWb/6WQaz1Z8ApmOla0kC0jybaZtHxWoW1XC1kNtw2qOHXM2EDVBEZGhyw1czx6xnFmno3GCGYsFoLYTkQ92iqQtuGp0RCXo2fQhQ12XTNNM0WpNOZCrSZ6qm3msQi33CkxyZK1liLqXMQ3f3UR69ytuLaVvSqWIeusay/eCnwtjU3V/NtdK+7LRlXV6I3vF/f/3Km4fHX/mfuyWtsbV1XyWCltl2RzA5tB+LZTEoSEQVQL57/65rbpnnY2KBTyaSBeinWlNWPGpv2GzWbFer3FB6WScP6W4VhweRHvidlLFDs/nWRqsesl+ogPESlCbRa6JSVcrURnQTCCmgI2Z8O8m62yEw/em8nZNBvo58RovVMyqmcf2e8P/P7v/wk/+fEn/NEf/j5/+1/8W/zuX/ltPvzwNb/xW/8cn37t+fwf/D5v3r1jnGZWqy0yWXBILTbDid7jS6E5VOBFrANftBwORD0uRmv2CqjzlmkhtkNeTEBrg0Xzst1VIxy4Boctu03Tj3hcVciVmgtVbP5EreYe2zIcbMGw410qTEdlTJnjkHn7rvDF1cSHL3qen3dcnq3Ybc5Blf3+yPWN4/puz5AyuJFupbgzQVdQpsDhDu7eVG6/qhzuPGnqQFfMIsxAlkQXnhTMV+eOF1eBi7PIeEi8/TKReuHZJjZdyi++/Rcq7iISscL+v1PV/3P79tci8mHr2j8E3rTvfwZ8870//xj44j/7mKr67wP/PsCHH73UJHdWsNW27q0ktg62GTy1UOUFi1iMoATXOkpPcLYdjcmc+lbHgTsKqzpQh5k0TKT9gelhTzkM6HGEcUbm3LBwkGLdubQCX7OJo3TB1ErB1WVoyKn4nbDlpcNFnjxVsglZdM7UVtylNtMg8ZQQqH20Rcx7K+5FcNkuvoUOmqWxg1BmUVKLr4vO4YJN7JdCqlSjRNZyUqvmZLYEtYbT53nynFf3HvzcTvDa2hwaRCLaDL7ce4Iu3+AJ47RrNfsCKYXSivtiLoarpkSV9z43liH50j2/f6IYXq4anzpuaLuhxbo3nIRSUgrizDnSREi2cxIi72FanDr4U2SfNLrk8ro4CaXCweYYBotVW9C9QUsEoQQHwaHBG+3QRXIQljRB55ZF0f5d2wKiKgy5nPyFpnnk8Lg3yKaH1TcrL1+tTLPw4JkfvFFbp0VxbMprCjjxRO/oQiW6FXUOzAfHdHSG29eAo0PUUylkEtUlxJk3P6pYBllTC58WcltARQTNSp4U1cxd3fPHf/QTfvD9L/j44/8vv/cv/Av81u/+Va6++Zrt91d0nyRkmKlpoopnChCLQTCrYFYhTrEkIm+f8wnFElM603UGLxVrrErwFGeJTqf9si7Tp2Zr0WpFaLRlRczaAQhhEfC1dykWIEMt+M7gtDQk5ibrd876m1pgHmAc4PZ25t27mXdvHa8uV7x4ds6zC9uBHg6Rd9cdX7913O8TU57xMRMuHeHcIbVjfBQebyr760oeBIpJNItWim/7cV9wobJbCx9+GPnWR2s+eNbhfeXh4cBcElMfmfhndIUUa9H/18Cfqeq/996P/q/A/wj4d9vX/8t73//fi8i/hw1UfwP4B7/8WZSs9zztYeUJ+oSTOlBoJxtC84I6XSTLUK5UC2ZO6plL4H5OvDtmZHxE5gIpU8eZcjhSDyMyTMg441JtJk+Cq2oFvNgK71qy/Gl4Sm0WnHIaaMopONeABRE7eV19wu1r2yIvnHSqxZHhPW61outds9W1AZ7MI3VyaMnN77lhiVqYqzKr2rDJCwRHiJ7q7XM6bSfgxOowwUw5CYeWWUU7zq3Ic6LNLVCS1VRvyFArgItZmA8WuC3OYItSjdEC1aCd8jRQtbupUUVqw+RNSGRU7GUQ+vNFWOhwbaApjZtuSVyYxkDMUx4x1o8stFGxIBfocGKMmdPuoLaCwBO7R52c/ImWrh3FcNrmOrkU9+owhkc2G+MS1EJInCC+QpRGEZVTcV8CURY3QJFKds0kTG1GkEthTs2yWcD1jhiE3m3odufE0OOcME2Zh/tHrm9uebwf0FpwQek6R/B2XpYEbhbKbOIg1ORctsuNlOY3//MXPKfF2HzlXVMwl3aeuDYMDZSi3Nzc8/Bw4P7+wE8/f0O8vOLzzz/h4f6WetjjayF00XzacXQhsulWiM6kNDcoskEiSZBs2cdOLK7SY3MhAUrwhqFjhVhb0X5SZJzKSWv6KqqFOU3kKRGdJzpnweYihC6YyVr77EqaqVLIxYq/x3qNZSmZ2mZdgOgqaZy4vbmjjyMgTFNlf5jZHytz6cD1hL6n3GfqVAmuIx2F4yGzHzNlqmgZEZ2xwJBE7BOZxNm28uKV5+U3Nrx4tePqvGPnKvkhMz8kboaJtP1nt/z9l4H/IfDHIvIH7Xv/S6yo/wci8m8CnwD/3VYQ/lMR+Q+AP8X2xf+TX8aUWY5G0fnUNC0L87KblWV7Lw7vDGNW0XaQOa3YlRa4oMZcyOo5Hgv7m8r6YcDnhoHPGaYJnWbclHGpOcqpkI2qgj/hxxaG8XQR2CDTXqCpE0+h0gtVc5kCNDrgwk3XxYK3ZErJxq7RaukrqzWx3+K7gO87GxI93jPe38M4nALBSzWedZFKERsq4T012P0pEb0teNrw+2pDUgsVrpR2b3qvBp1YkPTihPcUWq0LWYfFBsI5dwo4tiIgqGZcsWNVG6xSmmXvAuMZFa+2C8/hiCZgWhZw307JKovTltEpHU2O71EJVM2tAVBbTEpbUGp+cpB0CZEOIVHFPIuM3Bqezpt2nOzxWeba0HYtqmqGbGodpr3ORWylJ1GQ5KZYdmIOfnnh3WsbKL9/b+/p1IHa3RTVWHLYkjnbAEivJtDpOk/X96xWivegFLxz5LkQXCSIkOeB0oy5LAYik2ul1BkngSDR2FBtFgQ8HR/3BKUtBX75nVOYSjuPQzC5/TiNfPLJJ1zf3RF2F4yPR6ZxQEqxcyQGxBv80jnPJkRo0Kc26NUMOqsxdWI7v8TCXHw5DeCaiG2B/2qbebcfthqxzOCWEJthSszDROcdfYx0zozvYow4hJSEYaykKZDm3l4Pi5DP/PS1VGsLXINqkkMqpLkSvX2uVR2FHt+v6DQiocfHntGnJiCLTCkzzMqQjJ1mj21JY12s+K6yW1devu751rd2fPDhjmfPVpytA1tX0XjOHIR5f2wK8F98+y/Clvl/80/G0QH+9V/wN38X+Lv/tMd+ugmRc94fb4nWk6DHrvsnmbeINA+X2jaSUBcmiZNT01ql4Mls5pHzo0Cyg1RSpuZm0Vot8qx4ZQKyii0AjQ9dfbFB2vI6dNnqK14dnZg1gFuEL62Iaqk/B+9WNUP+hDKqeYGkYn44xEB3eU7//ILt82dsrq7wMXL86c+Yf/RjjtfXpNn4tKWaRYOow4sVHGObmGEWzZ5BmlWynKh/0qLr6pPbY3OKFGeDwSf5/1PXvPiR287J2WIFT8ym94artZrnTi4OWXY72sIt1DB5J8bXFixOT4g4V+x9LPCMNOaOutPCLicGUAu2rm0vo82eGNtlWGZqaYXd41y2ou4iTvq2sGiDdGjQ1Ps0yqez3XoGpU6ZrAUXbFelLRjkZNPjtUXnuZN4i/T0WTa3Adr6f4JrTp+1YK9VrFAVh0VIencqqKKFoR6YhiOuzZUE4eJix2a9MrtoFSiOPBX2dwfGYWRKs3nKBNtpiloxydXgjfcX3veu39N9ofcumge3MHNyNmYLtvyknLi/u6fcHwjicdrOyxBRJ83NNJubpguoq7jQNXacWdom8VRxeDF4NSBW3MVeh5ysqO35S9MaANZYOZtVeWc+Nwt7LWULPRG1MJhFL6POqLVzVqajp8yBqis7DxsNKpeZmk24ZeeusciqRhu8qqVsiTfjNIKF1hQJ4CLOe4oIpQaDd+bEMCXGtAThKIWEaEbEFvWwhourntcfXvDi9YaLs8A6KrHMdF2PrpXxEcKD/6VV9VdCoSoIQS4Mf2YxY8uQM1IKXmzz3pAPRCBrsek5kJ1dEOqEGmzItXTXgcw2TVwMT/zglLP9bWmovjNjoAS2AygFWzZM1KLSfGm08dWL4mql00AnkegNX6fhzTnbAmKQsmG1lq1oCraDFg41MdfEXDPiPdvLM/j2R+y++THrjz5mtVpz7QPzzS3HxwM5K7nRIpxakQ1iCe24xY/8qfv23ibp6iNgHiLirKCVXJjnmWEohkXGjPeZEJ5SmRaBz1LgtAVu2O3p4l+6+BNM4w2Hz1h3p8uQtTn/WVW0JTxGbQXeEyXi/RObg8XbPVREjBllg+3Q7qkNbrN9LcueuZ4KtDiH9wnnOrx29rk4mmlcy2pVaZY/i1OHnN77AhNqahm11ZkuSrTBcGKvsWI7hwUJE6HmZVHiNHDWZQB7Kux6KvyINSXlvVmAcy3EwTtUKrkMJ3gkxo6u7+lix3rd4TYWEB5dh3eRd/01ac5Mx5maW+6vCFKVTCHn+t4we4Hvno6rHbN6KuZLoV/8g5bfWbp635g3Nqg0SqUEbzMgtcfpnMcMFa156GJPLx1BMs5X5igUH3HBPNKDOHyx4XjJ+SlE5rQDLo0S654aQ99yAZQTrCa0lCsfTpRScFacS2GcEzXtIK+aN5IRBRTFu5nMQGHA6Qx1BgXnOkvGDBBDs8vGFp5SK6U6kJmq0c5fhHnODMc9x+ORca5417dBdrAmRZSCMmeh4Al9z/Z8x+7CEfVIOhzp1rDZeFbna+L0F6C4g0kZLAO8+fgtToPtmjGDpqfGagnMMEGEnrp2cUZjWgqQVGw4Os9IykjOSC3tbthvXfBsnrZ82k7upUtHnwREi2/KgvSfsI3KachqFgOwbO9LaUPNUqgKzpsJWRQhXJ5z/tFHnH/3u5y/fMl6tzttRc0HRHHi6VqYiVdM5NEWuFqVOmfDgVNpFMN2aJslQAhm0atqPOlpKoTBilrXVbrOYCfXTnzev+BpEEbrNFWXQmB/I64JdarFD/riqdU1+b39nuqiE1gEUULXgWhAGg9YSsWEP/60iosoTpq5l7xX3NWj7UKzzr1BaCweMdVEVrWzQJBQmof/coYp2oJeqmLDvXa+NcQNbcN7zdqGxEbPRCxAhAU6WdIdnNrrd4K49wJEFh+U0+/a97UV93aCP/nr1EWkBeod1QtZMsbJscU0Z9vqDxwRTAuy7la4zp5is+148eqM6JWH2yPjY2KaJrMRBnxYBv+/uGtfwtKX76eUTpDN8rOlixYRYoC16wyKa3OgRKGLPR9cXfHxh9+gppk6JWOjiSBe6IKneA/eUXwgxp5+3dPH2DzaZ47lSE2jeSm165OWOetaKLcPBjeZj6ixzaZpxovjfLdj1feY7EvogullpnEyhXax+dcisDqd4+06dOII4gkSCE4JC/zTFhBUT8Ks4M0QLudEmTNFKqkWxqMxj0Rry9uV5oD7nq1JXRYITyWC66zRdBW3ElwoiK/4mpHwXwJb5r+K24J5ngZZCz6DNDqyqf+Wi27xdQFTKrqlMzp1Xe1xqlBzocxqXUUz9ToNsbSyKExV21auwSqiT54tiwjpJEpqJ8HS6QvuSTuDqS/NQsA4uyXl5syYzTUvdMS+o1uv2bx6wfm3vs35d3+d1aqnmybmu3v0sRmYpUp0jtD1eIwbr7UwzRPUTMrlSbSREk56loALcVYkgrch1XJBplQZhraktc/beUG19a9yWrqejsd7t1Pgd/td7x1aFyvkZcC5FIB6GuTWoi0wS6jV4aWzFK6c8VWJih34BjFZgS8YZzyfjoAubCmqde/V6KFVE0oBMX+boKX9dwWxpCiR0BZrOQ1ul8XLsF157z9YvMaoYu6OSMPNnTPLmgU/FztfxTnjdbfzVuTpHK7uCZaxfmAZutpCsfjqa2MUVac4p6jMKCNLFKFztkDVtjvyPnDseqJ/gAp9WLFaRbpXl2xWHQ/9nsNjJo0NMkQtBH5ZoFuRf7/Yn9hfbVc2zzOllFOYzPs/A7t+O+dIiqWeeWvX4rrnWx99k+9+/C3e3H6f2rjmNnuCKg4JTT8RIqt+zWa9Yd11MGfmIiQ3P3koVUtQWjJvA47og1FBnYdlgcLox9EHNusNXYzkXJrVQk/fdVCFGAf+f+2dz4tkVxXHP+fdV139Y7qZyUwYYzKYDgRhdgYVf4ALE1BRTJZZZKHgUgm6kORPUFy4EyQoA/5CBlfZqKDrYEwQDEk0zohJzJDO6Mwk0931493j4pz76nV30ZYQp7qrz6cp+r1Xr6pefeu98849995zmjSmYtu8+vY6sXH1mkdUWL3hVKm1gIrjKNY3VGXLJpqoSGoDMpphw3A0Zqy7jPKQ4Y7SDEFyRZLKZiE3yWYa130qsbBlSmOapsft25lbt3apGXOqP2StL7AETTVGmyF6XApkQ7HJk1EM7TaZGO02G5xWFi/fk4FQPDYstH62x3obr2dpQyq1veNa022fYc8226zsU67byaPj9fh7ZDJVLqMtStN7UnKv8bS6ubGYc0qJ/tIya6trnNrYYOP0GdbPnqWHwu6A4fYO7A6sXJdmkliCqdo9j9zY+PTRWOwmkm1yj5b4ePuoOh6aG2zXxJraPtRUcaMyzRuYTHQujgodGffvO3FNy2u0/cysZtxLB28ux5B13/t1Ffex72XZmmn+XSbFMDR7xknxCzvTZpDseqSTGr3aduCr/87dm5ifij7qqSSTpo2jW0jGh9RRnA3zAswzLaeke/OUBokPBfXZrCV0Y+GbbidruXFkSBYezFlBxtQ+OqqkntasDHYHjBgiKoyrcVs1a3lliY0L5xFNbN/e5frWDa5v7Vh+I+/w3e/BdztY69rMxGhkpe+y96O0YTh/niZbwRtVxmJDRSVVDEcj0ExPJo5DShWNh0dHUtHUNaI2AXGl32elv8xy3aNpBE096srmsDRN4+m6AWw8e10llqSm5/Vvy6DOpEK/XqJX23wKyxySvd/MWqn9/jLr62vsjG+RhztIlawztck+c9vKMUq5CQsIlc0t8GyoSWuS9hG1yYFVY5MVm8GI0QAaKruJ7YIOwOqmq03axFrjdUrUtYUql5YyuUncvLHDtd6ujao5k+n3hKaqPCxt+fkP42gY9wyjV7wiTvEGW+vhF0i56opt18lEA/XtxRGD1q7w9gDeE3j5XLmevZnpF+PkXSYhiD0Wq+O0+lScchitka/atclrJsbQY5i5pKr1zxBLplWlROor9b+ukJ6/Zo2VkaUB3j5Tsf2xBxmP7vcpNtKau7azS8vDtTi1yvl+zbnz91CaEiJKSkKvVzEYJZp/C1WybI5tR1/78E5rJga4NNHBw2A+2aSMkrFOy1J0O5HzCjn3UV1vy+tR9G5bXHZxpar2OOeIKt2gSu9S8s20yndCAyUcNPnvn0uN6hrKKq1xLIZVSvxbqWQHYQA+S7b93crpVfLKiH9Ws8pjX3hk8ruxZ+epp7N9uY4H3NlX9mz0k8XXp75b2ejHVbQT7zsoSfQmd9LyXpO/0ilbiWUAHQ5GDAZNG4/uHOSB98HPccA7MMv1KR0nwg/Bw5LeYG5z4NdVxcbqGrKSuHt5k9NnP2Chk5wtbYeYl48Xm0+9mkoT1dhCXXm1Ydwfs3nXuB0hk3P3Bli1oaI9oaSSPVSsNVXOacDzU5l2zdkxzYVxG15tb6+5E6KiRNXMwKdqEsKpOueYjavHR7V5LWWfT+5pp9xvtCu6TX4sYmUGE9Q1LPWg3xO2B8o/r8M7t6D3plUAE8+QevO9M9POmMmpM91Tu7OIyBZwG3hn3sdyxDlHaDQLodNshE6zcZR1+pCq3j3tiSNh3AFE5HlV/ei8j+MoExrNRug0G6HTbBxXnar/vksQBEFw3AjjHgRBsIAcJeP+w3kfwDEgNJqN0Gk2QqfZOJY6HZmYexAEQfD+cZQ89yAIguB9Yu7GXUQ+LyKvishrXov1xCIiF0Tk9yLysoi8JCJP+va7ROS3IvJX/3+m85qnXbtXReRz8zv6O4uIJBF5UUSe9fXQaB8iclpELovIK35OfTJ0OoiIfNOvtz+LyM9FZHkhdNo/c+9OPrC55H8DHsBK3f8JuDjPY5qzHvcAD/nyOvAX4CLwXeAp3/4U8B1fvuia9YFN1zLN+3vcIa2+BfwMeNbXQ6ODGl0CvubLS8Dp0OmARvcCV4EVX/8l8JVF0GnenvvHgddU9YqqDoFfYAW2TySq+paqvuDL7wLdYuSXfLdLwGO+/ChejFxVrwKlGPlCIyL3AV8EnulsDo06iMgG8Bms0A6qOlTVG4RO06iBFRGpgVWsctyx12nexv1e4PXO+gzFtE8GckgxcqBbjPwk6vd94NtAN3NSaLSXB4At4McevnpGRNYInfagqm8C38MKDr0F3FTV37AAOs3buE9Lp3Hih+/IvmLkh+06ZdtC6yciXwLeVtU/zvqSKdsWWiOnBh4CfqCqH8HSexzWp3UidfJY+qNYiOWDwJqIPHHYS6ZsO5I6zdu4z1RM+yQhhxQj9+f/52LkC8angS+LyN+xMN5nReQnhEb7eQN4Q1Wf8/XLmLEPnfbyCHBVVbdUdQT8CvgUC6DTvI37H4AHRWRTRJaAx7EC2ycSsdSLhxUjh4PFyB8Xkb6IbDJTMfLjjao+rar3qer92PnyO1V9gtBoD6p6DXhdRD7smx7G6hqHTnv5B/AJEVn16+9hrK/r2Os015S/qjoWka8Dv8ZGzvxIVV+a5zHNmTtQjHxhCY0O8g3gp+44XQG+ijl0oZOjqs+JyGXgBex7v4jNSD3FMdcpZqgGQRAsIPMOywRBEAT/B8K4B0EQLCBh3IMgCBaQMO5BEAQLSBj3IAiCBSSMexAEwQISxj0IgmABCeMeBEGwgPwHvNXfNzyRLeMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funtion to plot image\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[:4])\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 102 target classes in the training dataset\n"
     ]
    }
   ],
   "source": [
    "# Show how many classes there are in the training dataset\n",
    "print(f\"There are {len(class_names)} target classes in the training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of training images per class\n",
      "Class 0028: 10 images\n",
      "Class 0075: 10 images\n",
      "Class 0029: 10 images\n",
      "Class 0009: 10 images\n",
      "Class 0092: 10 images\n",
      "Class 0085: 10 images\n",
      "Class 0061: 10 images\n",
      "Class 0056: 10 images\n",
      "Class 0074: 10 images\n",
      "Class 0050: 10 images\n",
      "Class 0013: 10 images\n",
      "Class 0069: 10 images\n",
      "Class 0040: 10 images\n",
      "Class 0072: 10 images\n",
      "Class 0003: 10 images\n",
      "Class 0017: 10 images\n",
      "Class 0101: 10 images\n",
      "Class 0094: 10 images\n",
      "Class 0026: 10 images\n",
      "Class 0046: 10 images\n",
      "Class 0008: 10 images\n",
      "Class 0071: 10 images\n",
      "Class 0049: 10 images\n",
      "Class 0080: 10 images\n",
      "Class 0067: 10 images\n",
      "Class 0044: 10 images\n",
      "Class 0100: 10 images\n",
      "Class 0102: 10 images\n",
      "Class 0007: 10 images\n",
      "Class 0089: 10 images\n",
      "Class 0015: 10 images\n",
      "Class 0087: 10 images\n",
      "Class 0004: 10 images\n",
      "Class 0077: 10 images\n",
      "Class 0022: 10 images\n",
      "Class 0030: 10 images\n",
      "Class 0090: 10 images\n",
      "Class 0060: 10 images\n",
      "Class 0005: 10 images\n",
      "Class 0025: 10 images\n",
      "Class 0036: 10 images\n",
      "Class 0064: 10 images\n",
      "Class 0053: 10 images\n",
      "Class 0099: 10 images\n",
      "Class 0034: 10 images\n",
      "Class 0065: 10 images\n",
      "Class 0024: 10 images\n",
      "Class 0066: 10 images\n",
      "Class 0059: 10 images\n",
      "Class 0032: 10 images\n",
      "Class 0097: 10 images\n",
      "Class 0096: 10 images\n",
      "Class 0084: 10 images\n",
      "Class 0042: 10 images\n",
      "Class 0016: 10 images\n",
      "Class 0086: 10 images\n",
      "Class 0012: 10 images\n",
      "Class 0051: 10 images\n",
      "Class 0095: 10 images\n",
      "Class 0045: 10 images\n",
      "Class 0037: 10 images\n",
      "Class 0058: 10 images\n",
      "Class 0057: 10 images\n",
      "Class 0055: 10 images\n",
      "Class 0020: 10 images\n",
      "Class 0014: 10 images\n",
      "Class 0019: 10 images\n",
      "Class 0073: 10 images\n",
      "Class 0070: 10 images\n",
      "Class 0078: 10 images\n",
      "Class 0018: 10 images\n",
      "Class 0011: 10 images\n",
      "Class 0039: 10 images\n",
      "Class 0083: 10 images\n",
      "Class 0082: 10 images\n",
      "Class 0047: 10 images\n",
      "Class 0023: 10 images\n",
      "Class 0063: 10 images\n",
      "Class 0021: 10 images\n",
      "Class 0001: 10 images\n",
      "Class 0006: 10 images\n",
      "Class 0062: 10 images\n",
      "Class 0031: 10 images\n",
      "Class 0033: 10 images\n",
      "Class 0041: 10 images\n",
      "Class 0027: 10 images\n",
      "Class 0093: 10 images\n",
      "Class 0052: 10 images\n",
      "Class 0098: 10 images\n",
      "Class 0081: 10 images\n",
      "Class 0038: 10 images\n",
      "Class 0002: 10 images\n",
      "Class 0010: 10 images\n",
      "Class 0076: 10 images\n",
      "Class 0088: 10 images\n",
      "Class 0054: 10 images\n",
      "Class 0035: 10 images\n",
      "Class 0048: 10 images\n",
      "Class 0043: 10 images\n",
      "Class 0091: 10 images\n",
      "Class 0079: 10 images\n",
      "Class 0068: 10 images\n"
     ]
    }
   ],
   "source": [
    "# Show the distribution of training images per class\n",
    "base_dir = '/home/jupyter/data/vgg-flowers/train/'\n",
    "\n",
    "print(\"Distribution of training images per class\")\n",
    "for cn in os.listdir(base_dir):\n",
    "    cn_images = os.listdir(os.path.join(base_dir, cn))\n",
    "    print(f'Class {cn}: {len(cn_images)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained Resnet50 and change final layer\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b: (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 60 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=60, gamma=0.1)\n",
    "\n",
    "# Send model to device\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6541 Acc: 0.0176\n",
      "val Loss: 4.5981 Acc: 0.0078\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.5648 Acc: 0.0255\n",
      "val Loss: 4.4994 Acc: 0.0382\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.4600 Acc: 0.0745\n",
      "val Loss: 4.3805 Acc: 0.1206\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.3438 Acc: 0.1431\n",
      "val Loss: 4.2416 Acc: 0.2118\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.2196 Acc: 0.2392\n",
      "val Loss: 4.0840 Acc: 0.2804\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.0521 Acc: 0.3029\n",
      "val Loss: 3.8919 Acc: 0.3176\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 3.8705 Acc: 0.3490\n",
      "val Loss: 3.6627 Acc: 0.3461\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 3.6879 Acc: 0.3755\n",
      "val Loss: 3.4247 Acc: 0.3716\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 3.4827 Acc: 0.4333\n",
      "val Loss: 3.2394 Acc: 0.3961\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 3.2875 Acc: 0.4667\n",
      "val Loss: 3.0428 Acc: 0.4343\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 3.1260 Acc: 0.4824\n",
      "val Loss: 2.8849 Acc: 0.4657\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 2.9638 Acc: 0.5118\n",
      "val Loss: 2.7072 Acc: 0.4824\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 2.8026 Acc: 0.5245\n",
      "val Loss: 2.5756 Acc: 0.4961\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 2.6495 Acc: 0.5490\n",
      "val Loss: 2.4368 Acc: 0.5255\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 2.5247 Acc: 0.5716\n",
      "val Loss: 2.3351 Acc: 0.5461\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 2.3614 Acc: 0.5843\n",
      "val Loss: 2.2159 Acc: 0.5500\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 2.2587 Acc: 0.6020\n",
      "val Loss: 2.1421 Acc: 0.5657\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 2.1477 Acc: 0.6147\n",
      "val Loss: 2.0468 Acc: 0.5814\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 2.0952 Acc: 0.6412\n",
      "val Loss: 1.9515 Acc: 0.5873\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 1.9700 Acc: 0.6422\n",
      "val Loss: 1.8811 Acc: 0.6078\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 1.8772 Acc: 0.6657\n",
      "val Loss: 1.8338 Acc: 0.6167\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 1.8035 Acc: 0.6735\n",
      "val Loss: 1.7590 Acc: 0.6382\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 1.7058 Acc: 0.7098\n",
      "val Loss: 1.6986 Acc: 0.6412\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 1.6948 Acc: 0.6794\n",
      "val Loss: 1.6427 Acc: 0.6529\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 1.6028 Acc: 0.7127\n",
      "val Loss: 1.6018 Acc: 0.6529\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 1.5189 Acc: 0.7343\n",
      "val Loss: 1.5589 Acc: 0.6725\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 1.4194 Acc: 0.7520\n",
      "val Loss: 1.5115 Acc: 0.6735\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 1.3713 Acc: 0.7686\n",
      "val Loss: 1.4668 Acc: 0.6833\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 1.3496 Acc: 0.7676\n",
      "val Loss: 1.4515 Acc: 0.6794\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 1.3080 Acc: 0.7667\n",
      "val Loss: 1.3952 Acc: 0.6892\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 1.2339 Acc: 0.7980\n",
      "val Loss: 1.3636 Acc: 0.7059\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 1.2206 Acc: 0.7941\n",
      "val Loss: 1.3364 Acc: 0.7137\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 1.1355 Acc: 0.8020\n",
      "val Loss: 1.2922 Acc: 0.7108\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 1.0932 Acc: 0.8147\n",
      "val Loss: 1.2706 Acc: 0.7098\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 1.0422 Acc: 0.8206\n",
      "val Loss: 1.2498 Acc: 0.7235\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 1.0436 Acc: 0.8147\n",
      "val Loss: 1.2105 Acc: 0.7176\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.9754 Acc: 0.8235\n",
      "val Loss: 1.1930 Acc: 0.7225\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.9305 Acc: 0.8382\n",
      "val Loss: 1.1645 Acc: 0.7343\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.9124 Acc: 0.8500\n",
      "val Loss: 1.1544 Acc: 0.7265\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.8575 Acc: 0.8637\n",
      "val Loss: 1.1304 Acc: 0.7284\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.8439 Acc: 0.8608\n",
      "val Loss: 1.1032 Acc: 0.7451\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.7960 Acc: 0.8657\n",
      "val Loss: 1.0831 Acc: 0.7392\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.7621 Acc: 0.8667\n",
      "val Loss: 1.0728 Acc: 0.7402\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.7547 Acc: 0.8667\n",
      "val Loss: 1.0659 Acc: 0.7461\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.7746 Acc: 0.8608\n",
      "val Loss: 1.0398 Acc: 0.7431\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.7186 Acc: 0.8814\n",
      "val Loss: 1.0276 Acc: 0.7520\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.6804 Acc: 0.8912\n",
      "val Loss: 1.0256 Acc: 0.7549\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.6749 Acc: 0.8951\n",
      "val Loss: 1.0075 Acc: 0.7588\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.9010\n",
      "val Loss: 0.9978 Acc: 0.7598\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.5780 Acc: 0.9137\n",
      "val Loss: 0.9843 Acc: 0.7559\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.5820 Acc: 0.9069\n",
      "val Loss: 0.9725 Acc: 0.7578\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.5827 Acc: 0.9078\n",
      "val Loss: 0.9523 Acc: 0.7647\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.5590 Acc: 0.9108\n",
      "val Loss: 0.9509 Acc: 0.7676\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.5293 Acc: 0.9186\n",
      "val Loss: 0.9545 Acc: 0.7765\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.4844 Acc: 0.9265\n",
      "val Loss: 0.9485 Acc: 0.7716\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.4770 Acc: 0.9333\n",
      "val Loss: 0.9362 Acc: 0.7755\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.4722 Acc: 0.9294\n",
      "val Loss: 0.9242 Acc: 0.7676\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.4572 Acc: 0.9333\n",
      "val Loss: 0.9312 Acc: 0.7696\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.4849 Acc: 0.9225\n",
      "val Loss: 0.9192 Acc: 0.7794\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.4482 Acc: 0.9284\n",
      "val Loss: 0.9163 Acc: 0.7676\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.3796 Acc: 0.9441\n",
      "val Loss: 0.9034 Acc: 0.7735\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.4971 Acc: 0.9147\n",
      "val Loss: 0.8984 Acc: 0.7745\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.4152 Acc: 0.9314\n",
      "val Loss: 0.8944 Acc: 0.7745\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.4496 Acc: 0.9225\n",
      "val Loss: 0.8944 Acc: 0.7784\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.4160 Acc: 0.9392\n",
      "val Loss: 0.8933 Acc: 0.7794\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.4026 Acc: 0.9451\n",
      "val Loss: 0.8951 Acc: 0.7814\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.4398 Acc: 0.9255\n",
      "val Loss: 0.8954 Acc: 0.7794\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.4300 Acc: 0.9275\n",
      "val Loss: 0.8972 Acc: 0.7784\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.4258 Acc: 0.9353\n",
      "val Loss: 0.8879 Acc: 0.7824\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.3976 Acc: 0.9392\n",
      "val Loss: 0.8927 Acc: 0.7824\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.4709 Acc: 0.9245\n",
      "val Loss: 0.8938 Acc: 0.7755\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.4218 Acc: 0.9284\n",
      "val Loss: 0.8929 Acc: 0.7775\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.4025 Acc: 0.9422\n",
      "val Loss: 0.8917 Acc: 0.7775\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.4028 Acc: 0.9392\n",
      "val Loss: 0.8887 Acc: 0.7765\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.4275 Acc: 0.9275\n",
      "val Loss: 0.8872 Acc: 0.7794\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.4153 Acc: 0.9382\n",
      "val Loss: 0.8949 Acc: 0.7794\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.4073 Acc: 0.9422\n",
      "val Loss: 0.8907 Acc: 0.7784\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.4240 Acc: 0.9333\n",
      "val Loss: 0.8860 Acc: 0.7804\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.3927 Acc: 0.9431\n",
      "val Loss: 0.8872 Acc: 0.7824\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.3921 Acc: 0.9402\n",
      "val Loss: 0.8838 Acc: 0.7775\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.4153 Acc: 0.9431\n",
      "val Loss: 0.8895 Acc: 0.7765\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.4055 Acc: 0.9392\n",
      "val Loss: 0.8948 Acc: 0.7755\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.4413 Acc: 0.9304\n",
      "val Loss: 0.8856 Acc: 0.7814\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.4123 Acc: 0.9373\n",
      "val Loss: 0.8857 Acc: 0.7784\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.3753 Acc: 0.9441\n",
      "val Loss: 0.8843 Acc: 0.7794\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.3896 Acc: 0.9461\n",
      "val Loss: 0.8840 Acc: 0.7794\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.4210 Acc: 0.9284\n",
      "val Loss: 0.8810 Acc: 0.7814\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.3831 Acc: 0.9353\n",
      "val Loss: 0.8836 Acc: 0.7833\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.3808 Acc: 0.9382\n",
      "val Loss: 0.8778 Acc: 0.7794\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.3752 Acc: 0.9392\n",
      "val Loss: 0.8815 Acc: 0.7814\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.3862 Acc: 0.9333\n",
      "val Loss: 0.8774 Acc: 0.7804\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.4232 Acc: 0.9353\n",
      "val Loss: 0.8824 Acc: 0.7814\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.3772 Acc: 0.9520\n",
      "val Loss: 0.8833 Acc: 0.7853\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.3947 Acc: 0.9480\n",
      "val Loss: 0.8802 Acc: 0.7853\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.3956 Acc: 0.9392\n",
      "val Loss: 0.8815 Acc: 0.7853\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.3786 Acc: 0.9412\n",
      "val Loss: 0.8773 Acc: 0.7814\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.3352 Acc: 0.9588\n",
      "val Loss: 0.8788 Acc: 0.7833\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.3999 Acc: 0.9324\n",
      "val Loss: 0.8791 Acc: 0.7833\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.3716 Acc: 0.9441\n",
      "val Loss: 0.8754 Acc: 0.7784\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.4027 Acc: 0.9422\n",
      "val Loss: 0.8834 Acc: 0.7833\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3983 Acc: 0.9422\n",
      "val Loss: 0.8806 Acc: 0.7843\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.3782 Acc: 0.9500\n",
      "val Loss: 0.8789 Acc: 0.7863\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.3727 Acc: 0.9412\n",
      "val Loss: 0.8778 Acc: 0.7824\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.4020 Acc: 0.9333\n",
      "val Loss: 0.8732 Acc: 0.7843\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.3636 Acc: 0.9441\n",
      "val Loss: 0.8723 Acc: 0.7824\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.3916 Acc: 0.9363\n",
      "val Loss: 0.8758 Acc: 0.7853\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.3610 Acc: 0.9461\n",
      "val Loss: 0.8763 Acc: 0.7863\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.3908 Acc: 0.9392\n",
      "val Loss: 0.8772 Acc: 0.7853\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.3130 Acc: 0.9637\n",
      "val Loss: 0.8738 Acc: 0.7863\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.3582 Acc: 0.9500\n",
      "val Loss: 0.8732 Acc: 0.7824\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.3805 Acc: 0.9451\n",
      "val Loss: 0.8717 Acc: 0.7824\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.3392 Acc: 0.9520\n",
      "val Loss: 0.8752 Acc: 0.7824\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.3575 Acc: 0.9539\n",
      "val Loss: 0.8737 Acc: 0.7853\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.3931 Acc: 0.9480\n",
      "val Loss: 0.8701 Acc: 0.7873\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.3617 Acc: 0.9520\n",
      "val Loss: 0.8635 Acc: 0.7902\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.3830 Acc: 0.9382\n",
      "val Loss: 0.8700 Acc: 0.7833\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.3754 Acc: 0.9402\n",
      "val Loss: 0.8752 Acc: 0.7804\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.3751 Acc: 0.9451\n",
      "val Loss: 0.8709 Acc: 0.7853\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.3805 Acc: 0.9343\n",
      "val Loss: 0.8704 Acc: 0.7843\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.3530 Acc: 0.9461\n",
      "val Loss: 0.8731 Acc: 0.7824\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.3602 Acc: 0.9451\n",
      "val Loss: 0.8712 Acc: 0.7833\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.3511 Acc: 0.9471\n",
      "val Loss: 0.8713 Acc: 0.7804\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.3562 Acc: 0.9471\n",
      "val Loss: 0.8718 Acc: 0.7814\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.3638 Acc: 0.9402\n",
      "val Loss: 0.8775 Acc: 0.7814\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.3572 Acc: 0.9431\n",
      "val Loss: 0.8779 Acc: 0.7804\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.3697 Acc: 0.9441\n",
      "val Loss: 0.8729 Acc: 0.7843\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.3657 Acc: 0.9480\n",
      "val Loss: 0.8692 Acc: 0.7833\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.3733 Acc: 0.9353\n",
      "val Loss: 0.8691 Acc: 0.7824\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.3652 Acc: 0.9412\n",
      "val Loss: 0.8690 Acc: 0.7863\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.3515 Acc: 0.9471\n",
      "val Loss: 0.8749 Acc: 0.7814\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.3451 Acc: 0.9500\n",
      "val Loss: 0.8700 Acc: 0.7873\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.3344 Acc: 0.9539\n",
      "val Loss: 0.8692 Acc: 0.7873\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.3602 Acc: 0.9480\n",
      "val Loss: 0.8712 Acc: 0.7824\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.3704 Acc: 0.9363\n",
      "val Loss: 0.8688 Acc: 0.7814\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.3606 Acc: 0.9510\n",
      "val Loss: 0.8730 Acc: 0.7804\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.3951 Acc: 0.9451\n",
      "val Loss: 0.8714 Acc: 0.7824\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.3684 Acc: 0.9471\n",
      "val Loss: 0.8764 Acc: 0.7824\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.3901 Acc: 0.9431\n",
      "val Loss: 0.8732 Acc: 0.7833\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.3477 Acc: 0.9490\n",
      "val Loss: 0.8636 Acc: 0.7853\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.3872 Acc: 0.9353\n",
      "val Loss: 0.8719 Acc: 0.7843\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.3834 Acc: 0.9480\n",
      "val Loss: 0.8708 Acc: 0.7814\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.3577 Acc: 0.9441\n",
      "val Loss: 0.8663 Acc: 0.7863\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.3583 Acc: 0.9441\n",
      "val Loss: 0.8655 Acc: 0.7833\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.3181 Acc: 0.9588\n",
      "val Loss: 0.8706 Acc: 0.7843\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.3530 Acc: 0.9471\n",
      "val Loss: 0.8654 Acc: 0.7882\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.3416 Acc: 0.9480\n",
      "val Loss: 0.8709 Acc: 0.7843\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.3323 Acc: 0.9539\n",
      "val Loss: 0.8729 Acc: 0.7824\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.3580 Acc: 0.9451\n",
      "val Loss: 0.8692 Acc: 0.7824\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.3629 Acc: 0.9422\n",
      "val Loss: 0.8713 Acc: 0.7833\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.3791 Acc: 0.9373\n",
      "val Loss: 0.8647 Acc: 0.7863\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.3728 Acc: 0.9402\n",
      "val Loss: 0.8707 Acc: 0.7853\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.3655 Acc: 0.9373\n",
      "val Loss: 0.8676 Acc: 0.7882\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.3244 Acc: 0.9608\n",
      "val Loss: 0.8733 Acc: 0.7843\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.3358 Acc: 0.9500\n",
      "val Loss: 0.8724 Acc: 0.7833\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.3680 Acc: 0.9441\n",
      "val Loss: 0.8693 Acc: 0.7863\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.3452 Acc: 0.9451\n",
      "val Loss: 0.8705 Acc: 0.7843\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.3508 Acc: 0.9490\n",
      "val Loss: 0.8687 Acc: 0.7824\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.3758 Acc: 0.9382\n",
      "val Loss: 0.8700 Acc: 0.7824\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.3674 Acc: 0.9422\n",
      "val Loss: 0.8653 Acc: 0.7882\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.3655 Acc: 0.9412\n",
      "val Loss: 0.8678 Acc: 0.7863\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.3614 Acc: 0.9392\n",
      "val Loss: 0.8713 Acc: 0.7804\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.3456 Acc: 0.9549\n",
      "val Loss: 0.8687 Acc: 0.7824\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.3619 Acc: 0.9480\n",
      "val Loss: 0.8649 Acc: 0.7873\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.3915 Acc: 0.9373\n",
      "val Loss: 0.8707 Acc: 0.7853\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.3846 Acc: 0.9412\n",
      "val Loss: 0.8699 Acc: 0.7833\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.3157 Acc: 0.9510\n",
      "val Loss: 0.8690 Acc: 0.7833\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.3545 Acc: 0.9402\n",
      "val Loss: 0.8679 Acc: 0.7804\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.3252 Acc: 0.9647\n",
      "val Loss: 0.8688 Acc: 0.7824\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.3591 Acc: 0.9441\n",
      "val Loss: 0.8679 Acc: 0.7824\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.3605 Acc: 0.9392\n",
      "val Loss: 0.8650 Acc: 0.7873\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.3372 Acc: 0.9461\n",
      "val Loss: 0.8636 Acc: 0.7863\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.3760 Acc: 0.9382\n",
      "val Loss: 0.8746 Acc: 0.7843\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.3689 Acc: 0.9451\n",
      "val Loss: 0.8690 Acc: 0.7863\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.3481 Acc: 0.9549\n",
      "val Loss: 0.8646 Acc: 0.7814\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.3442 Acc: 0.9431\n",
      "val Loss: 0.8659 Acc: 0.7843\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.3467 Acc: 0.9441\n",
      "val Loss: 0.8668 Acc: 0.7833\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.3649 Acc: 0.9461\n",
      "val Loss: 0.8642 Acc: 0.7824\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.3585 Acc: 0.9490\n",
      "val Loss: 0.8667 Acc: 0.7843\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.3664 Acc: 0.9333\n",
      "val Loss: 0.8657 Acc: 0.7863\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.3279 Acc: 0.9559\n",
      "val Loss: 0.8641 Acc: 0.7882\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.3518 Acc: 0.9431\n",
      "val Loss: 0.8663 Acc: 0.7863\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.3725 Acc: 0.9382\n",
      "val Loss: 0.8637 Acc: 0.7814\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.3285 Acc: 0.9500\n",
      "val Loss: 0.8647 Acc: 0.7833\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.3462 Acc: 0.9441\n",
      "val Loss: 0.8690 Acc: 0.7833\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.3492 Acc: 0.9412\n",
      "val Loss: 0.8707 Acc: 0.7863\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.3533 Acc: 0.9412\n",
      "val Loss: 0.8710 Acc: 0.7873\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.3369 Acc: 0.9559\n",
      "val Loss: 0.8672 Acc: 0.7882\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.3532 Acc: 0.9480\n",
      "val Loss: 0.8638 Acc: 0.7882\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.3737 Acc: 0.9402\n",
      "val Loss: 0.8678 Acc: 0.7863\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.3553 Acc: 0.9412\n",
      "val Loss: 0.8678 Acc: 0.7873\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.4060 Acc: 0.9343\n",
      "val Loss: 0.8679 Acc: 0.7843\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.3704 Acc: 0.9373\n",
      "val Loss: 0.8677 Acc: 0.7833\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.3501 Acc: 0.9471\n",
      "val Loss: 0.8664 Acc: 0.7863\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.3423 Acc: 0.9510\n",
      "val Loss: 0.8650 Acc: 0.7814\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.3398 Acc: 0.9480\n",
      "val Loss: 0.8728 Acc: 0.7853\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.3722 Acc: 0.9451\n",
      "val Loss: 0.8670 Acc: 0.7824\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.3566 Acc: 0.9471\n",
      "val Loss: 0.8700 Acc: 0.7863\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.3802 Acc: 0.9422\n",
      "val Loss: 0.8691 Acc: 0.7873\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.3812 Acc: 0.9343\n",
      "val Loss: 0.8687 Acc: 0.7853\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.3791 Acc: 0.9431\n",
      "val Loss: 0.8644 Acc: 0.7863\n",
      "\n",
      "Training complete in 48m 58s\n",
      "Best val Acc: 0.790196\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c: (6 points)\n",
    "\n",
    "Repeating the exercise above except with uniform learning rate equal to 0.1 and 0.01 and no learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Performing experiment with fixed lr=0.1 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 5.0230 Acc: 0.0245\n",
      "val Loss: 1188.2370 Acc: 0.0098\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.5461 Acc: 0.0275\n",
      "val Loss: 14788.8348 Acc: 0.0098\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.1641 Acc: 0.0363\n",
      "val Loss: 59.8500 Acc: 0.0304\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 3.9120 Acc: 0.0706\n",
      "val Loss: 4.5217 Acc: 0.0794\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 3.7342 Acc: 0.0971\n",
      "val Loss: 3.9473 Acc: 0.0922\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 3.6327 Acc: 0.1029\n",
      "val Loss: 4.1090 Acc: 0.1294\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 3.3773 Acc: 0.1343\n",
      "val Loss: 3.9952 Acc: 0.1500\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 3.3204 Acc: 0.1529\n",
      "val Loss: 3.9736 Acc: 0.1441\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 3.2241 Acc: 0.1627\n",
      "val Loss: 3.6558 Acc: 0.1667\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 3.0926 Acc: 0.2049\n",
      "val Loss: 3.5001 Acc: 0.1961\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 2.9732 Acc: 0.2314\n",
      "val Loss: 3.4149 Acc: 0.2059\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 2.9596 Acc: 0.2294\n",
      "val Loss: 3.6754 Acc: 0.2098\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 2.7680 Acc: 0.2686\n",
      "val Loss: 3.3179 Acc: 0.2471\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 2.6343 Acc: 0.3010\n",
      "val Loss: 3.5188 Acc: 0.2167\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 2.5608 Acc: 0.2980\n",
      "val Loss: 3.1286 Acc: 0.2686\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 2.4953 Acc: 0.3373\n",
      "val Loss: 3.0954 Acc: 0.2627\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 2.4081 Acc: 0.3373\n",
      "val Loss: 3.2428 Acc: 0.2725\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 2.2843 Acc: 0.3775\n",
      "val Loss: 3.2412 Acc: 0.2441\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 2.3193 Acc: 0.3647\n",
      "val Loss: 3.7029 Acc: 0.2284\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 2.2626 Acc: 0.3755\n",
      "val Loss: 3.1593 Acc: 0.2814\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 2.0970 Acc: 0.4167\n",
      "val Loss: 3.2173 Acc: 0.2824\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 2.0324 Acc: 0.4402\n",
      "val Loss: 3.0991 Acc: 0.3196\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 2.0293 Acc: 0.4422\n",
      "val Loss: 4.3548 Acc: 0.2245\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 2.1002 Acc: 0.4127\n",
      "val Loss: 3.0582 Acc: 0.3294\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 1.8523 Acc: 0.4716\n",
      "val Loss: 3.0693 Acc: 0.3284\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 1.8447 Acc: 0.4775\n",
      "val Loss: 3.2350 Acc: 0.2843\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 1.7467 Acc: 0.5039\n",
      "val Loss: 3.4250 Acc: 0.3069\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 1.6676 Acc: 0.5069\n",
      "val Loss: 3.0015 Acc: 0.3490\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 1.6556 Acc: 0.5157\n",
      "val Loss: 3.4569 Acc: 0.3078\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 1.6661 Acc: 0.5333\n",
      "val Loss: 3.0616 Acc: 0.3431\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 1.5866 Acc: 0.5451\n",
      "val Loss: 3.3183 Acc: 0.3216\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 1.5575 Acc: 0.5431\n",
      "val Loss: 3.1633 Acc: 0.3618\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 1.6078 Acc: 0.5382\n",
      "val Loss: 3.7462 Acc: 0.3118\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 1.4766 Acc: 0.5667\n",
      "val Loss: 3.3091 Acc: 0.3510\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 1.5113 Acc: 0.5676\n",
      "val Loss: 3.2379 Acc: 0.3157\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 1.3319 Acc: 0.6118\n",
      "val Loss: 3.1930 Acc: 0.3676\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 1.3080 Acc: 0.6157\n",
      "val Loss: 3.2886 Acc: 0.3412\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 1.3095 Acc: 0.6471\n",
      "val Loss: 3.2952 Acc: 0.3559\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 1.2210 Acc: 0.6422\n",
      "val Loss: 3.7023 Acc: 0.3324\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 1.3174 Acc: 0.6157\n",
      "val Loss: 3.5243 Acc: 0.3343\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 1.1687 Acc: 0.6588\n",
      "val Loss: 3.2449 Acc: 0.3775\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 1.0783 Acc: 0.6775\n",
      "val Loss: 3.3094 Acc: 0.3824\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.9749 Acc: 0.7039\n",
      "val Loss: 3.7309 Acc: 0.3735\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 1.0041 Acc: 0.6902\n",
      "val Loss: 3.2172 Acc: 0.3951\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 1.0270 Acc: 0.7059\n",
      "val Loss: 3.4982 Acc: 0.3539\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 1.0463 Acc: 0.6961\n",
      "val Loss: 3.7809 Acc: 0.3598\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.9860 Acc: 0.7029\n",
      "val Loss: 3.9250 Acc: 0.3676\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.9260 Acc: 0.7265\n",
      "val Loss: 3.3012 Acc: 0.4118\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.9080 Acc: 0.7216\n",
      "val Loss: 3.7469 Acc: 0.3608\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 1.0286 Acc: 0.6922\n",
      "val Loss: 3.6689 Acc: 0.3657\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.9216 Acc: 0.7402\n",
      "val Loss: 3.4925 Acc: 0.3804\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.9107 Acc: 0.7510\n",
      "val Loss: 3.3125 Acc: 0.4069\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.9799 Acc: 0.7069\n",
      "val Loss: 3.5656 Acc: 0.3853\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.8706 Acc: 0.7569\n",
      "val Loss: 3.7822 Acc: 0.4108\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.8376 Acc: 0.7500\n",
      "val Loss: 3.7109 Acc: 0.3608\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.8924 Acc: 0.7412\n",
      "val Loss: 3.4158 Acc: 0.3833\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.8189 Acc: 0.7696\n",
      "val Loss: 3.3871 Acc: 0.4118\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.7141 Acc: 0.7980\n",
      "val Loss: 3.8053 Acc: 0.4225\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.7728 Acc: 0.7598\n",
      "val Loss: 3.5624 Acc: 0.3990\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.6982 Acc: 0.8039\n",
      "val Loss: 3.3675 Acc: 0.4167\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.5997 Acc: 0.8265\n",
      "val Loss: 3.8289 Acc: 0.3941\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.5567 Acc: 0.8284\n",
      "val Loss: 3.6919 Acc: 0.4373\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.6194 Acc: 0.8186\n",
      "val Loss: 3.6988 Acc: 0.4137\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.6270 Acc: 0.8157\n",
      "val Loss: 3.5679 Acc: 0.4363\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.6447 Acc: 0.8039\n",
      "val Loss: 3.9813 Acc: 0.4039\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.6636 Acc: 0.8049\n",
      "val Loss: 3.6616 Acc: 0.4118\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.6518 Acc: 0.8147\n",
      "val Loss: 3.3215 Acc: 0.4275\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.5487 Acc: 0.8412\n",
      "val Loss: 3.6158 Acc: 0.4275\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.5582 Acc: 0.8353\n",
      "val Loss: 3.3843 Acc: 0.4235\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.5687 Acc: 0.8441\n",
      "val Loss: 3.9581 Acc: 0.3882\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.6293 Acc: 0.8078\n",
      "val Loss: 3.3619 Acc: 0.4284\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.5789 Acc: 0.8412\n",
      "val Loss: 3.5819 Acc: 0.4392\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.5123 Acc: 0.8559\n",
      "val Loss: 3.5726 Acc: 0.4549\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.5089 Acc: 0.8510\n",
      "val Loss: 3.5838 Acc: 0.4137\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.5519 Acc: 0.8373\n",
      "val Loss: 3.6279 Acc: 0.4137\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.4354 Acc: 0.8569\n",
      "val Loss: 3.5625 Acc: 0.4147\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.4340 Acc: 0.8833\n",
      "val Loss: 3.5110 Acc: 0.4373\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.5434 Acc: 0.8265\n",
      "val Loss: 4.1875 Acc: 0.4147\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.4542 Acc: 0.8725\n",
      "val Loss: 3.3474 Acc: 0.4559\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.5157 Acc: 0.8431\n",
      "val Loss: 3.9023 Acc: 0.4176\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.5312 Acc: 0.8461\n",
      "val Loss: 3.9444 Acc: 0.4333\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.4675 Acc: 0.8529\n",
      "val Loss: 3.3557 Acc: 0.4598\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.5608 Acc: 0.8324\n",
      "val Loss: 3.4296 Acc: 0.4461\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.5483 Acc: 0.8461\n",
      "val Loss: 3.6125 Acc: 0.4265\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.4393 Acc: 0.8706\n",
      "val Loss: 3.3813 Acc: 0.4588\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.3806 Acc: 0.9010\n",
      "val Loss: 3.5435 Acc: 0.4373\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.3945 Acc: 0.8814\n",
      "val Loss: 3.6228 Acc: 0.4441\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.4369 Acc: 0.8706\n",
      "val Loss: 4.2188 Acc: 0.4137\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.4285 Acc: 0.8863\n",
      "val Loss: 3.5323 Acc: 0.4441\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.4360 Acc: 0.8716\n",
      "val Loss: 3.8042 Acc: 0.4500\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.3855 Acc: 0.8873\n",
      "val Loss: 3.5395 Acc: 0.4255\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.4622 Acc: 0.8667\n",
      "val Loss: 3.8684 Acc: 0.4363\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.4417 Acc: 0.8716\n",
      "val Loss: 4.2997 Acc: 0.4078\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.3746 Acc: 0.8931\n",
      "val Loss: 3.4384 Acc: 0.4716\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.3983 Acc: 0.8735\n",
      "val Loss: 3.5314 Acc: 0.4686\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.4581 Acc: 0.8667\n",
      "val Loss: 3.7081 Acc: 0.4422\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.3854 Acc: 0.8853\n",
      "val Loss: 3.8074 Acc: 0.4402\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.4339 Acc: 0.8667\n",
      "val Loss: 4.2179 Acc: 0.4343\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.3202 Acc: 0.9078\n",
      "val Loss: 3.6005 Acc: 0.4667\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.3503 Acc: 0.8961\n",
      "val Loss: 3.5855 Acc: 0.4696\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3990 Acc: 0.8853\n",
      "val Loss: 3.9307 Acc: 0.4559\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.4301 Acc: 0.8725\n",
      "val Loss: 3.7411 Acc: 0.4490\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.3872 Acc: 0.8922\n",
      "val Loss: 3.5348 Acc: 0.4725\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.2956 Acc: 0.9098\n",
      "val Loss: 3.4910 Acc: 0.4696\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.3592 Acc: 0.9039\n",
      "val Loss: 3.4927 Acc: 0.4706\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.3344 Acc: 0.8961\n",
      "val Loss: 4.1697 Acc: 0.4441\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.3623 Acc: 0.8853\n",
      "val Loss: 3.8875 Acc: 0.4333\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.3554 Acc: 0.8912\n",
      "val Loss: 3.8136 Acc: 0.4500\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.3509 Acc: 0.9010\n",
      "val Loss: 3.6857 Acc: 0.4706\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.3047 Acc: 0.9098\n",
      "val Loss: 3.5070 Acc: 0.4618\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.2899 Acc: 0.9157\n",
      "val Loss: 3.5545 Acc: 0.4755\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.3471 Acc: 0.9059\n",
      "val Loss: 3.5896 Acc: 0.4569\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.3204 Acc: 0.9078\n",
      "val Loss: 3.7066 Acc: 0.4637\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.3449 Acc: 0.8961\n",
      "val Loss: 3.8260 Acc: 0.4402\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.3212 Acc: 0.8990\n",
      "val Loss: 4.0319 Acc: 0.4333\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.3338 Acc: 0.9020\n",
      "val Loss: 3.8548 Acc: 0.4480\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.2886 Acc: 0.9147\n",
      "val Loss: 3.4517 Acc: 0.4765\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.3100 Acc: 0.9088\n",
      "val Loss: 3.5141 Acc: 0.4902\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.3260 Acc: 0.9020\n",
      "val Loss: 3.4292 Acc: 0.4706\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.3064 Acc: 0.9049\n",
      "val Loss: 3.5160 Acc: 0.4608\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.2551 Acc: 0.9275\n",
      "val Loss: 3.9506 Acc: 0.4431\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.2694 Acc: 0.9235\n",
      "val Loss: 3.7601 Acc: 0.4755\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.2495 Acc: 0.9294\n",
      "val Loss: 3.6689 Acc: 0.4667\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.2858 Acc: 0.9157\n",
      "val Loss: 3.8110 Acc: 0.4412\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.3169 Acc: 0.9078\n",
      "val Loss: 3.9528 Acc: 0.4363\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.2687 Acc: 0.9186\n",
      "val Loss: 3.7609 Acc: 0.4559\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.3059 Acc: 0.9049\n",
      "val Loss: 3.6245 Acc: 0.4804\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.2397 Acc: 0.9275\n",
      "val Loss: 3.6122 Acc: 0.4941\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.2579 Acc: 0.9235\n",
      "val Loss: 3.4540 Acc: 0.4686\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.2287 Acc: 0.9343\n",
      "val Loss: 3.5715 Acc: 0.4716\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.2806 Acc: 0.9167\n",
      "val Loss: 3.7945 Acc: 0.4667\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.3025 Acc: 0.9196\n",
      "val Loss: 3.7289 Acc: 0.4676\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.2652 Acc: 0.9225\n",
      "val Loss: 3.4626 Acc: 0.4892\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.1987 Acc: 0.9441\n",
      "val Loss: 3.7625 Acc: 0.4657\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.2718 Acc: 0.9255\n",
      "val Loss: 3.7731 Acc: 0.4637\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.3175 Acc: 0.9078\n",
      "val Loss: 4.4147 Acc: 0.4196\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.2977 Acc: 0.9216\n",
      "val Loss: 3.5566 Acc: 0.4686\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.2750 Acc: 0.9176\n",
      "val Loss: 3.5984 Acc: 0.4853\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.2890 Acc: 0.9059\n",
      "val Loss: 4.4122 Acc: 0.4176\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.2852 Acc: 0.9206\n",
      "val Loss: 3.9711 Acc: 0.4539\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.2528 Acc: 0.9314\n",
      "val Loss: 3.8217 Acc: 0.4510\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.2665 Acc: 0.9216\n",
      "val Loss: 3.5571 Acc: 0.4696\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.2999 Acc: 0.9196\n",
      "val Loss: 3.6761 Acc: 0.4686\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.2892 Acc: 0.9137\n",
      "val Loss: 3.8992 Acc: 0.4627\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.3215 Acc: 0.9118\n",
      "val Loss: 3.4271 Acc: 0.4833\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.3071 Acc: 0.9108\n",
      "val Loss: 3.8100 Acc: 0.4686\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.2761 Acc: 0.9225\n",
      "val Loss: 3.6216 Acc: 0.4824\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.2677 Acc: 0.9186\n",
      "val Loss: 3.4715 Acc: 0.4804\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.2250 Acc: 0.9333\n",
      "val Loss: 3.3663 Acc: 0.5029\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.2096 Acc: 0.9392\n",
      "val Loss: 3.6006 Acc: 0.4980\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.2469 Acc: 0.9225\n",
      "val Loss: 3.8461 Acc: 0.4696\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.2543 Acc: 0.9275\n",
      "val Loss: 3.9981 Acc: 0.4647\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.2098 Acc: 0.9402\n",
      "val Loss: 3.4484 Acc: 0.4598\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.2467 Acc: 0.9294\n",
      "val Loss: 3.6363 Acc: 0.4814\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.2266 Acc: 0.9343\n",
      "val Loss: 3.6762 Acc: 0.4745\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.1717 Acc: 0.9490\n",
      "val Loss: 3.7705 Acc: 0.4647\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9304\n",
      "val Loss: 3.5517 Acc: 0.5078\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.2723 Acc: 0.9167\n",
      "val Loss: 3.6844 Acc: 0.4833\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.2045 Acc: 0.9402\n",
      "val Loss: 3.4378 Acc: 0.4912\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.2171 Acc: 0.9422\n",
      "val Loss: 3.3555 Acc: 0.4873\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.1834 Acc: 0.9490\n",
      "val Loss: 3.7366 Acc: 0.4608\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.1718 Acc: 0.9539\n",
      "val Loss: 3.5739 Acc: 0.4961\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 0.9353\n",
      "val Loss: 3.5879 Acc: 0.4892\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.1948 Acc: 0.9382\n",
      "val Loss: 3.6239 Acc: 0.4618\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.2348 Acc: 0.9392\n",
      "val Loss: 3.7025 Acc: 0.4667\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.2457 Acc: 0.9275\n",
      "val Loss: 3.5023 Acc: 0.4922\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.2557 Acc: 0.9294\n",
      "val Loss: 3.9311 Acc: 0.4588\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.2051 Acc: 0.9431\n",
      "val Loss: 3.8999 Acc: 0.4647\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.1828 Acc: 0.9471\n",
      "val Loss: 3.5215 Acc: 0.4892\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.2012 Acc: 0.9480\n",
      "val Loss: 3.5489 Acc: 0.4833\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.2145 Acc: 0.9363\n",
      "val Loss: 3.8285 Acc: 0.4608\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.2014 Acc: 0.9431\n",
      "val Loss: 3.6284 Acc: 0.4745\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.1821 Acc: 0.9490\n",
      "val Loss: 3.5700 Acc: 0.4990\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.1781 Acc: 0.9431\n",
      "val Loss: 3.6251 Acc: 0.4833\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.2133 Acc: 0.9373\n",
      "val Loss: 3.7771 Acc: 0.4745\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.2020 Acc: 0.9412\n",
      "val Loss: 3.7644 Acc: 0.4833\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.1923 Acc: 0.9392\n",
      "val Loss: 4.2887 Acc: 0.4618\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.1881 Acc: 0.9422\n",
      "val Loss: 3.5313 Acc: 0.4922\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.1588 Acc: 0.9588\n",
      "val Loss: 3.7166 Acc: 0.4843\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.1504 Acc: 0.9578\n",
      "val Loss: 3.5979 Acc: 0.4951\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.2430 Acc: 0.9225\n",
      "val Loss: 3.8116 Acc: 0.4873\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.1762 Acc: 0.9451\n",
      "val Loss: 3.6056 Acc: 0.4931\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.1454 Acc: 0.9529\n",
      "val Loss: 3.5575 Acc: 0.4824\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.1898 Acc: 0.9451\n",
      "val Loss: 3.8243 Acc: 0.4843\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.2081 Acc: 0.9441\n",
      "val Loss: 3.7471 Acc: 0.4755\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.1889 Acc: 0.9510\n",
      "val Loss: 3.5240 Acc: 0.4941\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.1833 Acc: 0.9471\n",
      "val Loss: 3.7881 Acc: 0.4853\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.1909 Acc: 0.9480\n",
      "val Loss: 3.4561 Acc: 0.5108\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.1641 Acc: 0.9569\n",
      "val Loss: 3.5970 Acc: 0.4814\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.1731 Acc: 0.9490\n",
      "val Loss: 3.5710 Acc: 0.5010\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.1249 Acc: 0.9569\n",
      "val Loss: 3.5974 Acc: 0.5029\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.1636 Acc: 0.9539\n",
      "val Loss: 4.0760 Acc: 0.4755\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.1609 Acc: 0.9529\n",
      "val Loss: 3.8222 Acc: 0.4814\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.1567 Acc: 0.9549\n",
      "val Loss: 3.6571 Acc: 0.5039\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.1684 Acc: 0.9461\n",
      "val Loss: 3.6537 Acc: 0.4853\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.1587 Acc: 0.9539\n",
      "val Loss: 3.6472 Acc: 0.4990\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.1676 Acc: 0.9510\n",
      "val Loss: 3.7562 Acc: 0.4833\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.1681 Acc: 0.9422\n",
      "val Loss: 4.1254 Acc: 0.4814\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.2026 Acc: 0.9500\n",
      "val Loss: 3.9635 Acc: 0.4843\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.2354 Acc: 0.9324\n",
      "val Loss: 4.0369 Acc: 0.4765\n",
      "\n",
      "Training complete in 48m 38s\n",
      "Best val Acc: 0.510784\n",
      "----- Performing experiment with fixed lr=0.01 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6141 Acc: 0.0235\n",
      "val Loss: 4.0886 Acc: 0.1765\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 3.7824 Acc: 0.2147\n",
      "val Loss: 2.9374 Acc: 0.2696\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 2.6746 Acc: 0.3922\n",
      "val Loss: 2.1099 Acc: 0.4647\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 1.8803 Acc: 0.5765\n",
      "val Loss: 1.6022 Acc: 0.6098\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 1.4665 Acc: 0.6578\n",
      "val Loss: 1.3419 Acc: 0.6676\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 1.0826 Acc: 0.7441\n",
      "val Loss: 1.1830 Acc: 0.6951\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.8419 Acc: 0.8029\n",
      "val Loss: 1.1081 Acc: 0.7176\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.7178 Acc: 0.8422\n",
      "val Loss: 1.0380 Acc: 0.7314\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.6117 Acc: 0.8549\n",
      "val Loss: 0.9652 Acc: 0.7559\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.5511 Acc: 0.8696\n",
      "val Loss: 0.9681 Acc: 0.7529\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.5261 Acc: 0.8765\n",
      "val Loss: 0.9258 Acc: 0.7588\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.4161 Acc: 0.9118\n",
      "val Loss: 0.9679 Acc: 0.7608\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.4081 Acc: 0.9059\n",
      "val Loss: 0.9104 Acc: 0.7745\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.3745 Acc: 0.9039\n",
      "val Loss: 0.9535 Acc: 0.7618\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.3302 Acc: 0.9196\n",
      "val Loss: 0.9443 Acc: 0.7539\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.3567 Acc: 0.9157\n",
      "val Loss: 0.8867 Acc: 0.7657\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.3149 Acc: 0.9225\n",
      "val Loss: 0.8906 Acc: 0.7745\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.3450 Acc: 0.9137\n",
      "val Loss: 0.8771 Acc: 0.7755\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.3240 Acc: 0.9265\n",
      "val Loss: 0.8948 Acc: 0.7686\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.3266 Acc: 0.9186\n",
      "val Loss: 0.9383 Acc: 0.7647\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.3156 Acc: 0.9176\n",
      "val Loss: 0.9662 Acc: 0.7716\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.3006 Acc: 0.9176\n",
      "val Loss: 0.9043 Acc: 0.7706\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.2859 Acc: 0.9324\n",
      "val Loss: 0.8514 Acc: 0.7824\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.2789 Acc: 0.9245\n",
      "val Loss: 0.9092 Acc: 0.7765\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.3171 Acc: 0.9167\n",
      "val Loss: 0.9195 Acc: 0.7765\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.2735 Acc: 0.9373\n",
      "val Loss: 0.9224 Acc: 0.7529\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.2344 Acc: 0.9402\n",
      "val Loss: 0.8775 Acc: 0.7784\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.2189 Acc: 0.9510\n",
      "val Loss: 0.9602 Acc: 0.7559\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.2384 Acc: 0.9471\n",
      "val Loss: 0.8072 Acc: 0.7902\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.2317 Acc: 0.9412\n",
      "val Loss: 0.8516 Acc: 0.7784\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.2048 Acc: 0.9480\n",
      "val Loss: 0.8448 Acc: 0.8010\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.2875 Acc: 0.9216\n",
      "val Loss: 0.8637 Acc: 0.7882\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.2833 Acc: 0.9314\n",
      "val Loss: 0.9146 Acc: 0.7725\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.2381 Acc: 0.9373\n",
      "val Loss: 0.8133 Acc: 0.8069\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.1999 Acc: 0.9431\n",
      "val Loss: 0.8251 Acc: 0.7971\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.1866 Acc: 0.9480\n",
      "val Loss: 0.8524 Acc: 0.7814\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.1974 Acc: 0.9549\n",
      "val Loss: 0.8082 Acc: 0.7941\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.1880 Acc: 0.9569\n",
      "val Loss: 0.8294 Acc: 0.8098\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.1720 Acc: 0.9618\n",
      "val Loss: 0.8209 Acc: 0.7961\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.1752 Acc: 0.9520\n",
      "val Loss: 0.8340 Acc: 0.8010\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.1586 Acc: 0.9647\n",
      "val Loss: 0.8439 Acc: 0.7922\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.1758 Acc: 0.9578\n",
      "val Loss: 0.8717 Acc: 0.7980\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.2116 Acc: 0.9471\n",
      "val Loss: 0.8801 Acc: 0.7912\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.1874 Acc: 0.9520\n",
      "val Loss: 0.8910 Acc: 0.8000\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.1792 Acc: 0.9500\n",
      "val Loss: 0.8473 Acc: 0.7853\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.1983 Acc: 0.9471\n",
      "val Loss: 0.9669 Acc: 0.7667\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.1668 Acc: 0.9559\n",
      "val Loss: 0.9344 Acc: 0.7814\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.2298 Acc: 0.9402\n",
      "val Loss: 0.8644 Acc: 0.7931\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.2054 Acc: 0.9480\n",
      "val Loss: 0.9289 Acc: 0.7863\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.1962 Acc: 0.9569\n",
      "val Loss: 0.8681 Acc: 0.7892\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.2031 Acc: 0.9412\n",
      "val Loss: 0.8934 Acc: 0.7990\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.1617 Acc: 0.9578\n",
      "val Loss: 0.9944 Acc: 0.7794\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.9471\n",
      "val Loss: 0.8817 Acc: 0.7804\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.1785 Acc: 0.9608\n",
      "val Loss: 0.8694 Acc: 0.7931\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.1642 Acc: 0.9559\n",
      "val Loss: 0.9425 Acc: 0.7931\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.1493 Acc: 0.9588\n",
      "val Loss: 0.9548 Acc: 0.7775\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.1662 Acc: 0.9588\n",
      "val Loss: 0.8801 Acc: 0.7892\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.1320 Acc: 0.9637\n",
      "val Loss: 0.8834 Acc: 0.7863\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.1572 Acc: 0.9618\n",
      "val Loss: 0.9304 Acc: 0.7676\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.1835 Acc: 0.9510\n",
      "val Loss: 0.9262 Acc: 0.7725\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.1374 Acc: 0.9608\n",
      "val Loss: 0.8971 Acc: 0.7775\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.9647\n",
      "val Loss: 0.8662 Acc: 0.7882\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.1186 Acc: 0.9725\n",
      "val Loss: 0.8623 Acc: 0.7843\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.1146 Acc: 0.9735\n",
      "val Loss: 0.8590 Acc: 0.7941\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.9676\n",
      "val Loss: 0.8933 Acc: 0.7961\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.1627 Acc: 0.9539\n",
      "val Loss: 0.9177 Acc: 0.7902\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.1556 Acc: 0.9598\n",
      "val Loss: 0.9146 Acc: 0.7922\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.1537 Acc: 0.9667\n",
      "val Loss: 0.9376 Acc: 0.7873\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.1301 Acc: 0.9676\n",
      "val Loss: 0.9291 Acc: 0.7873\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.1601 Acc: 0.9598\n",
      "val Loss: 0.9331 Acc: 0.7775\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.1210 Acc: 0.9676\n",
      "val Loss: 0.9899 Acc: 0.7755\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.1336 Acc: 0.9627\n",
      "val Loss: 0.9417 Acc: 0.7784\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.1117 Acc: 0.9716\n",
      "val Loss: 0.9783 Acc: 0.7725\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.1431 Acc: 0.9618\n",
      "val Loss: 0.8850 Acc: 0.7882\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.1384 Acc: 0.9569\n",
      "val Loss: 1.0008 Acc: 0.7676\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.1262 Acc: 0.9667\n",
      "val Loss: 0.8561 Acc: 0.7912\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.1236 Acc: 0.9627\n",
      "val Loss: 0.8898 Acc: 0.7922\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.1164 Acc: 0.9676\n",
      "val Loss: 0.9223 Acc: 0.7882\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.1591 Acc: 0.9618\n",
      "val Loss: 0.8918 Acc: 0.7951\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.1425 Acc: 0.9647\n",
      "val Loss: 0.8289 Acc: 0.8049\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.1691 Acc: 0.9569\n",
      "val Loss: 0.9093 Acc: 0.7912\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.1238 Acc: 0.9676\n",
      "val Loss: 0.9717 Acc: 0.7696\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.1345 Acc: 0.9637\n",
      "val Loss: 1.0054 Acc: 0.7833\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.1444 Acc: 0.9598\n",
      "val Loss: 0.9665 Acc: 0.7794\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.1443 Acc: 0.9608\n",
      "val Loss: 1.0379 Acc: 0.7608\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.1712 Acc: 0.9549\n",
      "val Loss: 0.9800 Acc: 0.7608\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.1073 Acc: 0.9735\n",
      "val Loss: 0.9013 Acc: 0.7814\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.1255 Acc: 0.9696\n",
      "val Loss: 0.9401 Acc: 0.7686\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.1078 Acc: 0.9706\n",
      "val Loss: 0.8994 Acc: 0.7912\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9627\n",
      "val Loss: 0.9515 Acc: 0.7882\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.1246 Acc: 0.9686\n",
      "val Loss: 0.8934 Acc: 0.7941\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.1398 Acc: 0.9637\n",
      "val Loss: 0.8691 Acc: 0.7941\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.1391 Acc: 0.9637\n",
      "val Loss: 0.9648 Acc: 0.7657\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.1604 Acc: 0.9510\n",
      "val Loss: 1.0320 Acc: 0.7814\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.1406 Acc: 0.9608\n",
      "val Loss: 0.9014 Acc: 0.7843\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.1169 Acc: 0.9765\n",
      "val Loss: 0.9729 Acc: 0.7755\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.1431 Acc: 0.9637\n",
      "val Loss: 0.9642 Acc: 0.7843\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.1278 Acc: 0.9657\n",
      "val Loss: 0.8625 Acc: 0.7990\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.0978 Acc: 0.9706\n",
      "val Loss: 0.9266 Acc: 0.7814\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.0728 Acc: 0.9824\n",
      "val Loss: 0.8339 Acc: 0.8020\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.1272 Acc: 0.9637\n",
      "val Loss: 0.9960 Acc: 0.7745\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.1395 Acc: 0.9569\n",
      "val Loss: 0.9938 Acc: 0.7686\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.1048 Acc: 0.9716\n",
      "val Loss: 0.9878 Acc: 0.7804\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.1291 Acc: 0.9618\n",
      "val Loss: 0.9714 Acc: 0.7824\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.1196 Acc: 0.9725\n",
      "val Loss: 0.9063 Acc: 0.7863\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9696\n",
      "val Loss: 0.9528 Acc: 0.7686\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.1751 Acc: 0.9490\n",
      "val Loss: 0.9366 Acc: 0.7804\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.1256 Acc: 0.9676\n",
      "val Loss: 0.9064 Acc: 0.7873\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.9725\n",
      "val Loss: 0.9171 Acc: 0.7951\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.1206 Acc: 0.9725\n",
      "val Loss: 0.9150 Acc: 0.7980\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.1572 Acc: 0.9578\n",
      "val Loss: 0.9767 Acc: 0.7922\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.1304 Acc: 0.9676\n",
      "val Loss: 0.8900 Acc: 0.7971\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.1110 Acc: 0.9716\n",
      "val Loss: 0.8716 Acc: 0.7961\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.1089 Acc: 0.9755\n",
      "val Loss: 0.9548 Acc: 0.7833\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.1220 Acc: 0.9657\n",
      "val Loss: 0.8777 Acc: 0.7990\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.9735\n",
      "val Loss: 0.9657 Acc: 0.7804\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.1293 Acc: 0.9647\n",
      "val Loss: 1.0067 Acc: 0.7755\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.1151 Acc: 0.9716\n",
      "val Loss: 0.9945 Acc: 0.7863\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.1296 Acc: 0.9667\n",
      "val Loss: 1.0354 Acc: 0.7735\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.1384 Acc: 0.9657\n",
      "val Loss: 1.0252 Acc: 0.7843\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.1077 Acc: 0.9696\n",
      "val Loss: 0.9713 Acc: 0.7833\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.1218 Acc: 0.9667\n",
      "val Loss: 0.9795 Acc: 0.7853\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.0893 Acc: 0.9804\n",
      "val Loss: 0.9458 Acc: 0.7931\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.0798 Acc: 0.9775\n",
      "val Loss: 0.9247 Acc: 0.8020\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.0975 Acc: 0.9735\n",
      "val Loss: 0.9552 Acc: 0.7882\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9588\n",
      "val Loss: 0.9323 Acc: 0.7922\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.1347 Acc: 0.9686\n",
      "val Loss: 1.0008 Acc: 0.7716\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.0888 Acc: 0.9765\n",
      "val Loss: 0.9712 Acc: 0.7755\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.1366 Acc: 0.9627\n",
      "val Loss: 1.0013 Acc: 0.7843\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.9775\n",
      "val Loss: 1.0189 Acc: 0.7824\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.0839 Acc: 0.9725\n",
      "val Loss: 0.9996 Acc: 0.7882\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.0987 Acc: 0.9716\n",
      "val Loss: 0.9832 Acc: 0.8069\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.0954 Acc: 0.9735\n",
      "val Loss: 0.9728 Acc: 0.7882\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.1013 Acc: 0.9716\n",
      "val Loss: 0.8791 Acc: 0.8059\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.0733 Acc: 0.9814\n",
      "val Loss: 1.0026 Acc: 0.7814\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.0929 Acc: 0.9765\n",
      "val Loss: 1.0331 Acc: 0.7824\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.0812 Acc: 0.9804\n",
      "val Loss: 0.9722 Acc: 0.7941\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.0829 Acc: 0.9784\n",
      "val Loss: 0.9643 Acc: 0.7980\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.1123 Acc: 0.9696\n",
      "val Loss: 1.0214 Acc: 0.7814\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.0759 Acc: 0.9853\n",
      "val Loss: 1.0148 Acc: 0.7716\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.1077 Acc: 0.9725\n",
      "val Loss: 1.0145 Acc: 0.7784\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.1210 Acc: 0.9676\n",
      "val Loss: 1.0555 Acc: 0.7814\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.0931 Acc: 0.9755\n",
      "val Loss: 0.9779 Acc: 0.7873\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.0846 Acc: 0.9735\n",
      "val Loss: 0.9873 Acc: 0.7745\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.1062 Acc: 0.9657\n",
      "val Loss: 0.9955 Acc: 0.7775\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.0942 Acc: 0.9696\n",
      "val Loss: 0.9733 Acc: 0.7873\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.0873 Acc: 0.9725\n",
      "val Loss: 0.9486 Acc: 0.7902\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.0599 Acc: 0.9833\n",
      "val Loss: 0.9336 Acc: 0.7961\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.0900 Acc: 0.9745\n",
      "val Loss: 0.9688 Acc: 0.7892\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.0816 Acc: 0.9804\n",
      "val Loss: 0.9607 Acc: 0.7941\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.1093 Acc: 0.9667\n",
      "val Loss: 0.9755 Acc: 0.7863\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.0679 Acc: 0.9843\n",
      "val Loss: 0.9485 Acc: 0.7814\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.0949 Acc: 0.9706\n",
      "val Loss: 0.9733 Acc: 0.7804\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.0726 Acc: 0.9784\n",
      "val Loss: 0.9487 Acc: 0.7971\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.0859 Acc: 0.9745\n",
      "val Loss: 0.9321 Acc: 0.8029\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.0691 Acc: 0.9804\n",
      "val Loss: 0.9304 Acc: 0.7951\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.0900 Acc: 0.9745\n",
      "val Loss: 0.9553 Acc: 0.7873\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.0553 Acc: 0.9843\n",
      "val Loss: 0.9058 Acc: 0.8088\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.0917 Acc: 0.9725\n",
      "val Loss: 0.9048 Acc: 0.7941\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.0683 Acc: 0.9843\n",
      "val Loss: 0.8867 Acc: 0.8039\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.0707 Acc: 0.9794\n",
      "val Loss: 0.9105 Acc: 0.7961\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.0883 Acc: 0.9765\n",
      "val Loss: 0.9175 Acc: 0.7971\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.0468 Acc: 0.9863\n",
      "val Loss: 0.9412 Acc: 0.7892\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.1011 Acc: 0.9725\n",
      "val Loss: 0.9208 Acc: 0.7990\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.0991 Acc: 0.9755\n",
      "val Loss: 0.9811 Acc: 0.7873\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.0680 Acc: 0.9824\n",
      "val Loss: 0.9802 Acc: 0.7784\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.0622 Acc: 0.9833\n",
      "val Loss: 0.9429 Acc: 0.7804\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.0731 Acc: 0.9765\n",
      "val Loss: 0.9833 Acc: 0.7941\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.0619 Acc: 0.9833\n",
      "val Loss: 0.9984 Acc: 0.7892\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.0819 Acc: 0.9794\n",
      "val Loss: 1.0258 Acc: 0.7873\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.1026 Acc: 0.9686\n",
      "val Loss: 1.0255 Acc: 0.7873\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.0665 Acc: 0.9784\n",
      "val Loss: 0.9574 Acc: 0.8029\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.0636 Acc: 0.9843\n",
      "val Loss: 0.9736 Acc: 0.7971\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.0970 Acc: 0.9765\n",
      "val Loss: 0.9646 Acc: 0.7873\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.0757 Acc: 0.9794\n",
      "val Loss: 0.9404 Acc: 0.7980\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 0.9755\n",
      "val Loss: 0.9562 Acc: 0.7824\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.1066 Acc: 0.9676\n",
      "val Loss: 0.9407 Acc: 0.7882\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.0920 Acc: 0.9735\n",
      "val Loss: 0.9531 Acc: 0.7922\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.1033 Acc: 0.9706\n",
      "val Loss: 0.9518 Acc: 0.7922\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.0691 Acc: 0.9814\n",
      "val Loss: 0.9145 Acc: 0.7951\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.0830 Acc: 0.9794\n",
      "val Loss: 0.9191 Acc: 0.7892\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.0732 Acc: 0.9784\n",
      "val Loss: 0.9472 Acc: 0.7843\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.0791 Acc: 0.9765\n",
      "val Loss: 0.9170 Acc: 0.8039\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.0804 Acc: 0.9784\n",
      "val Loss: 0.9542 Acc: 0.7912\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.0550 Acc: 0.9853\n",
      "val Loss: 0.9007 Acc: 0.8020\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.0665 Acc: 0.9814\n",
      "val Loss: 0.8929 Acc: 0.8059\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.0875 Acc: 0.9765\n",
      "val Loss: 0.9228 Acc: 0.7980\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.0746 Acc: 0.9843\n",
      "val Loss: 0.9304 Acc: 0.7882\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.0951 Acc: 0.9716\n",
      "val Loss: 0.9885 Acc: 0.7951\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.0584 Acc: 0.9853\n",
      "val Loss: 0.8983 Acc: 0.8010\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.9716\n",
      "val Loss: 0.9166 Acc: 0.8010\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.0669 Acc: 0.9804\n",
      "val Loss: 0.9056 Acc: 0.7990\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.0703 Acc: 0.9804\n",
      "val Loss: 0.8988 Acc: 0.8127\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.0799 Acc: 0.9804\n",
      "val Loss: 0.9285 Acc: 0.7951\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.0706 Acc: 0.9794\n",
      "val Loss: 0.9717 Acc: 0.7902\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.0513 Acc: 0.9833\n",
      "val Loss: 0.9289 Acc: 0.7951\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.0708 Acc: 0.9794\n",
      "val Loss: 0.9157 Acc: 0.8039\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.0570 Acc: 0.9843\n",
      "val Loss: 0.8530 Acc: 0.8098\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.0692 Acc: 0.9804\n",
      "val Loss: 0.8495 Acc: 0.8167\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.0560 Acc: 0.9863\n",
      "val Loss: 0.8749 Acc: 0.8069\n",
      "\n",
      "Training complete in 49m 13s\n",
      "Best val Acc: 0.816667\n"
     ]
    }
   ],
   "source": [
    "LR_SET = [0.1, 0.01]\n",
    "\n",
    "for lr in LR_SET:\n",
    "    print(f'----- Performing experiment with fixed lr={lr} -----\\n')\n",
    "    \n",
    "    # Load pretrained Resnet50 and change final layer\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "    # Set up criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 1000 epochs (never going to step, setting artifically high for fixing lr)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1000, gamma=0.1)\n",
    "\n",
    "    # Send model to device\n",
    "    model_ft = model_ft.to(device)\n",
    "    \n",
    "    # Train model\n",
    "    model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for the three learning rate approaches are below in the table.\n",
    "\n",
    "| Learning Rate Setting | Best Validation Accuracy |\n",
    "| ----------------------|--------------------------|\n",
    "| 0.01 with decay       | 0.790                    |\n",
    "| 0.1 fixed             | 0.511                    |\n",
    "| 0.01 fixed            | 0.812                    |\n",
    "\n",
    "The fixed learning rate of 0.01 for all 200 epochs gave the best validation accuracy for the three experiments.\n",
    "\n",
    "#### Q2\n",
    "\n",
    "* a: (8 points)\n",
    "\n",
    "Treating `ResNet50` as a feature extractor and only training the final layer with various learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Performing experiment with fixed lr=0.001 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6677 Acc: 0.0108\n",
      "val Loss: 4.6238 Acc: 0.0118\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.6135 Acc: 0.0118\n",
      "val Loss: 4.5593 Acc: 0.0186\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.5441 Acc: 0.0284\n",
      "val Loss: 4.5050 Acc: 0.0343\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.5042 Acc: 0.0539\n",
      "val Loss: 4.4517 Acc: 0.0686\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.4565 Acc: 0.0961\n",
      "val Loss: 4.3994 Acc: 0.1147\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.4064 Acc: 0.1167\n",
      "val Loss: 4.3486 Acc: 0.1510\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 4.3431 Acc: 0.1559\n",
      "val Loss: 4.2957 Acc: 0.1706\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 4.3102 Acc: 0.1824\n",
      "val Loss: 4.2480 Acc: 0.2294\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 4.2588 Acc: 0.2392\n",
      "val Loss: 4.2025 Acc: 0.2539\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 4.1990 Acc: 0.2588\n",
      "val Loss: 4.1507 Acc: 0.2833\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 4.1591 Acc: 0.2804\n",
      "val Loss: 4.1033 Acc: 0.3098\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 4.1218 Acc: 0.3010\n",
      "val Loss: 4.0585 Acc: 0.3265\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 4.0681 Acc: 0.3412\n",
      "val Loss: 4.0123 Acc: 0.3451\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 4.0180 Acc: 0.3588\n",
      "val Loss: 3.9624 Acc: 0.3598\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 3.9779 Acc: 0.3725\n",
      "val Loss: 3.9123 Acc: 0.3824\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 3.9351 Acc: 0.3892\n",
      "val Loss: 3.8746 Acc: 0.3706\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 3.8872 Acc: 0.3980\n",
      "val Loss: 3.8295 Acc: 0.4039\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 3.8446 Acc: 0.4382\n",
      "val Loss: 3.7874 Acc: 0.4196\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 3.8247 Acc: 0.4275\n",
      "val Loss: 3.7538 Acc: 0.4167\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 3.7757 Acc: 0.4275\n",
      "val Loss: 3.7033 Acc: 0.4382\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 3.7375 Acc: 0.4324\n",
      "val Loss: 3.6688 Acc: 0.4343\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 3.6845 Acc: 0.4824\n",
      "val Loss: 3.6240 Acc: 0.4441\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 3.6557 Acc: 0.4745\n",
      "val Loss: 3.5905 Acc: 0.4598\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 3.6232 Acc: 0.4775\n",
      "val Loss: 3.5506 Acc: 0.4735\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 3.5967 Acc: 0.4696\n",
      "val Loss: 3.5096 Acc: 0.4637\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 3.5691 Acc: 0.4814\n",
      "val Loss: 3.4731 Acc: 0.4794\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 3.4975 Acc: 0.5275\n",
      "val Loss: 3.4360 Acc: 0.4843\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 3.4836 Acc: 0.4873\n",
      "val Loss: 3.4014 Acc: 0.4931\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 3.4590 Acc: 0.4951\n",
      "val Loss: 3.3715 Acc: 0.4873\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 3.3934 Acc: 0.5176\n",
      "val Loss: 3.3295 Acc: 0.4990\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 3.3960 Acc: 0.5127\n",
      "val Loss: 3.3058 Acc: 0.5088\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 3.3343 Acc: 0.5422\n",
      "val Loss: 3.2703 Acc: 0.5118\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 3.3331 Acc: 0.5353\n",
      "val Loss: 3.2354 Acc: 0.5275\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 3.2713 Acc: 0.5539\n",
      "val Loss: 3.2003 Acc: 0.5265\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 3.2472 Acc: 0.5324\n",
      "val Loss: 3.1733 Acc: 0.5275\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 3.1928 Acc: 0.5686\n",
      "val Loss: 3.1381 Acc: 0.5225\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 3.1949 Acc: 0.5529\n",
      "val Loss: 3.1134 Acc: 0.5343\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 3.1289 Acc: 0.5667\n",
      "val Loss: 3.0809 Acc: 0.5490\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 3.1141 Acc: 0.5627\n",
      "val Loss: 3.0646 Acc: 0.5480\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 3.1082 Acc: 0.5765\n",
      "val Loss: 3.0241 Acc: 0.5343\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 3.0752 Acc: 0.5716\n",
      "val Loss: 3.0022 Acc: 0.5412\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 3.0527 Acc: 0.5510\n",
      "val Loss: 2.9729 Acc: 0.5520\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 3.0171 Acc: 0.5863\n",
      "val Loss: 2.9526 Acc: 0.5471\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 3.0141 Acc: 0.5716\n",
      "val Loss: 2.9204 Acc: 0.5627\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 2.9860 Acc: 0.5784\n",
      "val Loss: 2.8955 Acc: 0.5627\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 2.9658 Acc: 0.5745\n",
      "val Loss: 2.8748 Acc: 0.5569\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 2.9165 Acc: 0.5990\n",
      "val Loss: 2.8486 Acc: 0.5539\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 2.9020 Acc: 0.5990\n",
      "val Loss: 2.8240 Acc: 0.5657\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 2.8562 Acc: 0.6167\n",
      "val Loss: 2.8018 Acc: 0.5804\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 2.8458 Acc: 0.5941\n",
      "val Loss: 2.7685 Acc: 0.5745\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 2.8017 Acc: 0.5961\n",
      "val Loss: 2.7442 Acc: 0.5657\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 2.8294 Acc: 0.6098\n",
      "val Loss: 2.7297 Acc: 0.5843\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 2.7720 Acc: 0.6196\n",
      "val Loss: 2.7157 Acc: 0.5775\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 2.7470 Acc: 0.6069\n",
      "val Loss: 2.6819 Acc: 0.5824\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 2.7442 Acc: 0.6010\n",
      "val Loss: 2.6611 Acc: 0.5873\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 2.7372 Acc: 0.5755\n",
      "val Loss: 2.6426 Acc: 0.5892\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 2.6773 Acc: 0.6255\n",
      "val Loss: 2.6227 Acc: 0.5931\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 2.6508 Acc: 0.6235\n",
      "val Loss: 2.6092 Acc: 0.5931\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 2.6890 Acc: 0.6000\n",
      "val Loss: 2.5914 Acc: 0.5873\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 2.5921 Acc: 0.6510\n",
      "val Loss: 2.5640 Acc: 0.5961\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 2.6278 Acc: 0.6324\n",
      "val Loss: 2.5665 Acc: 0.5951\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 2.5990 Acc: 0.6324\n",
      "val Loss: 2.5645 Acc: 0.6010\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 2.6240 Acc: 0.6314\n",
      "val Loss: 2.5735 Acc: 0.5980\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 2.6335 Acc: 0.6108\n",
      "val Loss: 2.5688 Acc: 0.6020\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 2.5849 Acc: 0.6343\n",
      "val Loss: 2.5667 Acc: 0.5990\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 2.6190 Acc: 0.6314\n",
      "val Loss: 2.5637 Acc: 0.5990\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 2.6149 Acc: 0.6127\n",
      "val Loss: 2.5646 Acc: 0.6000\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 2.6025 Acc: 0.6284\n",
      "val Loss: 2.5530 Acc: 0.5961\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 2.5681 Acc: 0.6480\n",
      "val Loss: 2.5534 Acc: 0.5951\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 2.6039 Acc: 0.6245\n",
      "val Loss: 2.5575 Acc: 0.6000\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 2.5958 Acc: 0.6294\n",
      "val Loss: 2.5452 Acc: 0.6010\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 2.6133 Acc: 0.6137\n",
      "val Loss: 2.5553 Acc: 0.6000\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 2.6165 Acc: 0.6284\n",
      "val Loss: 2.5500 Acc: 0.5980\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 2.5994 Acc: 0.6275\n",
      "val Loss: 2.5453 Acc: 0.6000\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 2.5820 Acc: 0.6176\n",
      "val Loss: 2.5471 Acc: 0.5971\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 2.5776 Acc: 0.6235\n",
      "val Loss: 2.5310 Acc: 0.6039\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 2.5687 Acc: 0.6461\n",
      "val Loss: 2.5360 Acc: 0.6010\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 2.5193 Acc: 0.6490\n",
      "val Loss: 2.5251 Acc: 0.6078\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 2.6110 Acc: 0.6098\n",
      "val Loss: 2.5450 Acc: 0.6029\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 2.5554 Acc: 0.6363\n",
      "val Loss: 2.5322 Acc: 0.5980\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 2.6098 Acc: 0.6245\n",
      "val Loss: 2.5397 Acc: 0.6059\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 2.5469 Acc: 0.6500\n",
      "val Loss: 2.5289 Acc: 0.6059\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 2.5485 Acc: 0.6480\n",
      "val Loss: 2.5203 Acc: 0.5961\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 2.5834 Acc: 0.6284\n",
      "val Loss: 2.5303 Acc: 0.6020\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 2.5530 Acc: 0.6422\n",
      "val Loss: 2.5264 Acc: 0.6039\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 2.5641 Acc: 0.6294\n",
      "val Loss: 2.5275 Acc: 0.6049\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 2.5347 Acc: 0.6225\n",
      "val Loss: 2.5120 Acc: 0.6098\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 2.5702 Acc: 0.6225\n",
      "val Loss: 2.5250 Acc: 0.6059\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 2.5535 Acc: 0.6490\n",
      "val Loss: 2.5132 Acc: 0.6000\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 2.5341 Acc: 0.6510\n",
      "val Loss: 2.5210 Acc: 0.6000\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 2.4858 Acc: 0.6490\n",
      "val Loss: 2.5053 Acc: 0.6010\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 2.5569 Acc: 0.6402\n",
      "val Loss: 2.5135 Acc: 0.6049\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 2.5572 Acc: 0.6353\n",
      "val Loss: 2.5228 Acc: 0.6088\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 2.5370 Acc: 0.6373\n",
      "val Loss: 2.5035 Acc: 0.6049\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 2.5331 Acc: 0.6382\n",
      "val Loss: 2.5001 Acc: 0.6069\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 2.5514 Acc: 0.6294\n",
      "val Loss: 2.4963 Acc: 0.6059\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 2.5094 Acc: 0.6510\n",
      "val Loss: 2.4968 Acc: 0.6039\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 2.5212 Acc: 0.6412\n",
      "val Loss: 2.4975 Acc: 0.6029\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 2.4913 Acc: 0.6627\n",
      "val Loss: 2.4968 Acc: 0.6049\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 2.5440 Acc: 0.6373\n",
      "val Loss: 2.5004 Acc: 0.5971\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 2.5296 Acc: 0.6294\n",
      "val Loss: 2.4919 Acc: 0.6049\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 2.5440 Acc: 0.6353\n",
      "val Loss: 2.4898 Acc: 0.6069\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 2.4963 Acc: 0.6569\n",
      "val Loss: 2.4928 Acc: 0.6098\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 2.5183 Acc: 0.6471\n",
      "val Loss: 2.4975 Acc: 0.6059\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 2.5181 Acc: 0.6353\n",
      "val Loss: 2.4853 Acc: 0.6088\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 2.5403 Acc: 0.6422\n",
      "val Loss: 2.4883 Acc: 0.6088\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 2.5266 Acc: 0.6157\n",
      "val Loss: 2.4860 Acc: 0.6049\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 2.5128 Acc: 0.6500\n",
      "val Loss: 2.4838 Acc: 0.6059\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 2.4828 Acc: 0.6441\n",
      "val Loss: 2.4892 Acc: 0.6020\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 2.5587 Acc: 0.6422\n",
      "val Loss: 2.4948 Acc: 0.6039\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 2.5179 Acc: 0.6363\n",
      "val Loss: 2.4776 Acc: 0.6049\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 2.5384 Acc: 0.6412\n",
      "val Loss: 2.4951 Acc: 0.6000\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 2.5485 Acc: 0.6510\n",
      "val Loss: 2.4869 Acc: 0.6059\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 2.4850 Acc: 0.6500\n",
      "val Loss: 2.4814 Acc: 0.6078\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 2.4742 Acc: 0.6559\n",
      "val Loss: 2.4718 Acc: 0.6029\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 2.4859 Acc: 0.6510\n",
      "val Loss: 2.4581 Acc: 0.6069\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 2.5058 Acc: 0.6392\n",
      "val Loss: 2.4597 Acc: 0.6059\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 2.5358 Acc: 0.6392\n",
      "val Loss: 2.4654 Acc: 0.6069\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 2.5210 Acc: 0.6373\n",
      "val Loss: 2.4682 Acc: 0.6049\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 2.5127 Acc: 0.6471\n",
      "val Loss: 2.4659 Acc: 0.6039\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 2.4753 Acc: 0.6451\n",
      "val Loss: 2.4604 Acc: 0.6088\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 2.4686 Acc: 0.6490\n",
      "val Loss: 2.4634 Acc: 0.6059\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 2.5242 Acc: 0.6206\n",
      "val Loss: 2.4587 Acc: 0.6049\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 2.4807 Acc: 0.6539\n",
      "val Loss: 2.4727 Acc: 0.6049\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 2.5055 Acc: 0.6275\n",
      "val Loss: 2.4745 Acc: 0.6029\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 2.4535 Acc: 0.6696\n",
      "val Loss: 2.4730 Acc: 0.6049\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 2.5141 Acc: 0.6343\n",
      "val Loss: 2.4666 Acc: 0.6118\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 2.5101 Acc: 0.6333\n",
      "val Loss: 2.4677 Acc: 0.6088\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 2.5010 Acc: 0.6520\n",
      "val Loss: 2.4565 Acc: 0.6059\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 2.5312 Acc: 0.6451\n",
      "val Loss: 2.4701 Acc: 0.6039\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 2.4596 Acc: 0.6873\n",
      "val Loss: 2.4673 Acc: 0.6049\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 2.5306 Acc: 0.6206\n",
      "val Loss: 2.4611 Acc: 0.6069\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 2.5174 Acc: 0.6245\n",
      "val Loss: 2.4699 Acc: 0.6029\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 2.5177 Acc: 0.6431\n",
      "val Loss: 2.4605 Acc: 0.6039\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 2.4829 Acc: 0.6304\n",
      "val Loss: 2.4570 Acc: 0.6020\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 2.5231 Acc: 0.6324\n",
      "val Loss: 2.4552 Acc: 0.6088\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 2.4813 Acc: 0.6422\n",
      "val Loss: 2.4563 Acc: 0.6078\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 2.4765 Acc: 0.6461\n",
      "val Loss: 2.4569 Acc: 0.6059\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 2.5044 Acc: 0.6373\n",
      "val Loss: 2.4646 Acc: 0.6049\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 2.4917 Acc: 0.6422\n",
      "val Loss: 2.4569 Acc: 0.6039\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 2.5176 Acc: 0.6539\n",
      "val Loss: 2.4582 Acc: 0.6069\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 2.5039 Acc: 0.6343\n",
      "val Loss: 2.4675 Acc: 0.6049\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 2.5273 Acc: 0.6373\n",
      "val Loss: 2.4781 Acc: 0.6039\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 2.4699 Acc: 0.6647\n",
      "val Loss: 2.4519 Acc: 0.6127\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 2.4779 Acc: 0.6539\n",
      "val Loss: 2.4546 Acc: 0.6059\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 2.4996 Acc: 0.6373\n",
      "val Loss: 2.4651 Acc: 0.6039\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 2.4996 Acc: 0.6373\n",
      "val Loss: 2.4583 Acc: 0.6078\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 2.5016 Acc: 0.6412\n",
      "val Loss: 2.4599 Acc: 0.6078\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 2.4893 Acc: 0.6304\n",
      "val Loss: 2.4678 Acc: 0.6069\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 2.4542 Acc: 0.6569\n",
      "val Loss: 2.4521 Acc: 0.6059\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 2.4919 Acc: 0.6471\n",
      "val Loss: 2.4591 Acc: 0.6078\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 2.4730 Acc: 0.6500\n",
      "val Loss: 2.4532 Acc: 0.6059\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 2.4756 Acc: 0.6539\n",
      "val Loss: 2.4573 Acc: 0.6098\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 2.4596 Acc: 0.6578\n",
      "val Loss: 2.4573 Acc: 0.6000\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 2.4755 Acc: 0.6549\n",
      "val Loss: 2.4519 Acc: 0.6039\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 2.4752 Acc: 0.6422\n",
      "val Loss: 2.4519 Acc: 0.6078\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 2.4971 Acc: 0.6500\n",
      "val Loss: 2.4598 Acc: 0.6088\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 2.4845 Acc: 0.6451\n",
      "val Loss: 2.4616 Acc: 0.6039\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 2.5005 Acc: 0.6608\n",
      "val Loss: 2.4589 Acc: 0.6039\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 2.5208 Acc: 0.6314\n",
      "val Loss: 2.4682 Acc: 0.6049\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 2.5224 Acc: 0.6353\n",
      "val Loss: 2.4669 Acc: 0.6069\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 2.4778 Acc: 0.6559\n",
      "val Loss: 2.4544 Acc: 0.6059\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 2.4687 Acc: 0.6637\n",
      "val Loss: 2.4450 Acc: 0.6049\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 2.4657 Acc: 0.6549\n",
      "val Loss: 2.4507 Acc: 0.6078\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 2.5146 Acc: 0.6324\n",
      "val Loss: 2.4628 Acc: 0.6069\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 2.4850 Acc: 0.6569\n",
      "val Loss: 2.4597 Acc: 0.6049\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 2.5441 Acc: 0.6382\n",
      "val Loss: 2.4586 Acc: 0.6078\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 2.5111 Acc: 0.6343\n",
      "val Loss: 2.4506 Acc: 0.6049\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 2.5229 Acc: 0.6461\n",
      "val Loss: 2.4684 Acc: 0.6088\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 2.4589 Acc: 0.6559\n",
      "val Loss: 2.4559 Acc: 0.6078\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 2.4778 Acc: 0.6500\n",
      "val Loss: 2.4544 Acc: 0.6127\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 2.4795 Acc: 0.6529\n",
      "val Loss: 2.4541 Acc: 0.6069\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 2.4997 Acc: 0.6627\n",
      "val Loss: 2.4603 Acc: 0.6069\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 2.5150 Acc: 0.6412\n",
      "val Loss: 2.4603 Acc: 0.6039\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 2.5231 Acc: 0.6324\n",
      "val Loss: 2.4667 Acc: 0.6049\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 2.4797 Acc: 0.6500\n",
      "val Loss: 2.4541 Acc: 0.6069\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 2.5061 Acc: 0.6431\n",
      "val Loss: 2.4712 Acc: 0.6049\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 2.4991 Acc: 0.6324\n",
      "val Loss: 2.4635 Acc: 0.6049\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 2.4726 Acc: 0.6461\n",
      "val Loss: 2.4511 Acc: 0.6108\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 2.5106 Acc: 0.6225\n",
      "val Loss: 2.4586 Acc: 0.6098\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 2.4868 Acc: 0.6196\n",
      "val Loss: 2.4487 Acc: 0.6098\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 2.5163 Acc: 0.6353\n",
      "val Loss: 2.4555 Acc: 0.6039\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 2.4904 Acc: 0.6304\n",
      "val Loss: 2.4536 Acc: 0.6069\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 2.4909 Acc: 0.6431\n",
      "val Loss: 2.4555 Acc: 0.6078\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 2.5197 Acc: 0.6245\n",
      "val Loss: 2.4597 Acc: 0.6020\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 2.4792 Acc: 0.6431\n",
      "val Loss: 2.4466 Acc: 0.6098\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 2.4834 Acc: 0.6480\n",
      "val Loss: 2.4570 Acc: 0.6049\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 2.4982 Acc: 0.6529\n",
      "val Loss: 2.4604 Acc: 0.6059\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 2.4626 Acc: 0.6539\n",
      "val Loss: 2.4484 Acc: 0.6118\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 2.4832 Acc: 0.6471\n",
      "val Loss: 2.4530 Acc: 0.6108\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 2.5069 Acc: 0.6343\n",
      "val Loss: 2.4400 Acc: 0.6078\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 2.4657 Acc: 0.6598\n",
      "val Loss: 2.4568 Acc: 0.6069\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 2.4759 Acc: 0.6500\n",
      "val Loss: 2.4556 Acc: 0.6039\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 2.5088 Acc: 0.6373\n",
      "val Loss: 2.4611 Acc: 0.6059\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 2.4456 Acc: 0.6578\n",
      "val Loss: 2.4552 Acc: 0.6069\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 2.5034 Acc: 0.6490\n",
      "val Loss: 2.4516 Acc: 0.6049\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 2.4718 Acc: 0.6578\n",
      "val Loss: 2.4579 Acc: 0.6078\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 2.4736 Acc: 0.6520\n",
      "val Loss: 2.4525 Acc: 0.6088\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 2.4663 Acc: 0.6363\n",
      "val Loss: 2.4579 Acc: 0.6059\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 2.4989 Acc: 0.6441\n",
      "val Loss: 2.4661 Acc: 0.6049\n",
      "\n",
      "Training complete in 25m 20s\n",
      "Best val Acc: 0.612745\n",
      "\n",
      "----- Performing experiment with fixed lr=0.01 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6804 Acc: 0.0059\n",
      "val Loss: 4.6329 Acc: 0.0137\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.6210 Acc: 0.0078\n",
      "val Loss: 4.5669 Acc: 0.0314\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.5657 Acc: 0.0216\n",
      "val Loss: 4.5102 Acc: 0.0500\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.5122 Acc: 0.0490\n",
      "val Loss: 4.4559 Acc: 0.0775\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.4591 Acc: 0.0784\n",
      "val Loss: 4.4045 Acc: 0.1275\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.4121 Acc: 0.1275\n",
      "val Loss: 4.3537 Acc: 0.1735\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 4.3602 Acc: 0.1657\n",
      "val Loss: 4.3015 Acc: 0.2088\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 4.3119 Acc: 0.1765\n",
      "val Loss: 4.2516 Acc: 0.2343\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 4.2639 Acc: 0.2255\n",
      "val Loss: 4.2023 Acc: 0.2765\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 4.2095 Acc: 0.2637\n",
      "val Loss: 4.1485 Acc: 0.3098\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "val Loss: 4.1029 Acc: 0.3245\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 4.1460 Acc: 0.2882\n",
      "val Loss: 4.0573 Acc: 0.3245\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 4.0801 Acc: 0.3078\n",
      "val Loss: 4.0113 Acc: 0.3529\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 3.2400 Acc: 0.5343\n",
      "val Loss: 3.1774 Acc: 0.5441\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 3.2241 Acc: 0.5618\n",
      "val Loss: 3.1431 Acc: 0.5578\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 3.1880 Acc: 0.5627\n",
      "val Loss: 3.1042 Acc: 0.5647\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 3.1437 Acc: 0.5431\n",
      "val Loss: 3.0767 Acc: 0.5588\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 3.1481 Acc: 0.5422\n",
      "val Loss: 3.0563 Acc: 0.5676\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 3.1124 Acc: 0.5627\n",
      "val Loss: 3.0197 Acc: 0.5716\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 3.0646 Acc: 0.5471\n",
      "val Loss: 2.9964 Acc: 0.5706\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 3.0477 Acc: 0.5824\n",
      "val Loss: 2.9689 Acc: 0.5647\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 2.9879 Acc: 0.6078\n",
      "val Loss: 2.9386 Acc: 0.5804\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 3.0016 Acc: 0.5833\n",
      "val Loss: 2.9219 Acc: 0.5804\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 2.9923 Acc: 0.5775\n",
      "val Loss: 2.9019 Acc: 0.5833\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 2.9491 Acc: 0.5716\n",
      "val Loss: 2.8701 Acc: 0.5794\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 2.9329 Acc: 0.5873\n",
      "val Loss: 2.8474 Acc: 0.5882\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 2.8631 Acc: 0.6216\n",
      "val Loss: 2.8143 Acc: 0.5814\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 2.8744 Acc: 0.5902\n",
      "val Loss: 2.7886 Acc: 0.5902\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 2.8689 Acc: 0.5961\n",
      "val Loss: 2.7749 Acc: 0.5902\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 2.8070 Acc: 0.6127\n",
      "val Loss: 2.7468 Acc: 0.5824\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 2.8346 Acc: 0.5843\n",
      "val Loss: 2.7258 Acc: 0.5912\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 2.7625 Acc: 0.6098\n",
      "val Loss: 2.7007 Acc: 0.5961\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 2.7547 Acc: 0.6176\n",
      "val Loss: 2.6852 Acc: 0.6039\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 2.7340 Acc: 0.6275\n",
      "val Loss: 2.6628 Acc: 0.6020\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 2.6956 Acc: 0.6343\n",
      "val Loss: 2.6388 Acc: 0.6039\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 2.6671 Acc: 0.6275\n",
      "val Loss: 2.6174 Acc: 0.6010\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 2.6980 Acc: 0.6078\n",
      "val Loss: 2.6070 Acc: 0.6020\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 2.6291 Acc: 0.6284\n",
      "val Loss: 2.5824 Acc: 0.5941\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 2.6537 Acc: 0.6069\n",
      "val Loss: 2.5630 Acc: 0.6020\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 2.5722 Acc: 0.6255\n",
      "val Loss: 2.5532 Acc: 0.6069\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 2.5873 Acc: 0.6324\n",
      "val Loss: 2.5510 Acc: 0.6020\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 2.5856 Acc: 0.6490\n",
      "val Loss: 2.5604 Acc: 0.6059\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 2.6058 Acc: 0.6157\n",
      "val Loss: 2.5514 Acc: 0.6088\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 2.5959 Acc: 0.6333\n",
      "val Loss: 2.5528 Acc: 0.6078\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 2.5775 Acc: 0.6520\n",
      "val Loss: 2.5551 Acc: 0.6049\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 2.5995 Acc: 0.6324\n",
      "val Loss: 2.5575 Acc: 0.6039\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 2.6001 Acc: 0.6275\n",
      "val Loss: 2.5549 Acc: 0.6078\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 2.6059 Acc: 0.6363\n",
      "val Loss: 2.5430 Acc: 0.6127\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 2.5782 Acc: 0.6412\n",
      "val Loss: 2.5446 Acc: 0.6069\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 2.5654 Acc: 0.6588\n",
      "val Loss: 2.5470 Acc: 0.6078\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 2.5485 Acc: 0.6500\n",
      "val Loss: 2.5340 Acc: 0.6108\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 2.6084 Acc: 0.6020\n",
      "val Loss: 2.5435 Acc: 0.6108\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 2.5778 Acc: 0.6510\n",
      "val Loss: 2.5336 Acc: 0.6127\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 2.5810 Acc: 0.6333\n",
      "val Loss: 2.5362 Acc: 0.6098\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 2.5741 Acc: 0.6471\n",
      "val Loss: 2.5304 Acc: 0.6078\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 2.5387 Acc: 0.6441\n",
      "val Loss: 2.5167 Acc: 0.6069\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 2.5788 Acc: 0.6363\n",
      "val Loss: 2.5271 Acc: 0.6098\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 2.5846 Acc: 0.6461\n",
      "val Loss: 2.5216 Acc: 0.6098\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 2.6053 Acc: 0.6167\n",
      "val Loss: 2.5238 Acc: 0.6069\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 2.5893 Acc: 0.6510\n",
      "val Loss: 2.5239 Acc: 0.6118\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 2.5821 Acc: 0.6412\n",
      "val Loss: 2.5339 Acc: 0.6078\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 2.5713 Acc: 0.6412\n",
      "val Loss: 2.5211 Acc: 0.6108\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 2.5361 Acc: 0.6490\n",
      "val Loss: 2.5081 Acc: 0.6088\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 2.5381 Acc: 0.6392\n",
      "val Loss: 2.5104 Acc: 0.6108\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 2.5730 Acc: 0.6314\n",
      "val Loss: 2.5117 Acc: 0.6118\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 2.5714 Acc: 0.6304\n",
      "val Loss: 2.5221 Acc: 0.6108\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 2.5442 Acc: 0.6510\n",
      "val Loss: 2.5057 Acc: 0.6118\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 2.5183 Acc: 0.6314\n",
      "val Loss: 2.5082 Acc: 0.6088\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 2.5578 Acc: 0.6265\n",
      "val Loss: 2.4997 Acc: 0.6088\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 2.5504 Acc: 0.6363\n",
      "val Loss: 2.5102 Acc: 0.6108\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 2.5633 Acc: 0.6196\n",
      "val Loss: 2.5087 Acc: 0.6118\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 2.5267 Acc: 0.6500\n",
      "val Loss: 2.5037 Acc: 0.6118\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 2.5329 Acc: 0.6245\n",
      "val Loss: 2.4962 Acc: 0.6206\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 2.5441 Acc: 0.6490\n",
      "val Loss: 2.5075 Acc: 0.6157\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 2.5464 Acc: 0.6304\n",
      "val Loss: 2.5005 Acc: 0.6157\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 2.5174 Acc: 0.6480\n",
      "val Loss: 2.4891 Acc: 0.6137\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 2.5409 Acc: 0.6412\n",
      "val Loss: 2.4942 Acc: 0.6147\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 2.5722 Acc: 0.6422\n",
      "val Loss: 2.5090 Acc: 0.6108\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 2.5518 Acc: 0.6382\n",
      "val Loss: 2.4939 Acc: 0.6137\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 2.5186 Acc: 0.6461\n",
      "val Loss: 2.4913 Acc: 0.6176\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 2.5375 Acc: 0.6412\n",
      "val Loss: 2.4860 Acc: 0.6127\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 2.5334 Acc: 0.6451\n",
      "val Loss: 2.4860 Acc: 0.6118\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 2.4951 Acc: 0.6529\n",
      "val Loss: 2.4766 Acc: 0.6167\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 2.5384 Acc: 0.6255\n",
      "val Loss: 2.4771 Acc: 0.6137\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 2.5495 Acc: 0.6255\n",
      "val Loss: 2.4809 Acc: 0.6088\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 2.5456 Acc: 0.6304\n",
      "val Loss: 2.4872 Acc: 0.6118\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 2.5446 Acc: 0.6343\n",
      "val Loss: 2.4877 Acc: 0.6157\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 2.5244 Acc: 0.6412\n",
      "val Loss: 2.4794 Acc: 0.6196\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 2.5211 Acc: 0.6392\n",
      "val Loss: 2.4852 Acc: 0.6176\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 2.5072 Acc: 0.6549\n",
      "val Loss: 2.4734 Acc: 0.6127\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 2.5324 Acc: 0.6206\n",
      "val Loss: 2.4715 Acc: 0.6186\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 2.5406 Acc: 0.6196\n",
      "val Loss: 2.4710 Acc: 0.6157\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 2.5421 Acc: 0.6216\n",
      "val Loss: 2.4648 Acc: 0.6167\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 2.5011 Acc: 0.6471\n",
      "val Loss: 2.4600 Acc: 0.6157\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 2.4834 Acc: 0.6520\n",
      "val Loss: 2.4606 Acc: 0.6216\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 2.5245 Acc: 0.6314\n",
      "val Loss: 2.4608 Acc: 0.6167\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 2.4511 Acc: 0.6598\n",
      "val Loss: 2.4563 Acc: 0.6216\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 2.5303 Acc: 0.6461\n",
      "val Loss: 2.4599 Acc: 0.6157\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 2.4946 Acc: 0.6520\n",
      "val Loss: 2.4647 Acc: 0.6118\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 2.5160 Acc: 0.6461\n",
      "val Loss: 2.4644 Acc: 0.6137\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 2.4776 Acc: 0.6343\n",
      "val Loss: 2.4552 Acc: 0.6186\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 2.4945 Acc: 0.6284\n",
      "val Loss: 2.4607 Acc: 0.6127\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 2.5031 Acc: 0.6549\n",
      "val Loss: 2.4615 Acc: 0.6147\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 2.4892 Acc: 0.6461\n",
      "val Loss: 2.4527 Acc: 0.6118\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 2.5018 Acc: 0.6333\n",
      "val Loss: 2.4491 Acc: 0.6186\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 2.4798 Acc: 0.6618\n",
      "val Loss: 2.4506 Acc: 0.6167\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 2.4834 Acc: 0.6529\n",
      "val Loss: 2.4501 Acc: 0.6176\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 2.5045 Acc: 0.6510\n",
      "val Loss: 2.4496 Acc: 0.6157\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 2.5093 Acc: 0.6422\n",
      "val Loss: 2.4505 Acc: 0.6157\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 2.5249 Acc: 0.6353\n",
      "val Loss: 2.4601 Acc: 0.6098\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 2.4630 Acc: 0.6618\n",
      "val Loss: 2.4501 Acc: 0.6167\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 2.4735 Acc: 0.6716\n",
      "val Loss: 2.4555 Acc: 0.6157\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 2.4852 Acc: 0.6402\n",
      "val Loss: 2.4542 Acc: 0.6176\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 2.4653 Acc: 0.6529\n",
      "val Loss: 2.4527 Acc: 0.6196\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 2.5156 Acc: 0.6304\n",
      "val Loss: 2.4591 Acc: 0.6186\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 2.5211 Acc: 0.6333\n",
      "val Loss: 2.4570 Acc: 0.6176\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 2.4812 Acc: 0.6569\n",
      "val Loss: 2.4559 Acc: 0.6157\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 2.4827 Acc: 0.6402\n",
      "val Loss: 2.4557 Acc: 0.6186\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 2.4967 Acc: 0.6539\n",
      "val Loss: 2.4593 Acc: 0.6216\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 2.5082 Acc: 0.6441\n",
      "val Loss: 2.4703 Acc: 0.6127\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 2.4714 Acc: 0.6520\n",
      "val Loss: 2.4483 Acc: 0.6206\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 2.4764 Acc: 0.6549\n",
      "val Loss: 2.4495 Acc: 0.6186\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 2.4674 Acc: 0.6608\n",
      "val Loss: 2.4491 Acc: 0.6157\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 2.4662 Acc: 0.6451\n",
      "val Loss: 2.4443 Acc: 0.6186\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 2.4685 Acc: 0.6471\n",
      "val Loss: 2.4436 Acc: 0.6167\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 2.4691 Acc: 0.6422\n",
      "val Loss: 2.4534 Acc: 0.6167\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 2.5374 Acc: 0.6431\n",
      "val Loss: 2.4598 Acc: 0.6176\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 2.4576 Acc: 0.6627\n",
      "val Loss: 2.4560 Acc: 0.6245\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 2.4838 Acc: 0.6412\n",
      "val Loss: 2.4497 Acc: 0.6216\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 2.4862 Acc: 0.6539\n",
      "val Loss: 2.4494 Acc: 0.6186\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 2.4725 Acc: 0.6520\n",
      "val Loss: 2.4509 Acc: 0.6157\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 2.5243 Acc: 0.6275\n",
      "val Loss: 2.4583 Acc: 0.6137\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 2.4698 Acc: 0.6539\n",
      "val Loss: 2.4491 Acc: 0.6206\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 2.4830 Acc: 0.6422\n",
      "val Loss: 2.4436 Acc: 0.6186\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 2.4907 Acc: 0.6461\n",
      "val Loss: 2.4441 Acc: 0.6206\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 2.4916 Acc: 0.6431\n",
      "val Loss: 2.4500 Acc: 0.6167\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 2.5168 Acc: 0.6373\n",
      "val Loss: 2.4579 Acc: 0.6196\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 2.4967 Acc: 0.6461\n",
      "val Loss: 2.4530 Acc: 0.6245\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 2.4515 Acc: 0.6539\n",
      "val Loss: 2.4446 Acc: 0.6196\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 2.5305 Acc: 0.6206\n",
      "val Loss: 2.4587 Acc: 0.6196\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 2.4963 Acc: 0.6451\n",
      "val Loss: 2.4517 Acc: 0.6167\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 2.5419 Acc: 0.6176\n",
      "val Loss: 2.4570 Acc: 0.6176\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 2.4744 Acc: 0.6441\n",
      "val Loss: 2.4473 Acc: 0.6216\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 2.4755 Acc: 0.6627\n",
      "val Loss: 2.4490 Acc: 0.6206\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 2.4755 Acc: 0.6422\n",
      "val Loss: 2.4538 Acc: 0.6186\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 2.4861 Acc: 0.6471\n",
      "val Loss: 2.4554 Acc: 0.6186\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 2.5313 Acc: 0.6382\n",
      "val Loss: 2.4592 Acc: 0.6127\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 2.4568 Acc: 0.6647\n",
      "val Loss: 2.4543 Acc: 0.6186\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 2.5091 Acc: 0.6402\n",
      "val Loss: 2.4596 Acc: 0.6147\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 2.4935 Acc: 0.6490\n",
      "val Loss: 2.4580 Acc: 0.6127\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 2.4687 Acc: 0.6451\n",
      "val Loss: 2.4473 Acc: 0.6167\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 2.5157 Acc: 0.6245\n",
      "val Loss: 2.4563 Acc: 0.6176\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 2.4821 Acc: 0.6608\n",
      "val Loss: 2.4571 Acc: 0.6137\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 2.5046 Acc: 0.6382\n",
      "val Loss: 2.4541 Acc: 0.6137\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 2.5011 Acc: 0.6353\n",
      "val Loss: 2.4529 Acc: 0.6147\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 2.4534 Acc: 0.6686\n",
      "val Loss: 2.4478 Acc: 0.6186\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 2.5169 Acc: 0.6304\n",
      "val Loss: 2.4522 Acc: 0.6118\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 2.4718 Acc: 0.6333\n",
      "val Loss: 2.4433 Acc: 0.6167\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 2.5211 Acc: 0.6412\n",
      "val Loss: 2.4441 Acc: 0.6127\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 2.5021 Acc: 0.6529\n",
      "val Loss: 2.4519 Acc: 0.6167\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 2.5055 Acc: 0.6422\n",
      "val Loss: 2.4441 Acc: 0.6196\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 2.4786 Acc: 0.6500\n",
      "val Loss: 2.4443 Acc: 0.6196\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 2.4859 Acc: 0.6441\n",
      "val Loss: 2.4505 Acc: 0.6176\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 2.4829 Acc: 0.6529\n",
      "val Loss: 2.4511 Acc: 0.6167\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 2.4832 Acc: 0.6461\n",
      "val Loss: 2.4445 Acc: 0.6167\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 2.4949 Acc: 0.6529\n",
      "val Loss: 2.4326 Acc: 0.6176\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 2.4832 Acc: 0.6510\n",
      "val Loss: 2.4432 Acc: 0.6275\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 2.4595 Acc: 0.6539\n",
      "val Loss: 2.4389 Acc: 0.6167\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 2.5025 Acc: 0.6520\n",
      "val Loss: 2.4512 Acc: 0.6127\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 2.4576 Acc: 0.6529\n",
      "val Loss: 2.4412 Acc: 0.6127\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 2.4991 Acc: 0.6314\n",
      "val Loss: 2.4612 Acc: 0.6157\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 2.4645 Acc: 0.6412\n",
      "val Loss: 2.4501 Acc: 0.6216\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 2.4982 Acc: 0.6539\n",
      "val Loss: 2.4475 Acc: 0.6157\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 2.4869 Acc: 0.6333\n",
      "val Loss: 2.4563 Acc: 0.6167\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 2.5209 Acc: 0.6304\n",
      "val Loss: 2.4458 Acc: 0.6137\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 2.4830 Acc: 0.6441\n",
      "val Loss: 2.4435 Acc: 0.6196\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 2.4817 Acc: 0.6461\n",
      "val Loss: 2.4414 Acc: 0.6206\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 2.4933 Acc: 0.6539\n",
      "val Loss: 2.4489 Acc: 0.6118\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 2.4631 Acc: 0.6529\n",
      "val Loss: 2.4406 Acc: 0.6255\n",
      "\n",
      "Training complete in 25m 20s\n",
      "Best val Acc: 0.627451\n",
      "\n",
      "----- Performing experiment with fixed lr=0.1 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6736 Acc: 0.0098\n",
      "val Loss: 4.6304 Acc: 0.0059\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.6188 Acc: 0.0108\n",
      "val Loss: 4.5633 Acc: 0.0196\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.5563 Acc: 0.0255\n",
      "val Loss: 4.5087 Acc: 0.0373\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.5086 Acc: 0.0490\n",
      "val Loss: 4.4570 Acc: 0.0804\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.4507 Acc: 0.0667\n",
      "val Loss: 4.4058 Acc: 0.1186\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.4037 Acc: 0.1010\n",
      "val Loss: 4.3520 Acc: 0.1588\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 4.3600 Acc: 0.1569\n",
      "val Loss: 4.3021 Acc: 0.1912\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 4.3087 Acc: 0.1902\n",
      "val Loss: 4.2506 Acc: 0.2294\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 4.2718 Acc: 0.2216\n",
      "val Loss: 4.2036 Acc: 0.2765\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 4.2083 Acc: 0.2765\n",
      "val Loss: 4.1532 Acc: 0.3039\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 4.1760 Acc: 0.2833\n",
      "val Loss: 4.1050 Acc: 0.3167\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 4.1291 Acc: 0.3186\n",
      "val Loss: 4.0589 Acc: 0.3343\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 4.0776 Acc: 0.3412\n",
      "val Loss: 4.0118 Acc: 0.3667\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 4.0305 Acc: 0.3520\n",
      "val Loss: 3.9718 Acc: 0.3931\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 3.9854 Acc: 0.4186\n",
      "val Loss: 3.9173 Acc: 0.4078\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 3.9367 Acc: 0.4078\n",
      "val Loss: 3.8793 Acc: 0.4010\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 3.8886 Acc: 0.4294\n",
      "val Loss: 3.8298 Acc: 0.4235\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 3.8492 Acc: 0.4245\n",
      "val Loss: 3.7855 Acc: 0.4422\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 3.8157 Acc: 0.4422\n",
      "val Loss: 3.7464 Acc: 0.4598\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 3.7682 Acc: 0.4490\n",
      "val Loss: 3.7060 Acc: 0.4627\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 3.7226 Acc: 0.4706\n",
      "val Loss: 3.6569 Acc: 0.4833\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 3.6953 Acc: 0.4902\n",
      "val Loss: 3.6201 Acc: 0.4931\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 3.6485 Acc: 0.4912\n",
      "val Loss: 3.5818 Acc: 0.4892\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 3.6237 Acc: 0.4931\n",
      "val Loss: 3.5514 Acc: 0.4912\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 3.5857 Acc: 0.5059\n",
      "val Loss: 3.5140 Acc: 0.5167\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 3.5413 Acc: 0.4961\n",
      "val Loss: 3.4711 Acc: 0.5137\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 3.5089 Acc: 0.5216\n",
      "val Loss: 3.4281 Acc: 0.5275\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 3.4764 Acc: 0.5284\n",
      "val Loss: 3.4014 Acc: 0.5206\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 3.4596 Acc: 0.5245\n",
      "val Loss: 3.3645 Acc: 0.5265\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 3.4108 Acc: 0.5176\n",
      "val Loss: 3.3283 Acc: 0.5275\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 3.3647 Acc: 0.5373\n",
      "val Loss: 3.2922 Acc: 0.5373\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 3.3271 Acc: 0.5529\n",
      "val Loss: 3.2624 Acc: 0.5402\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 3.3055 Acc: 0.5480\n",
      "val Loss: 3.2313 Acc: 0.5304\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 3.2924 Acc: 0.5382\n",
      "val Loss: 3.2059 Acc: 0.5490\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 3.2263 Acc: 0.5608\n",
      "val Loss: 3.1743 Acc: 0.5373\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 3.2265 Acc: 0.5637\n",
      "val Loss: 3.1401 Acc: 0.5520\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 3.1680 Acc: 0.5686\n",
      "val Loss: 3.1116 Acc: 0.5549\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 3.1596 Acc: 0.5549\n",
      "val Loss: 3.0861 Acc: 0.5569\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 3.1303 Acc: 0.5647\n",
      "val Loss: 3.0598 Acc: 0.5598\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 3.0476 Acc: 0.5735\n",
      "val Loss: 3.0208 Acc: 0.5725\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 3.0626 Acc: 0.5961\n",
      "val Loss: 2.9970 Acc: 0.5765\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 3.0769 Acc: 0.5647\n",
      "val Loss: 2.9760 Acc: 0.5686\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 2.9981 Acc: 0.5902\n",
      "val Loss: 2.9303 Acc: 0.5784\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 3.0078 Acc: 0.5941\n",
      "val Loss: 2.9141 Acc: 0.5735\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 2.9710 Acc: 0.5824\n",
      "val Loss: 2.8931 Acc: 0.5912\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 2.9442 Acc: 0.5931\n",
      "val Loss: 2.8704 Acc: 0.5745\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 2.9357 Acc: 0.6039\n",
      "val Loss: 2.8450 Acc: 0.5755\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 2.8891 Acc: 0.6088\n",
      "val Loss: 2.8127 Acc: 0.5833\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 2.8898 Acc: 0.5961\n",
      "val Loss: 2.8020 Acc: 0.5804\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 2.8322 Acc: 0.6176\n",
      "val Loss: 2.7722 Acc: 0.5843\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 2.7888 Acc: 0.6255\n",
      "val Loss: 2.7557 Acc: 0.6010\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 2.7884 Acc: 0.5912\n",
      "val Loss: 2.7226 Acc: 0.5980\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 2.7911 Acc: 0.6049\n",
      "val Loss: 2.7102 Acc: 0.6010\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 2.7494 Acc: 0.6078\n",
      "val Loss: 2.6857 Acc: 0.6029\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 2.7581 Acc: 0.6098\n",
      "val Loss: 2.6718 Acc: 0.5980\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 2.7025 Acc: 0.6284\n",
      "val Loss: 2.6445 Acc: 0.6039\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 2.6996 Acc: 0.6157\n",
      "val Loss: 2.6261 Acc: 0.6118\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 2.6506 Acc: 0.6392\n",
      "val Loss: 2.6062 Acc: 0.6029\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 2.6479 Acc: 0.6284\n",
      "val Loss: 2.5812 Acc: 0.6118\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 2.6478 Acc: 0.6098\n",
      "val Loss: 2.5702 Acc: 0.6029\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 2.5777 Acc: 0.6529\n",
      "val Loss: 2.5686 Acc: 0.6118\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 2.5718 Acc: 0.6343\n",
      "val Loss: 2.5666 Acc: 0.6147\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 2.6297 Acc: 0.6235\n",
      "val Loss: 2.5565 Acc: 0.6137\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 2.6189 Acc: 0.6108\n",
      "val Loss: 2.5597 Acc: 0.6176\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 2.5765 Acc: 0.6539\n",
      "val Loss: 2.5522 Acc: 0.6186\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 2.5693 Acc: 0.6412\n",
      "val Loss: 2.5461 Acc: 0.6206\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 2.5561 Acc: 0.6402\n",
      "val Loss: 2.5446 Acc: 0.6167\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 2.5340 Acc: 0.6500\n",
      "val Loss: 2.5495 Acc: 0.6186\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 2.6395 Acc: 0.6176\n",
      "val Loss: 2.5541 Acc: 0.6167\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 2.5990 Acc: 0.6373\n",
      "val Loss: 2.5495 Acc: 0.6157\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 2.5467 Acc: 0.6471\n",
      "val Loss: 2.5349 Acc: 0.6225\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 2.6010 Acc: 0.6157\n",
      "val Loss: 2.5446 Acc: 0.6176\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 2.5964 Acc: 0.6294\n",
      "val Loss: 2.5416 Acc: 0.6147\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 2.5644 Acc: 0.6618\n",
      "val Loss: 2.5363 Acc: 0.6118\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 2.5730 Acc: 0.6500\n",
      "val Loss: 2.5395 Acc: 0.6176\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 2.5442 Acc: 0.6657\n",
      "val Loss: 2.5269 Acc: 0.6186\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 2.5571 Acc: 0.6510\n",
      "val Loss: 2.5294 Acc: 0.6196\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 2.5661 Acc: 0.6451\n",
      "val Loss: 2.5285 Acc: 0.6147\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 2.5465 Acc: 0.6382\n",
      "val Loss: 2.5272 Acc: 0.6186\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 2.5533 Acc: 0.6441\n",
      "val Loss: 2.5357 Acc: 0.6206\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 2.5660 Acc: 0.6461\n",
      "val Loss: 2.5320 Acc: 0.6167\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 2.6106 Acc: 0.6284\n",
      "val Loss: 2.5394 Acc: 0.6206\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 2.5693 Acc: 0.6412\n",
      "val Loss: 2.5226 Acc: 0.6147\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 2.5754 Acc: 0.6304\n",
      "val Loss: 2.5324 Acc: 0.6108\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 2.5501 Acc: 0.6559\n",
      "val Loss: 2.5250 Acc: 0.6186\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 2.5651 Acc: 0.6422\n",
      "val Loss: 2.5189 Acc: 0.6186\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 2.5636 Acc: 0.6206\n",
      "val Loss: 2.5139 Acc: 0.6206\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 2.5395 Acc: 0.6353\n",
      "val Loss: 2.5116 Acc: 0.6225\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 2.5350 Acc: 0.6245\n",
      "val Loss: 2.4991 Acc: 0.6275\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 2.5532 Acc: 0.6441\n",
      "val Loss: 2.5127 Acc: 0.6216\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 2.5645 Acc: 0.6284\n",
      "val Loss: 2.5050 Acc: 0.6176\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 2.5716 Acc: 0.6461\n",
      "val Loss: 2.5176 Acc: 0.6186\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 2.5606 Acc: 0.6392\n",
      "val Loss: 2.5012 Acc: 0.6167\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 2.5410 Acc: 0.6294\n",
      "val Loss: 2.4907 Acc: 0.6225\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 2.5365 Acc: 0.6441\n",
      "val Loss: 2.4985 Acc: 0.6216\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 2.5299 Acc: 0.6402\n",
      "val Loss: 2.5001 Acc: 0.6186\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 2.5088 Acc: 0.6490\n",
      "val Loss: 2.4966 Acc: 0.6206\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 2.5644 Acc: 0.6618\n",
      "val Loss: 2.5052 Acc: 0.6225\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 2.5120 Acc: 0.6431\n",
      "val Loss: 2.4947 Acc: 0.6216\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 2.5573 Acc: 0.6373\n",
      "val Loss: 2.4999 Acc: 0.6216\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 2.5058 Acc: 0.6618\n",
      "val Loss: 2.4858 Acc: 0.6196\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 2.5074 Acc: 0.6431\n",
      "val Loss: 2.4863 Acc: 0.6245\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 2.5254 Acc: 0.6480\n",
      "val Loss: 2.4891 Acc: 0.6265\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 2.5048 Acc: 0.6471\n",
      "val Loss: 2.4856 Acc: 0.6196\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 2.5144 Acc: 0.6392\n",
      "val Loss: 2.4748 Acc: 0.6245\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 2.5214 Acc: 0.6480\n",
      "val Loss: 2.4774 Acc: 0.6284\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 2.5249 Acc: 0.6422\n",
      "val Loss: 2.4805 Acc: 0.6284\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 2.5099 Acc: 0.6294\n",
      "val Loss: 2.4720 Acc: 0.6216\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 2.5249 Acc: 0.6510\n",
      "val Loss: 2.4678 Acc: 0.6235\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 2.5278 Acc: 0.6245\n",
      "val Loss: 2.4747 Acc: 0.6265\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 2.5136 Acc: 0.6431\n",
      "val Loss: 2.4791 Acc: 0.6284\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 2.4912 Acc: 0.6539\n",
      "val Loss: 2.4825 Acc: 0.6275\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 2.5371 Acc: 0.6304\n",
      "val Loss: 2.4770 Acc: 0.6216\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 2.5521 Acc: 0.6343\n",
      "val Loss: 2.4795 Acc: 0.6265\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 2.5243 Acc: 0.6598\n",
      "val Loss: 2.4631 Acc: 0.6275\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 2.4872 Acc: 0.6598\n",
      "val Loss: 2.4633 Acc: 0.6294\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 2.4800 Acc: 0.6647\n",
      "val Loss: 2.4701 Acc: 0.6265\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 2.4950 Acc: 0.6637\n",
      "val Loss: 2.4660 Acc: 0.6294\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 2.5423 Acc: 0.6137\n",
      "val Loss: 2.4683 Acc: 0.6216\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 2.4952 Acc: 0.6480\n",
      "val Loss: 2.4588 Acc: 0.6206\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 2.4832 Acc: 0.6490\n",
      "val Loss: 2.4536 Acc: 0.6216\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 2.5323 Acc: 0.6402\n",
      "val Loss: 2.4633 Acc: 0.6206\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 2.5047 Acc: 0.6461\n",
      "val Loss: 2.4579 Acc: 0.6235\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 2.4999 Acc: 0.6225\n",
      "val Loss: 2.4526 Acc: 0.6314\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 2.5123 Acc: 0.6353\n",
      "val Loss: 2.4692 Acc: 0.6225\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 2.4949 Acc: 0.6686\n",
      "val Loss: 2.4566 Acc: 0.6225\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 2.5006 Acc: 0.6245\n",
      "val Loss: 2.4578 Acc: 0.6235\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 2.4848 Acc: 0.6529\n",
      "val Loss: 2.4449 Acc: 0.6255\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 2.5385 Acc: 0.6412\n",
      "val Loss: 2.4571 Acc: 0.6265\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 2.5063 Acc: 0.6235\n",
      "val Loss: 2.4707 Acc: 0.6206\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 2.5331 Acc: 0.6235\n",
      "val Loss: 2.4605 Acc: 0.6196\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 2.4473 Acc: 0.6441\n",
      "val Loss: 2.4665 Acc: 0.6265\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 2.4961 Acc: 0.6480\n",
      "val Loss: 2.4592 Acc: 0.6314\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 2.4938 Acc: 0.6461\n",
      "val Loss: 2.4625 Acc: 0.6255\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 2.4775 Acc: 0.6529\n",
      "val Loss: 2.4440 Acc: 0.6294\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 2.4941 Acc: 0.6480\n",
      "val Loss: 2.4578 Acc: 0.6225\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 2.4920 Acc: 0.6402\n",
      "val Loss: 2.4492 Acc: 0.6265\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 2.5089 Acc: 0.6559\n",
      "val Loss: 2.4598 Acc: 0.6304\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 2.5192 Acc: 0.6451\n",
      "val Loss: 2.4636 Acc: 0.6284\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 2.4838 Acc: 0.6392\n",
      "val Loss: 2.4707 Acc: 0.6294\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 2.4650 Acc: 0.6520\n",
      "val Loss: 2.4594 Acc: 0.6245\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 2.5159 Acc: 0.6480\n",
      "val Loss: 2.4548 Acc: 0.6216\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 2.4839 Acc: 0.6608\n",
      "val Loss: 2.4564 Acc: 0.6235\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 2.4852 Acc: 0.6471\n",
      "val Loss: 2.4508 Acc: 0.6284\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 2.4837 Acc: 0.6451\n",
      "val Loss: 2.4543 Acc: 0.6275\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 2.4775 Acc: 0.6382\n",
      "val Loss: 2.4486 Acc: 0.6255\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 2.4712 Acc: 0.6667\n",
      "val Loss: 2.4610 Acc: 0.6255\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 2.5044 Acc: 0.6392\n",
      "val Loss: 2.4546 Acc: 0.6206\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 2.4923 Acc: 0.6402\n",
      "val Loss: 2.4521 Acc: 0.6245\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 2.5011 Acc: 0.6510\n",
      "val Loss: 2.4548 Acc: 0.6186\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 2.4733 Acc: 0.6363\n",
      "val Loss: 2.4525 Acc: 0.6255\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 2.4840 Acc: 0.6480\n",
      "val Loss: 2.4546 Acc: 0.6235\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 2.4980 Acc: 0.6422\n",
      "val Loss: 2.4474 Acc: 0.6265\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 2.4632 Acc: 0.6618\n",
      "val Loss: 2.4501 Acc: 0.6275\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 2.4847 Acc: 0.6402\n",
      "val Loss: 2.4488 Acc: 0.6294\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 2.4620 Acc: 0.6676\n",
      "val Loss: 2.4456 Acc: 0.6343\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 2.5110 Acc: 0.6382\n",
      "val Loss: 2.4512 Acc: 0.6216\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 2.5015 Acc: 0.6490\n",
      "val Loss: 2.4612 Acc: 0.6275\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 2.4686 Acc: 0.6578\n",
      "val Loss: 2.4651 Acc: 0.6265\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 2.5404 Acc: 0.6343\n",
      "val Loss: 2.4678 Acc: 0.6255\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 2.4805 Acc: 0.6343\n",
      "val Loss: 2.4504 Acc: 0.6265\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 2.4531 Acc: 0.6471\n",
      "val Loss: 2.4527 Acc: 0.6284\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 2.4638 Acc: 0.6461\n",
      "val Loss: 2.4515 Acc: 0.6275\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 2.4627 Acc: 0.6588\n",
      "val Loss: 2.4479 Acc: 0.6275\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 2.4900 Acc: 0.6500\n",
      "val Loss: 2.4626 Acc: 0.6235\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 2.5307 Acc: 0.6500\n",
      "val Loss: 2.4562 Acc: 0.6216\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 2.5020 Acc: 0.6412\n",
      "val Loss: 2.4512 Acc: 0.6216\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 2.4775 Acc: 0.6471\n",
      "val Loss: 2.4578 Acc: 0.6216\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 2.4951 Acc: 0.6373\n",
      "val Loss: 2.4519 Acc: 0.6235\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 2.5114 Acc: 0.6412\n",
      "val Loss: 2.4492 Acc: 0.6245\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 2.5045 Acc: 0.6392\n",
      "val Loss: 2.4580 Acc: 0.6206\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 2.5168 Acc: 0.6412\n",
      "val Loss: 2.4568 Acc: 0.6206\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 2.4711 Acc: 0.6480\n",
      "val Loss: 2.4578 Acc: 0.6196\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 2.4799 Acc: 0.6618\n",
      "val Loss: 2.4499 Acc: 0.6255\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 2.4461 Acc: 0.6608\n",
      "val Loss: 2.4437 Acc: 0.6324\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 2.5465 Acc: 0.6304\n",
      "val Loss: 2.4577 Acc: 0.6206\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 2.4792 Acc: 0.6382\n",
      "val Loss: 2.4537 Acc: 0.6314\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 2.4659 Acc: 0.6539\n",
      "val Loss: 2.4537 Acc: 0.6255\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 2.4561 Acc: 0.6637\n",
      "val Loss: 2.4417 Acc: 0.6265\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 2.4677 Acc: 0.6637\n",
      "val Loss: 2.4478 Acc: 0.6245\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 2.4742 Acc: 0.6588\n",
      "val Loss: 2.4396 Acc: 0.6314\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 2.4762 Acc: 0.6422\n",
      "val Loss: 2.4492 Acc: 0.6275\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 2.4829 Acc: 0.6569\n",
      "val Loss: 2.4550 Acc: 0.6255\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 2.4839 Acc: 0.6539\n",
      "val Loss: 2.4478 Acc: 0.6216\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 2.4478 Acc: 0.6647\n",
      "val Loss: 2.4420 Acc: 0.6255\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 2.4895 Acc: 0.6294\n",
      "val Loss: 2.4516 Acc: 0.6245\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 2.4945 Acc: 0.6480\n",
      "val Loss: 2.4535 Acc: 0.6265\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 2.5056 Acc: 0.6324\n",
      "val Loss: 2.4521 Acc: 0.6225\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 2.4989 Acc: 0.6451\n",
      "val Loss: 2.4562 Acc: 0.6314\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 2.4579 Acc: 0.6676\n",
      "val Loss: 2.4516 Acc: 0.6255\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 2.4838 Acc: 0.6490\n",
      "val Loss: 2.4610 Acc: 0.6245\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 2.5186 Acc: 0.6343\n",
      "val Loss: 2.4508 Acc: 0.6275\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 2.4846 Acc: 0.6304\n",
      "val Loss: 2.4496 Acc: 0.6245\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 2.4464 Acc: 0.6569\n",
      "val Loss: 2.4524 Acc: 0.6265\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 2.4557 Acc: 0.6716\n",
      "val Loss: 2.4411 Acc: 0.6275\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 2.5177 Acc: 0.6559\n",
      "val Loss: 2.4610 Acc: 0.6196\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 2.5045 Acc: 0.6382\n",
      "val Loss: 2.4656 Acc: 0.6196\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 2.5008 Acc: 0.6363\n",
      "val Loss: 2.4640 Acc: 0.6235\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 2.5058 Acc: 0.6480\n",
      "val Loss: 2.4606 Acc: 0.6245\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 2.5150 Acc: 0.6402\n",
      "val Loss: 2.4519 Acc: 0.6206\n",
      "\n",
      "Training complete in 25m 19s\n",
      "Best val Acc: 0.634314\n",
      "\n",
      "----- Performing experiment with fixed lr=1 -----\n",
      "\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6876 Acc: 0.0059\n",
      "val Loss: 4.6339 Acc: 0.0137\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.6190 Acc: 0.0157\n",
      "val Loss: 4.5712 Acc: 0.0284\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.5743 Acc: 0.0206\n",
      "val Loss: 4.5161 Acc: 0.0598\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.5066 Acc: 0.0520\n",
      "val Loss: 4.4631 Acc: 0.0824\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.4653 Acc: 0.0657\n",
      "val Loss: 4.4089 Acc: 0.1069\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.4196 Acc: 0.0892\n",
      "val Loss: 4.3561 Acc: 0.1431\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 4.3632 Acc: 0.1480\n",
      "val Loss: 4.3055 Acc: 0.2029\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 4.3155 Acc: 0.1971\n",
      "val Loss: 4.2545 Acc: 0.2392\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 4.2704 Acc: 0.2255\n",
      "val Loss: 4.2049 Acc: 0.2539\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 4.2127 Acc: 0.2667\n",
      "val Loss: 4.1560 Acc: 0.2922\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 4.1697 Acc: 0.2784\n",
      "val Loss: 4.1085 Acc: 0.3206\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 4.1190 Acc: 0.3157\n",
      "val Loss: 4.0605 Acc: 0.3500\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 4.0737 Acc: 0.3539\n",
      "val Loss: 4.0089 Acc: 0.3588\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 4.0420 Acc: 0.3549\n",
      "val Loss: 3.9716 Acc: 0.3706\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 3.9884 Acc: 0.3931\n",
      "val Loss: 3.9239 Acc: 0.3931\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 3.9494 Acc: 0.3961\n",
      "val Loss: 3.8774 Acc: 0.4196\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 3.9161 Acc: 0.4049\n",
      "val Loss: 3.8355 Acc: 0.4118\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 3.8614 Acc: 0.4314\n",
      "val Loss: 3.7943 Acc: 0.4314\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 3.8244 Acc: 0.4382\n",
      "val Loss: 3.7483 Acc: 0.4500\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 3.7709 Acc: 0.4520\n",
      "val Loss: 3.7092 Acc: 0.4539\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 3.7527 Acc: 0.4559\n",
      "val Loss: 3.6726 Acc: 0.4529\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 3.6964 Acc: 0.4775\n",
      "val Loss: 3.6315 Acc: 0.4873\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 3.6488 Acc: 0.4990\n",
      "val Loss: 3.5865 Acc: 0.4922\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 3.6326 Acc: 0.4971\n",
      "val Loss: 3.5488 Acc: 0.4824\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 3.5803 Acc: 0.5147\n",
      "val Loss: 3.5152 Acc: 0.4941\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 3.5583 Acc: 0.4990\n",
      "val Loss: 3.4747 Acc: 0.4980\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 3.5153 Acc: 0.5020\n",
      "val Loss: 3.4412 Acc: 0.5000\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 3.4909 Acc: 0.5137\n",
      "val Loss: 3.4064 Acc: 0.5049\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 3.4477 Acc: 0.5118\n",
      "val Loss: 3.3700 Acc: 0.5088\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 3.3954 Acc: 0.5245\n",
      "val Loss: 3.3327 Acc: 0.5118\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 3.3980 Acc: 0.5127\n",
      "val Loss: 3.3031 Acc: 0.5196\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 3.3649 Acc: 0.5353\n",
      "val Loss: 3.2709 Acc: 0.5225\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 3.3337 Acc: 0.5324\n",
      "val Loss: 3.2401 Acc: 0.5225\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 3.2708 Acc: 0.5608\n",
      "val Loss: 3.2074 Acc: 0.5275\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 3.2802 Acc: 0.5333\n",
      "val Loss: 3.1848 Acc: 0.5265\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 3.2250 Acc: 0.5520\n",
      "val Loss: 3.1441 Acc: 0.5353\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 3.1870 Acc: 0.5520\n",
      "val Loss: 3.1148 Acc: 0.5471\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 3.1455 Acc: 0.5794\n",
      "val Loss: 3.0889 Acc: 0.5451\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 3.0863 Acc: 0.5961\n",
      "val Loss: 3.0507 Acc: 0.5539\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 3.1066 Acc: 0.5647\n",
      "val Loss: 3.0370 Acc: 0.5529\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 3.0705 Acc: 0.5794\n",
      "val Loss: 2.9955 Acc: 0.5637\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 3.0399 Acc: 0.5843\n",
      "val Loss: 2.9705 Acc: 0.5618\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 3.0212 Acc: 0.5843\n",
      "val Loss: 2.9383 Acc: 0.5647\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 2.9761 Acc: 0.5980\n",
      "val Loss: 2.9114 Acc: 0.5569\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 2.9688 Acc: 0.5814\n",
      "val Loss: 2.8951 Acc: 0.5637\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 2.9647 Acc: 0.5902\n",
      "val Loss: 2.8751 Acc: 0.5716\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 2.8894 Acc: 0.5902\n",
      "val Loss: 2.8504 Acc: 0.5892\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 2.8992 Acc: 0.5892\n",
      "val Loss: 2.8223 Acc: 0.5794\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 2.8827 Acc: 0.5637\n",
      "val Loss: 2.7927 Acc: 0.5804\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 2.8529 Acc: 0.5873\n",
      "val Loss: 2.7710 Acc: 0.5873\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 2.8246 Acc: 0.6137\n",
      "val Loss: 2.7593 Acc: 0.5922\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 2.8163 Acc: 0.6020\n",
      "val Loss: 2.7396 Acc: 0.5853\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 2.7965 Acc: 0.6167\n",
      "val Loss: 2.7258 Acc: 0.5912\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 2.7721 Acc: 0.6010\n",
      "val Loss: 2.6870 Acc: 0.6029\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 2.7450 Acc: 0.6069\n",
      "val Loss: 2.6680 Acc: 0.6000\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 2.6998 Acc: 0.6137\n",
      "val Loss: 2.6495 Acc: 0.5961\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 2.6842 Acc: 0.6235\n",
      "val Loss: 2.6265 Acc: 0.5931\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 2.6908 Acc: 0.6069\n",
      "val Loss: 2.6061 Acc: 0.6029\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 2.6484 Acc: 0.6314\n",
      "val Loss: 2.5909 Acc: 0.6108\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 2.6478 Acc: 0.6235\n",
      "val Loss: 2.5645 Acc: 0.6118\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 2.6139 Acc: 0.6275\n",
      "val Loss: 2.5660 Acc: 0.6039\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 2.6094 Acc: 0.6304\n",
      "val Loss: 2.5688 Acc: 0.6137\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 2.6483 Acc: 0.6098\n",
      "val Loss: 2.5825 Acc: 0.6029\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 2.6125 Acc: 0.6147\n",
      "val Loss: 2.5773 Acc: 0.6069\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 2.5975 Acc: 0.6108\n",
      "val Loss: 2.5547 Acc: 0.6098\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 2.5720 Acc: 0.6451\n",
      "val Loss: 2.5567 Acc: 0.6147\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 2.6194 Acc: 0.6314\n",
      "val Loss: 2.5573 Acc: 0.6108\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 2.6295 Acc: 0.6118\n",
      "val Loss: 2.5603 Acc: 0.6069\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 2.6253 Acc: 0.6118\n",
      "val Loss: 2.5585 Acc: 0.6088\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 2.5713 Acc: 0.6500\n",
      "val Loss: 2.5458 Acc: 0.6137\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 2.5757 Acc: 0.6402\n",
      "val Loss: 2.5540 Acc: 0.6147\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 2.5913 Acc: 0.6176\n",
      "val Loss: 2.5425 Acc: 0.6137\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 2.5651 Acc: 0.6422\n",
      "val Loss: 2.5435 Acc: 0.6098\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 2.5427 Acc: 0.6412\n",
      "val Loss: 2.5257 Acc: 0.6118\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 2.5505 Acc: 0.6637\n",
      "val Loss: 2.5266 Acc: 0.6157\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 2.6007 Acc: 0.6225\n",
      "val Loss: 2.5381 Acc: 0.6127\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 2.5634 Acc: 0.6402\n",
      "val Loss: 2.5362 Acc: 0.6118\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 2.5866 Acc: 0.6284\n",
      "val Loss: 2.5318 Acc: 0.6127\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 2.5715 Acc: 0.6275\n",
      "val Loss: 2.5418 Acc: 0.6137\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 2.5805 Acc: 0.6392\n",
      "val Loss: 2.5320 Acc: 0.6127\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 2.5637 Acc: 0.6333\n",
      "val Loss: 2.5206 Acc: 0.6137\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 2.5753 Acc: 0.6471\n",
      "val Loss: 2.5284 Acc: 0.6137\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 2.5579 Acc: 0.6304\n",
      "val Loss: 2.5257 Acc: 0.6127\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 2.5558 Acc: 0.6431\n",
      "val Loss: 2.5180 Acc: 0.6157\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 2.5279 Acc: 0.6392\n",
      "val Loss: 2.5175 Acc: 0.6167\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 2.5219 Acc: 0.6520\n",
      "val Loss: 2.5158 Acc: 0.6176\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 2.5339 Acc: 0.6471\n",
      "val Loss: 2.5162 Acc: 0.6186\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 2.5645 Acc: 0.6451\n",
      "val Loss: 2.5117 Acc: 0.6216\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 2.5569 Acc: 0.6363\n",
      "val Loss: 2.5105 Acc: 0.6147\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 2.5599 Acc: 0.6304\n",
      "val Loss: 2.5145 Acc: 0.6147\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 2.5355 Acc: 0.6461\n",
      "val Loss: 2.5059 Acc: 0.6157\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 2.5623 Acc: 0.6382\n",
      "val Loss: 2.5075 Acc: 0.6206\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 2.5722 Acc: 0.6402\n",
      "val Loss: 2.5128 Acc: 0.6147\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 2.5395 Acc: 0.6304\n",
      "val Loss: 2.4985 Acc: 0.6206\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 2.5558 Acc: 0.6284\n",
      "val Loss: 2.4964 Acc: 0.6137\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 2.5463 Acc: 0.6490\n",
      "val Loss: 2.5037 Acc: 0.6196\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 2.5704 Acc: 0.6343\n",
      "val Loss: 2.5009 Acc: 0.6176\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 2.5170 Acc: 0.6431\n",
      "val Loss: 2.4945 Acc: 0.6186\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 2.5326 Acc: 0.6539\n",
      "val Loss: 2.4991 Acc: 0.6137\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 2.5291 Acc: 0.6461\n",
      "val Loss: 2.4978 Acc: 0.6206\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 2.4812 Acc: 0.6490\n",
      "val Loss: 2.4855 Acc: 0.6206\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 2.5255 Acc: 0.6392\n",
      "val Loss: 2.4907 Acc: 0.6157\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 2.5441 Acc: 0.6255\n",
      "val Loss: 2.4915 Acc: 0.6167\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 2.5385 Acc: 0.6363\n",
      "val Loss: 2.4917 Acc: 0.6186\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 2.5359 Acc: 0.6490\n",
      "val Loss: 2.4830 Acc: 0.6186\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 2.5215 Acc: 0.6294\n",
      "val Loss: 2.4778 Acc: 0.6157\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 2.4682 Acc: 0.6529\n",
      "val Loss: 2.4782 Acc: 0.6186\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 2.5074 Acc: 0.6461\n",
      "val Loss: 2.4665 Acc: 0.6186\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 2.5302 Acc: 0.6324\n",
      "val Loss: 2.4687 Acc: 0.6176\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 2.5142 Acc: 0.6480\n",
      "val Loss: 2.4828 Acc: 0.6167\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 2.5257 Acc: 0.6431\n",
      "val Loss: 2.4773 Acc: 0.6235\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 2.4989 Acc: 0.6402\n",
      "val Loss: 2.4853 Acc: 0.6167\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 2.5442 Acc: 0.6382\n",
      "val Loss: 2.4857 Acc: 0.6127\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 2.5038 Acc: 0.6500\n",
      "val Loss: 2.4790 Acc: 0.6176\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 2.5455 Acc: 0.6284\n",
      "val Loss: 2.4634 Acc: 0.6137\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 2.5397 Acc: 0.6275\n",
      "val Loss: 2.4682 Acc: 0.6206\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 2.5108 Acc: 0.6431\n",
      "val Loss: 2.4565 Acc: 0.6196\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 2.4819 Acc: 0.6471\n",
      "val Loss: 2.4614 Acc: 0.6186\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 2.4883 Acc: 0.6510\n",
      "val Loss: 2.4732 Acc: 0.6176\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 2.5265 Acc: 0.6363\n",
      "val Loss: 2.4672 Acc: 0.6167\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 2.5349 Acc: 0.6392\n",
      "val Loss: 2.4664 Acc: 0.6176\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 2.4971 Acc: 0.6500\n",
      "val Loss: 2.4585 Acc: 0.6225\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 2.5039 Acc: 0.6422\n",
      "val Loss: 2.4653 Acc: 0.6147\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 2.5002 Acc: 0.6480\n",
      "val Loss: 2.4635 Acc: 0.6167\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 2.4982 Acc: 0.6490\n",
      "val Loss: 2.4504 Acc: 0.6216\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 2.4513 Acc: 0.6480\n",
      "val Loss: 2.4602 Acc: 0.6216\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 2.5072 Acc: 0.6392\n",
      "val Loss: 2.4548 Acc: 0.6176\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 2.5101 Acc: 0.6343\n",
      "val Loss: 2.4708 Acc: 0.6167\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 2.4853 Acc: 0.6667\n",
      "val Loss: 2.4563 Acc: 0.6176\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 2.5150 Acc: 0.6353\n",
      "val Loss: 2.4583 Acc: 0.6186\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 2.5030 Acc: 0.6510\n",
      "val Loss: 2.4599 Acc: 0.6196\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 2.4905 Acc: 0.6402\n",
      "val Loss: 2.4643 Acc: 0.6225\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 2.5056 Acc: 0.6598\n",
      "val Loss: 2.4638 Acc: 0.6196\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 2.4753 Acc: 0.6490\n",
      "val Loss: 2.4545 Acc: 0.6216\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 2.4579 Acc: 0.6451\n",
      "val Loss: 2.4487 Acc: 0.6206\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 2.5183 Acc: 0.6294\n",
      "val Loss: 2.4579 Acc: 0.6216\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 2.5305 Acc: 0.6255\n",
      "val Loss: 2.4621 Acc: 0.6167\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 2.5097 Acc: 0.6422\n",
      "val Loss: 2.4614 Acc: 0.6206\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 2.4954 Acc: 0.6471\n",
      "val Loss: 2.4575 Acc: 0.6225\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 2.4750 Acc: 0.6549\n",
      "val Loss: 2.4505 Acc: 0.6206\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 2.5452 Acc: 0.6304\n",
      "val Loss: 2.4636 Acc: 0.6186\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 2.4948 Acc: 0.6578\n",
      "val Loss: 2.4445 Acc: 0.6196\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 2.5227 Acc: 0.6324\n",
      "val Loss: 2.4518 Acc: 0.6235\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 2.4945 Acc: 0.6520\n",
      "val Loss: 2.4622 Acc: 0.6216\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 2.5009 Acc: 0.6520\n",
      "val Loss: 2.4670 Acc: 0.6167\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 2.4860 Acc: 0.6510\n",
      "val Loss: 2.4568 Acc: 0.6186\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 2.5433 Acc: 0.6480\n",
      "val Loss: 2.4698 Acc: 0.6147\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 2.4826 Acc: 0.6392\n",
      "val Loss: 2.4596 Acc: 0.6235\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 2.4525 Acc: 0.6598\n",
      "val Loss: 2.4453 Acc: 0.6196\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 2.5023 Acc: 0.6412\n",
      "val Loss: 2.4573 Acc: 0.6186\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 2.4919 Acc: 0.6627\n",
      "val Loss: 2.4531 Acc: 0.6245\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 2.4881 Acc: 0.6520\n",
      "val Loss: 2.4559 Acc: 0.6167\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 2.5015 Acc: 0.6412\n",
      "val Loss: 2.4638 Acc: 0.6206\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 2.4853 Acc: 0.6265\n",
      "val Loss: 2.4591 Acc: 0.6206\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 2.4785 Acc: 0.6412\n",
      "val Loss: 2.4523 Acc: 0.6196\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 2.4616 Acc: 0.6441\n",
      "val Loss: 2.4651 Acc: 0.6225\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 2.4899 Acc: 0.6461\n",
      "val Loss: 2.4614 Acc: 0.6176\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 2.5122 Acc: 0.6314\n",
      "val Loss: 2.4674 Acc: 0.6167\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 2.4844 Acc: 0.6578\n",
      "val Loss: 2.4615 Acc: 0.6206\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 2.5065 Acc: 0.6510\n",
      "val Loss: 2.4624 Acc: 0.6176\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 2.4859 Acc: 0.6461\n",
      "val Loss: 2.4560 Acc: 0.6196\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 2.4885 Acc: 0.6627\n",
      "val Loss: 2.4556 Acc: 0.6176\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 2.4660 Acc: 0.6667\n",
      "val Loss: 2.4518 Acc: 0.6206\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 2.4788 Acc: 0.6549\n",
      "val Loss: 2.4501 Acc: 0.6235\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 2.5155 Acc: 0.6441\n",
      "val Loss: 2.4520 Acc: 0.6206\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 2.4970 Acc: 0.6510\n",
      "val Loss: 2.4648 Acc: 0.6216\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 2.5086 Acc: 0.6304\n",
      "val Loss: 2.4672 Acc: 0.6137\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 2.5039 Acc: 0.6392\n",
      "val Loss: 2.4612 Acc: 0.6186\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 2.5013 Acc: 0.6392\n",
      "val Loss: 2.4448 Acc: 0.6206\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 2.4896 Acc: 0.6657\n",
      "val Loss: 2.4458 Acc: 0.6255\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 2.4811 Acc: 0.6441\n",
      "val Loss: 2.4475 Acc: 0.6216\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 2.4996 Acc: 0.6245\n",
      "val Loss: 2.4642 Acc: 0.6196\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 2.5185 Acc: 0.6569\n",
      "val Loss: 2.4470 Acc: 0.6206\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 2.5330 Acc: 0.6343\n",
      "val Loss: 2.4570 Acc: 0.6186\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 2.5227 Acc: 0.6451\n",
      "val Loss: 2.4553 Acc: 0.6216\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 2.4909 Acc: 0.6441\n",
      "val Loss: 2.4435 Acc: 0.6196\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 2.4866 Acc: 0.6353\n",
      "val Loss: 2.4519 Acc: 0.6216\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 2.4745 Acc: 0.6510\n",
      "val Loss: 2.4464 Acc: 0.6225\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 2.4382 Acc: 0.6627\n",
      "val Loss: 2.4499 Acc: 0.6157\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 2.4708 Acc: 0.6588\n",
      "val Loss: 2.4594 Acc: 0.6186\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 2.5218 Acc: 0.6324\n",
      "val Loss: 2.4570 Acc: 0.6196\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 2.5458 Acc: 0.6245\n",
      "val Loss: 2.4603 Acc: 0.6127\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 2.4973 Acc: 0.6353\n",
      "val Loss: 2.4598 Acc: 0.6167\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 2.4893 Acc: 0.6520\n",
      "val Loss: 2.4534 Acc: 0.6225\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 2.5236 Acc: 0.6373\n",
      "val Loss: 2.4680 Acc: 0.6147\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 2.4971 Acc: 0.6373\n",
      "val Loss: 2.4484 Acc: 0.6216\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 2.4555 Acc: 0.6520\n",
      "val Loss: 2.4550 Acc: 0.6206\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 2.4934 Acc: 0.6392\n",
      "val Loss: 2.4520 Acc: 0.6245\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 2.5236 Acc: 0.6412\n",
      "val Loss: 2.4562 Acc: 0.6176\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 2.4785 Acc: 0.6549\n",
      "val Loss: 2.4582 Acc: 0.6157\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 2.4802 Acc: 0.6343\n",
      "val Loss: 2.4554 Acc: 0.6235\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 2.5024 Acc: 0.6471\n",
      "val Loss: 2.4485 Acc: 0.6196\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 2.4883 Acc: 0.6520\n",
      "val Loss: 2.4431 Acc: 0.6206\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 2.5053 Acc: 0.6402\n",
      "val Loss: 2.4606 Acc: 0.6137\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 2.4793 Acc: 0.6422\n",
      "val Loss: 2.4501 Acc: 0.6147\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 2.4727 Acc: 0.6471\n",
      "val Loss: 2.4473 Acc: 0.6235\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 2.4702 Acc: 0.6520\n",
      "val Loss: 2.4513 Acc: 0.6176\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 2.5039 Acc: 0.6343\n",
      "val Loss: 2.4509 Acc: 0.6186\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 2.4832 Acc: 0.6441\n",
      "val Loss: 2.4471 Acc: 0.6196\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 2.4674 Acc: 0.6598\n",
      "val Loss: 2.4449 Acc: 0.6225\n",
      "\n",
      "Training complete in 25m 18s\n",
      "Best val Acc: 0.625490\n"
     ]
    }
   ],
   "source": [
    "LR_SET = [10**i for i in range(-3, 1)]\n",
    "\n",
    "for lr in LR_SET:\n",
    "    print(f'\\n----- Performing experiment with fixed lr={lr} -----\\n')\n",
    "    \n",
    "    model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "    for param in model_conv.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "    num_ftrs = model_conv.fc.in_features\n",
    "    model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "    model_conv = model_conv.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that only parameters of final layer are being optimized as\n",
    "    # opposed to before.\n",
    "    optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 60 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=60, gamma=0.1)\n",
    "    \n",
    "    # Train model\n",
    "    model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                             exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation accuracy when using `ResNet50` as a feature extractor on `vgg-flowers` for the various learning rates are below.\n",
    "\n",
    "| Learning Rate Setting | Best Validation Accuracy |\n",
    "| ----------------------|--------------------------|\n",
    "| 0.001                 | 0.613                    |\n",
    "| 0.01                  | 0.627                    |\n",
    "| 0.1                   | 0.634                    |\n",
    "| 1                     | 0.625                    |\n",
    "\n",
    "From above, we can see that 0.1 is the best learning rate for using `ResNet50` as a feature extractor for the `vgg-flowers` VDD dataset.\n",
    "\n",
    "* b: (4 points)\n",
    "\n",
    "Given the two transfer learning approaches (Fine Tuning and Feature Extraction) and the various learning rate schemes, using Fine Tuning with a fixed learning rate equal to 0.01 resulted in the best validation accuracy on the holdout dataset. This isn't surprising, as the `ResNet50` model was pretrained on `ImageNet`, not the `vgg-flowers` dataset. Allowing for weight updates on the layers prior to the final Dense layers allows for `ResNet50` to learn features and representations specific to the `vgg-flowers` dataset.\n",
    "\n",
    "### Problem 2: *Transfer Learning: Shallow Learning vs Fine Tuning, PyTorch* (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
